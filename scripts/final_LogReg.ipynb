{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78845404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Path berhasil diset.\n",
      "ğŸ“‚ Root Dir: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\n",
      "ğŸ“‚ Data Dir: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\n",
      "\n",
      "ğŸ” Checking files:\n",
      "   CSV exists: True\n",
      "   - Path: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\dataset_training_FULL_COMPLETE.csv\n",
      "   Embedding exists: True\n",
      "   LogReg exists: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression # Kita butuh ini untuk type-checking\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "# Notebook ada di folder scripts/, jadi naik 1 level ke root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "LR_PATH = os.path.join(MODEL_DIR, 'logisticregression (3).pkl') # Model Teman Anda\n",
    "\n",
    "# URUTAN FITUR WAJIB (Sesuai isi file .pkl teman Anda)\n",
    "LR_FEATURES = ['sbert_sim', 'overlap_score', 'jaccard_score', 'bm25_score'] \n",
    "\n",
    "print(\"âœ… Path berhasil diset.\")\n",
    "print(f\"ğŸ“‚ Root Dir: {ROOT_DIR}\")\n",
    "print(f\"ğŸ“‚ Data Dir: {DATA_DIR}\")\n",
    "print(f\"\\nğŸ” Checking files:\")\n",
    "print(f\"   CSV exists: {os.path.exists(CSV_PATH)}\")\n",
    "print(f\"   - Path: {CSV_PATH}\")\n",
    "print(f\"   Embedding exists: {os.path.exists(EMB_FILE)}\")\n",
    "print(f\"   LogReg exists: {os.path.exists(LR_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7340b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Dataset Lengkap...\n",
      "âœ… Data Termuat: 6031 dokumen tafsir unik.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Dataset Lengkap...\")\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"File {CSV_PATH} tidak ditemukan!\")\n",
    "\n",
    "# 1. Baca CSV\n",
    "df_full = pd.read_csv(CSV_PATH)\n",
    "df_full.columns = df_full.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. Ambil Tafsir Unik (Corpus Pencarian)\n",
    "df_index = df_full.drop_duplicates(subset=['text']).copy()\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "# 3. Buat Kamus Metadata\n",
    "metadata_map = {}\n",
    "for _, row in df_index.iterrows():\n",
    "    key = str(row['text']).strip()\n",
    "    metadata_map[key] = {\n",
    "        'lokasi': row.get('ayat_asal', '?'),\n",
    "        'arabic': row.get('arabic', ''),\n",
    "        'trans': row.get('translation', '')\n",
    "    }\n",
    "\n",
    "print(f\"âœ… Data Termuat: {len(unique_tafsirs)} dokumen tafsir unik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec3d0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Embedding & SBERT...\n",
      "âœ… Embedding Lama Berhasil Diload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT Loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Embedding & SBERT...\")\n",
    "\n",
    "# 1. Load Embedding Lama (.pt)\n",
    "if os.path.exists(EMB_FILE):\n",
    "    corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "    print(f\"âœ… Embedding Lama Berhasil Diload.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"File corpus_embeddings.pt hilang!\")\n",
    "\n",
    "# 2. Load SBERT (Hanya untuk encode query)\n",
    "SBERT_FOLDER = os.path.join(MODEL_DIR, 'sbert_finetuned_quran')\n",
    "try:\n",
    "    sbert_model = SentenceTransformer(SBERT_FOLDER, device='cpu')\n",
    "    print(\"âœ… SBERT Loaded.\")\n",
    "except:\n",
    "    sbert_model = SentenceTransformer(SBERT_FOLDER, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cf1ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Model Regresi Logistik...\n",
      "âœ… Model LR berhasil dimuat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Model Regresi Logistik...\")\n",
    "\n",
    "if not os.path.exists(LR_PATH):\n",
    "    raise FileNotFoundError(f\"Model {LR_PATH} tidak ditemukan.\")\n",
    "\n",
    "# 1. Load Model\n",
    "try:\n",
    "    # LR biasanya disimpan dengan joblib, atau kadang dengan pickle biasa\n",
    "    try:\n",
    "        lr_model = joblib.load(LR_PATH)\n",
    "    except:\n",
    "        with open(LR_PATH, 'rb') as f:\n",
    "            lr_model = pickle.load(f)\n",
    "            \n",
    "    print(\"âœ… Model LR berhasil dimuat.\")\n",
    "    \n",
    "    # Cek apakah objek yang diload benar-benar model LR\n",
    "    if not isinstance(lr_model, LogisticRegression):\n",
    "         print(f\"âš ï¸ Objek yang diload bukan LogisticRegression, melainkan {type(lr_model)}. Hasil prediksi mungkin error.\")\n",
    "         \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Gagal memuat model LR: {e}\")\n",
    "    raise SystemExit(\"Model LR gagal dimuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171af849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Menyiapkan BM25...\n",
      "âœ… BM25 Siap.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Menyiapkan BM25...\")\n",
    "\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "except:\n",
    "    stop_words = set(['yang', 'dan', 'di'])\n",
    "\n",
    "def clean_tokens(text):\n",
    "    text = str(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stop_words]\n",
    "\n",
    "# Build Index BM25\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "# Pastikan fungsi clean_tokens dapat diakses di cell lain\n",
    "clean_func = clean_tokens \n",
    "\n",
    "print(\"âœ… BM25 Siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76922477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cari_regresi_logistik(query_text, threshold=0.5): # Threshold default 0.5 untuk LR\n",
    "    print(f\"\\nğŸ” QUERY: '{query_text}'\")\n",
    "    print(f\"âš™ï¸ Threshold yang digunakan: {threshold:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Retrieval (SBERT)\n",
    "    query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0]\n",
    "    \n",
    "    candidates = []\n",
    "    q_toks = clean_func(query_text)\n",
    "    \n",
    "    # 2. Hitung Fitur\n",
    "    for hit in hits:\n",
    "        idx = hit['corpus_id']\n",
    "        txt = unique_tafsirs[idx]\n",
    "        \n",
    "        # Hitung Fitur\n",
    "        t_toks = clean_func(txt)\n",
    "        sq, st = set(q_toks), set(t_toks)\n",
    "        ov = len(sq & st) / len(sq) if sq else 0\n",
    "        jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "        bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "        \n",
    "        candidates.append({\n",
    "            'text_key': txt,\n",
    "            'sbert_sim': hit['score'],\n",
    "            'bm25_score': bm25_s,\n",
    "            'overlap_score': ov,\n",
    "            'jaccard_score': jac\n",
    "        })\n",
    "    \n",
    "    # 3. Prediksi dengan Regresi Logistik\n",
    "    df_cand = pd.DataFrame(candidates)\n",
    "    \n",
    "    # SUSUN KOLOM SESUAI URUTAN MODEL TEMAN ANDA (WAJIB!)\n",
    "    X_pred = df_cand[LR_FEATURES]\n",
    "    \n",
    "    # Regresi Logistik predict_proba (ambil probabilitas kelas 1)\n",
    "    try:\n",
    "        # Paling umum, ambil probabilitas kelas 1\n",
    "        scores = lr_model.predict_proba(X_pred)[:, 1]\n",
    "    except AttributeError:\n",
    "        # Jika model dilatih tanpa predict_proba (jarang untuk LR)\n",
    "        scores = lr_model.predict(X_pred)\n",
    "    \n",
    "    df_cand['final_score'] = scores\n",
    "    \n",
    "    # 4. Filter & Tampilkan\n",
    "    results = df_cand[df_cand['final_score'] > threshold].sort_values('final_score', ascending=False)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(\"âŒ Tidak ada hasil relevan (Coba turunkan threshold).\")\n",
    "    else:\n",
    "        print(f\"âœ… Top 3 Hasil (Engine: Regresi Logistik):\\n\")\n",
    "        \n",
    "        for i, row in enumerate(results.head(3).iterrows()):\n",
    "            _, data = row\n",
    "            key = str(data['text_key']).strip()\n",
    "            score = data['final_score']\n",
    "            info = metadata_map.get(key)\n",
    "            \n",
    "            print(f\"ğŸ… [RANK {i+1}] Skor: {score:.4f}\")\n",
    "            if info:\n",
    "                print(f\"ğŸ“ {info['lokasi']}\")\n",
    "                print(f\"ğŸ‡¸ğŸ‡¦ {info['arabic'][:60]}...\")\n",
    "                print(f\"ğŸ‡®ğŸ‡© {info['trans'][:100]}...\")\n",
    "            print(f\"ğŸ’¬ TAFSIR: {key[:100]}...\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cea6690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” QUERY: 'Kisah sapi betina bani israil'\n",
      "âš™ï¸ Threshold yang digunakan: 0.5000\n",
      "============================================================\n",
      "âŒ Tidak ada hasil relevan (Coba turunkan threshold).\n",
      "\n",
      "ğŸ” QUERY: 'Hukum warisan bagi perempuan'\n",
      "âš™ï¸ Threshold yang digunakan: 0.5000\n",
      "============================================================\n",
      "âœ… Top 3 Hasil (Engine: Regresi Logistik):\n",
      "\n",
      "ğŸ… [RANK 1] Skor: 0.7355\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 11\n",
      "ğŸ‡¸ğŸ‡¦ ÙŠÙÙˆÙ’ØµÙÙŠÙ’ÙƒÙÙ…Ù Ø§Ù„Ù„Ù‘Ù°Ù‡Ù ÙÙÙŠÙ’Ù“ Ø§ÙÙˆÙ’Ù„ÙØ§Ø¯ÙÙƒÙÙ…Ù’ Ù„ÙÙ„Ø°Ù‘ÙÙƒÙØ±Ù Ù…ÙØ«Ù’Ù„Ù Ø­...\n",
      "ğŸ‡®ğŸ‡© Allah mensyariatkan (mewajibkan) kepadamu tentang (pembagian warisan untuk) anak-anakmu, (yaitu) bag...\n",
      "ğŸ’¬ TAFSIR: Setelah ayat sebelumnya menjelaskan dampak orang yang mengabaikan hak orang lain, ayat ini menjelask...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 2] Skor: 0.6486\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 127\n",
      "ğŸ‡¸ğŸ‡¦ ÙˆÙÙŠÙØ³Ù’ØªÙÙÙ’ØªÙÙˆÙ’Ù†ÙÙƒÙ ÙÙÙ‰ Ø§Ù„Ù†Ù‘ÙØ³ÙØ§Û¤Ø¡ÙÛ—  Ù‚ÙÙ„Ù Ø§Ù„Ù„Ù‘Ù°Ù‡Ù ÙŠÙÙÙ’ØªÙÙŠÙ’ÙƒÙ...\n",
      "ğŸ‡®ğŸ‡© Mereka meminta fatwa kepada engkau (Nabi Muhammad) tentang perempuan. Katakanlah, â€œAllah memberi fat...\n",
      "ğŸ’¬ TAFSIR: Dan mereka meminta fatwa, yaitu penjelasan hukum, kepadamu, wahai Muhammad, tentang berbagai persoal...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 3] Skor: 0.5451\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 15\n",
      "ğŸ‡¸ğŸ‡¦ ÙˆÙØ§Ù„Ù‘Ù°ØªÙÙŠÙ’ ÙŠÙØ£Ù’ØªÙÙŠÙ’Ù†Ù Ø§Ù„Ù’ÙÙØ§Ø­ÙØ´ÙØ©Ù Ù…ÙÙ†Ù’ Ù†Ù‘ÙØ³ÙØ§Û¤Ù‰Ù•ÙÙƒÙÙ…Ù’ ÙÙØ§Ø³Ù’...\n",
      "ğŸ‡®ğŸ‡© Para wanita yang melakukan perbuatan keji148) di antara wanita-wanita kamu, maka mintalah kesaksian ...\n",
      "ğŸ’¬ TAFSIR: Setelah Allah menjelaskan peringatan bagi pelanggar ketentuan Allah terkait dengan kewarisan, selanj...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cari_regresi_logistik(\"Kisah sapi betina bani israil\")\n",
    "cari_regresi_logistik(\"Hukum warisan bagi perempuan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c538717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
