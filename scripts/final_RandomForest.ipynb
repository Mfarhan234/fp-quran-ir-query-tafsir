{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b45cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Path berhasil diset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib # Untuk load file .pkl\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'models')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "RF_PATH = os.path.join(MODEL_DIR, 'randomforest (2).pkl') # Model Teman Anda\n",
    "\n",
    "# URUTAN FITUR STANDAR UNTUK RANDOM FOREST (asumsi urutan standar)\n",
    "RF_FEATURES = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score'] \n",
    "\n",
    "print(\"âœ… Path berhasil diset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73462f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Dataset Lengkap...\n",
      "âœ… Data Termuat: 6031 dokumen tafsir unik.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Dataset Lengkap...\")\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"File {CSV_PATH} tidak ditemukan!\")\n",
    "\n",
    "# 1. Baca CSV\n",
    "df_full = pd.read_csv(CSV_PATH)\n",
    "df_full.columns = df_full.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. Ambil Tafsir Unik (Corpus Pencarian)\n",
    "df_index = df_full.drop_duplicates(subset=['text']).copy()\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "# 3. Buat Kamus Metadata\n",
    "metadata_map = {}\n",
    "for _, row in df_index.iterrows():\n",
    "    key = str(row['text']).strip()\n",
    "    metadata_map[key] = {\n",
    "        'lokasi': row.get('ayat_asal', '?'),\n",
    "        'arabic': row.get('arabic', ''),\n",
    "        'trans': row.get('translation', '')\n",
    "    }\n",
    "\n",
    "print(f\"âœ… Data Termuat: {len(unique_tafsirs)} dokumen tafsir unik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c54847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Embedding & SBERT...\n",
      "âœ… Embedding Lama Berhasil Diload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT Loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Embedding & SBERT...\")\n",
    "\n",
    "# 1. Load Embedding Lama (.pt)\n",
    "if os.path.exists(EMB_FILE):\n",
    "    corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "    print(f\"âœ… Embedding Lama Berhasil Diload.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"File corpus_embeddings.pt hilang!\")\n",
    "\n",
    "# 2. Load SBERT (Hanya untuk encode query)\n",
    "SBERT_FOLDER = os.path.join(MODEL_DIR, 'sbert_finetuned_quran')\n",
    "try:\n",
    "    sbert_model = SentenceTransformer(SBERT_FOLDER, device='cpu')\n",
    "    print(\"âœ… SBERT Loaded.\")\n",
    "except:\n",
    "    sbert_model = SentenceTransformer(SBERT_FOLDER, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0c3e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Model Random Forest...\n",
      "âŒ Gagal memuat model RF: node array from the pickle has an incompatible dtype:\n",
      "- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n",
      "- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Model RF gagal dimuat. Cek instalasi joblib/sklearn Anda.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Model RF gagal dimuat. Cek instalasi joblib/sklearn Anda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farhan\\AppData\\Roaming\\Python\\Python314\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Model Random Forest...\")\n",
    "\n",
    "if not os.path.exists(RF_PATH):\n",
    "    raise FileNotFoundError(f\"Model {RF_PATH} tidak ditemukan.\")\n",
    "\n",
    "try:\n",
    "    # Coba load pakai joblib (paling umum untuk model sklearn)\n",
    "    rf_model = joblib.load(RF_PATH)\n",
    "    print(\"âœ… Model RF berhasil dimuat via Joblib.\")\n",
    "except Exception as e:\n",
    "    # Fallback/Error handling jika gagal\n",
    "    print(f\"âŒ Gagal memuat model RF: {e}\")\n",
    "    raise SystemExit(\"Model RF gagal dimuat. Cek instalasi joblib/sklearn Anda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0db3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ MEMULAI TRAINING RANDOM FOREST...\n",
      "   -> Data Siap: 170372 baris.\n",
      "   -> Fitur Digunakan: ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score']\n",
      "âš™ï¸ Melatih Model Random Forest...\n",
      "   -> Training Selesai.\n",
      "ğŸ“Š Evaluasi Model & Mencari Threshold Optimal...\n",
      "   -> Threshold Optimal (Target Recall 85.0%): 0.2943\n",
      "   -> Recall Dicapai: 0.8501\n",
      "   -> Precision: 0.3682\n",
      "   -> ROC-AUC Score: 0.8067\n",
      "âœ… MODEL DISIMPAN: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\randomforest_custom.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    roc_auc_score, \n",
    "    precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# --- 1. SETUP & LOAD DATA ---\n",
    "if 'ROOT_DIR' not in locals(): ROOT_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "print(\"ğŸš€ MEMULAI TRAINING RANDOM FOREST...\")\n",
    "\n",
    "# Load Data Matang\n",
    "df = pd.read_csv(DATA_PATH, on_bad_lines='skip')\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('\"', '')\n",
    "\n",
    "# Definisi Fitur (Diambil langsung dari CSV)\n",
    "FEATURES = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score']\n",
    "TARGET = 'label'\n",
    "TARGET_RECALL = 0.85 # Target kita adalah 85% kelengkapan (Recall)\n",
    "\n",
    "# Preprocessing: Pastikan tipe data benar dan tidak ada NaN\n",
    "for col in FEATURES + [TARGET]:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna(subset=FEATURES + [TARGET])\n",
    "\n",
    "print(f\"   -> Data Siap: {len(df)} baris.\")\n",
    "print(f\"   -> Fitur Digunakan: {FEATURES}\")\n",
    "\n",
    "# --- 2. SPLIT DATA ---\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 3. TRAINING RANDOM FOREST ---\n",
    "print(\"âš™ï¸ Melatih Model Random Forest...\")\n",
    "\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=300, # Jumlah pohon yang cukup besar\n",
    "    max_depth=10,     # Batasan kedalaman\n",
    "    min_samples_leaf=5, # Mencegah overfitting\n",
    "    class_weight='balanced', # KUNCI UTAMA: Menangani ketidakseimbangan kelas\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # Gunakan semua core CPU\n",
    ")\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "print(\"   -> Training Selesai.\")\n",
    "\n",
    "# --- 4. EVALUASI & MENCARI THRESHOLD OPTIMAL ---\n",
    "print(\"ğŸ“Š Evaluasi Model & Mencari Threshold Optimal...\")\n",
    "\n",
    "y_prob = model_rf.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# Cari Threshold Terbaik untuk Target Recall\n",
    "valid_indices = np.where(recall >= TARGET_RECALL)[0]\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "    idx = valid_indices[-1]\n",
    "    best_thresh = thresholds[idx]\n",
    "    rec_at_thresh = recall[idx]\n",
    "    prec_at_thresh = precision[idx]\n",
    "else:\n",
    "    best_thresh = 0.5\n",
    "    rec_at_thresh = 0\n",
    "    prec_at_thresh = 0\n",
    "\n",
    "print(f\"   -> Threshold Optimal (Target Recall {TARGET_RECALL*100}%): {best_thresh:.4f}\")\n",
    "print(f\"   -> Recall Dicapai: {rec_at_thresh:.4f}\")\n",
    "print(f\"   -> Precision: {prec_at_thresh:.4f}\")\n",
    "print(f\"   -> ROC-AUC Score: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# --- 5. SIMPAN MODEL & THRESHOLD ---\n",
    "model_file = os.path.join(MODEL_DIR, 'randomforest_custom.pkl')\n",
    "thresh_file = os.path.join(MODEL_DIR, 'threshold_rf.txt')\n",
    "\n",
    "# Simpan model Random Forest (Joblib lebih disarankan untuk sklearn)\n",
    "joblib.dump(model_rf, model_file)\n",
    "with open(thresh_file, 'w') as f:\n",
    "    f.write(str(best_thresh))\n",
    "\n",
    "print(f\"âœ… MODEL DISIMPAN: {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3046d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Path berhasil diset. Siap menguji model custom.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'models')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "RF_PATH = os.path.join(MODEL_DIR, 'randomforest_custom.pkl') # MODEL CUSTOM ANDA\n",
    "RF_THRESHOLD_FILE = os.path.join(MODEL_DIR, 'threshold_rf.txt')\n",
    "\n",
    "# URUTAN FITUR WAJIB (Sesuai saat training)\n",
    "RF_FEATURES = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score'] \n",
    "\n",
    "print(\"âœ… Path berhasil diset. Siap menguji model custom.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f36fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Dataset Lengkap...\n",
      "âœ… Data Termuat: 6031 dokumen tafsir unik.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Dataset Lengkap...\")\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"File {CSV_PATH} tidak ditemukan!\")\n",
    "\n",
    "# 1. Baca CSV\n",
    "df_full = pd.read_csv(CSV_PATH)\n",
    "df_full.columns = df_full.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. Ambil Tafsir Unik (Corpus Pencarian)\n",
    "df_index = df_full.drop_duplicates(subset=['text']).copy()\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "# 3. Buat Kamus Metadata\n",
    "metadata_map = {}\n",
    "for _, row in df_index.iterrows():\n",
    "    key = str(row['text']).strip()\n",
    "    metadata_map[key] = {\n",
    "        'lokasi': row.get('ayat_asal', '?'),\n",
    "        'arabic': row.get('arabic', ''),\n",
    "        'trans': row.get('translation', '')\n",
    "    }\n",
    "\n",
    "print(f\"âœ… Data Termuat: {len(unique_tafsirs)} dokumen tafsir unik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80b3767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Embedding & SBERT...\n",
      "âœ… Embedding Lama Berhasil Diload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT Loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Embedding & SBERT...\")\n",
    "\n",
    "# 1. Load Embedding Lama (.pt)\n",
    "if os.path.exists(EMB_FILE):\n",
    "    corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "    print(f\"âœ… Embedding Lama Berhasil Diload.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"File corpus_embeddings.pt hilang!\")\n",
    "\n",
    "# 2. Load SBERT (Hanya untuk encode query)\n",
    "SBERT_FOLDER = os.path.join(MODEL_DIR, 'sbert_finetuned_quran')\n",
    "try:\n",
    "    sbert_model = SentenceTransformer(SBERT_FOLDER, device='cpu')\n",
    "    print(\"âœ… SBERT Loaded.\")\n",
    "except:\n",
    "    sbert_model = SentenceTransformer(SBERT_FOLDER, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5997a830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Model Random Forest Custom...\n",
      "âœ… Model RF berhasil dimuat via Joblib.\n",
      "âœ… Threshold Optimal Diload: 0.2943\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Model Random Forest Custom...\")\n",
    "\n",
    "if not os.path.exists(RF_PATH):\n",
    "    raise FileNotFoundError(f\"Model {RF_PATH} tidak ditemukan.\")\n",
    "    \n",
    "# 1. Load Model\n",
    "try:\n",
    "    rf_model = joblib.load(RF_PATH)\n",
    "    print(\"âœ… Model RF berhasil dimuat via Joblib.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Gagal memuat model RF: {e}\")\n",
    "    raise SystemExit(\"Model RF gagal dimuat.\")\n",
    "    \n",
    "# 2. Load Threshold Optimal\n",
    "if os.path.exists(RF_THRESHOLD_FILE):\n",
    "    with open(RF_THRESHOLD_FILE, 'r') as f:\n",
    "        RF_THRESHOLD = float(f.read().strip())\n",
    "    print(f\"âœ… Threshold Optimal Diload: {RF_THRESHOLD:.4f}\")\n",
    "else:\n",
    "    RF_THRESHOLD = 0.5\n",
    "    print(\"âš ï¸ Threshold file hilang. Menggunakan default 0.5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e71c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Menyiapkan BM25...\n",
      "âœ… BM25 Siap.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Menyiapkan BM25...\")\n",
    "\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "except:\n",
    "    stop_words = set(['yang', 'dan', 'di'])\n",
    "\n",
    "def clean_tokens(text):\n",
    "    text = str(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stop_words]\n",
    "\n",
    "# Build Index BM25\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "# Pastikan fungsi clean_tokens dapat diakses di cell lain\n",
    "clean_func = clean_tokens \n",
    "\n",
    "print(\"âœ… BM25 Siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe2831b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cari_random_forest(query_text, threshold=RF_THRESHOLD):\n",
    "    print(f\"\\nğŸ” QUERY: '{query_text}'\")\n",
    "    print(f\"âš™ï¸ Threshold yang digunakan: {threshold:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Retrieval (SBERT)\n",
    "    query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0]\n",
    "    \n",
    "    candidates = []\n",
    "    q_toks = clean_func(query_text)\n",
    "    \n",
    "    for hit in hits:\n",
    "        idx = hit['corpus_id']\n",
    "        txt = unique_tafsirs[idx]\n",
    "        \n",
    "        # Hitung Fitur\n",
    "        t_toks = clean_func(txt)\n",
    "        sq, st = set(q_toks), set(t_toks)\n",
    "        ov = len(sq & st) / len(sq) if sq else 0\n",
    "        jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "        bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "        \n",
    "        candidates.append({\n",
    "            'text_key': txt,\n",
    "            'sbert_sim': hit['score'],\n",
    "            'bm25_score': bm25_s,\n",
    "            'overlap_score': ov,\n",
    "            'jaccard_score': jac\n",
    "        })\n",
    "    \n",
    "    # 2. Prediksi dengan Random Forest\n",
    "    df_cand = pd.DataFrame(candidates)\n",
    "    \n",
    "    # Susun kolom sesuai urutan training\n",
    "    X_pred = df_cand[RF_FEATURES]\n",
    "    \n",
    "    # Random Forest predict_proba (ambil probabilitas kelas 1)\n",
    "    try:\n",
    "        scores = rf_model.predict_proba(X_pred)[:, 1]\n",
    "    except AttributeError:\n",
    "        # Jika model dilatih sebagai regresi atau format beda\n",
    "        scores = rf_model.predict(X_pred)\n",
    "    \n",
    "    df_cand['final_score'] = scores\n",
    "    \n",
    "    # 3. Filter & Tampilkan\n",
    "    results = df_cand[df_cand['final_score'] > threshold].sort_values('final_score', ascending=False)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(\"âŒ Tidak ada hasil relevan (Coba turunkan threshold).\")\n",
    "    else:\n",
    "        print(f\"âœ… Top 3 Hasil (Engine: Random Forest Custom):\\n\")\n",
    "        \n",
    "        for i, row in enumerate(results.head(3).iterrows()):\n",
    "            _, data = row\n",
    "            key = str(data['text_key']).strip()\n",
    "            score = data['final_score']\n",
    "            info = metadata_map.get(key)\n",
    "            \n",
    "            print(f\"ğŸ… [RANK {i+1}] Skor: {score:.4f}\")\n",
    "            if info:\n",
    "                print(f\"ğŸ“ {info['lokasi']}\")\n",
    "                print(f\"ğŸ‡¸ğŸ‡¦ {info['arabic'][:60]}...\")\n",
    "                print(f\"ğŸ‡®ğŸ‡© {info['trans'][:100]}...\")\n",
    "            print(f\"ğŸ’¬ TAFSIR: {key[:100]}...\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94138ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” QUERY: 'Kisah sapi betina bani israil'\n",
      "âš™ï¸ Threshold yang digunakan: 0.2943\n",
      "============================================================\n",
      "âœ… Top 3 Hasil (Engine: Random Forest Custom):\n",
      "\n",
      "ğŸ… [RANK 1] Skor: 0.8939\n",
      "ğŸ“ QS. á¹¬ÄhÄ : Ayat 96\n",
      "ğŸ‡¸ğŸ‡¦ Ù‚ÙØ§Ù„Ù Ø¨ÙØµÙØ±Ù’ØªÙ Ø¨ÙÙ…ÙØ§ Ù„ÙÙ…Ù’ ÙŠÙØ¨Ù’ØµÙØ±ÙÙˆÙ’Ø§ Ø¨ÙÙ‡Ù– ÙÙÙ‚ÙØ¨ÙØ¶Ù’ØªÙ Ù‚ÙØ¨Ù’Ø¶Ù...\n",
      "ğŸ‡®ğŸ‡© Dia (Samiri) menjawab, â€œAku melihat sesuatu yang tidak mereka lihat. Kemudian, aku ambil segenggam (...\n",
      "ğŸ’¬ TAFSIR: Mendapat pertanyaan dari Nabi Musa, dia menjawab, â€œAku melihat dan mengetahui sesuatu yang tidak mer...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 2] Skor: 0.8514\n",
      "ğŸ“ QS. Al-Baqarah  : Ayat 51\n",
      "ğŸ‡¸ğŸ‡¦ ÙˆÙØ§ÙØ°Ù’ ÙˆÙ°Ø¹ÙØ¯Ù’Ù†ÙØ§ Ù…ÙÙˆÙ’Ø³Ù°Ù‰Ù“ Ø§ÙØ±Ù’Ø¨ÙØ¹ÙÙŠÙ’Ù†Ù Ù„ÙÙŠÙ’Ù„ÙØ©Ù‹ Ø«ÙÙ…Ù‘Ù Ø§ØªÙ‘ÙØ®Ù...\n",
      "ğŸ‡®ğŸ‡© (Ingatlah) ketika Kami menjanjikan (petunjuk Taurat) kepada Musa (melalui munajat selama) empat pulu...\n",
      "ğŸ’¬ TAFSIR: Setelah menerima nikmat dalam bentuk penyelamatan dari dua bencana pembunuhan dan tenggelam di Laut ...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 3] Skor: 0.8396\n",
      "ğŸ“ QS. Al-MÄ'idah : Ayat 72\n",
      "ğŸ‡¸ğŸ‡¦ Ù„ÙÙ‚ÙØ¯Ù’ ÙƒÙÙÙØ±Ù Ø§Ù„Ù‘ÙØ°ÙÙŠÙ’Ù†Ù Ù‚ÙØ§Ù„ÙÙˆÙ’Ù“Ø§ Ø§ÙÙ†Ù‘Ù Ø§Ù„Ù„Ù‘Ù°Ù‡Ù Ù‡ÙÙˆÙ Ø§Ù„Ù’Ù…ÙØ³...\n",
      "ğŸ‡®ğŸ‡© Sungguh, telah kufur orang-orang yang berkata, â€œSesungguhnya Allah itulah Almasih putra Maryam.â€ Alm...\n",
      "ğŸ’¬ TAFSIR: Bila pada ayat-ayat yang lalu diterangkan tentang penyimpanganpenyimpangan yang dilakukan orang Yahu...\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ” QUERY: 'Hukum warisan bagi perempuan'\n",
      "âš™ï¸ Threshold yang digunakan: 0.2943\n",
      "============================================================\n",
      "âœ… Top 3 Hasil (Engine: Random Forest Custom):\n",
      "\n",
      "ğŸ… [RANK 1] Skor: 0.8690\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 11\n",
      "ğŸ‡¸ğŸ‡¦ ÙŠÙÙˆÙ’ØµÙÙŠÙ’ÙƒÙÙ…Ù Ø§Ù„Ù„Ù‘Ù°Ù‡Ù ÙÙÙŠÙ’Ù“ Ø§ÙÙˆÙ’Ù„ÙØ§Ø¯ÙÙƒÙÙ…Ù’ Ù„ÙÙ„Ø°Ù‘ÙÙƒÙØ±Ù Ù…ÙØ«Ù’Ù„Ù Ø­...\n",
      "ğŸ‡®ğŸ‡© Allah mensyariatkan (mewajibkan) kepadamu tentang (pembagian warisan untuk) anak-anakmu, (yaitu) bag...\n",
      "ğŸ’¬ TAFSIR: Setelah ayat sebelumnya menjelaskan dampak orang yang mengabaikan hak orang lain, ayat ini menjelask...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 2] Skor: 0.7995\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 127\n",
      "ğŸ‡¸ğŸ‡¦ ÙˆÙÙŠÙØ³Ù’ØªÙÙÙ’ØªÙÙˆÙ’Ù†ÙÙƒÙ ÙÙÙ‰ Ø§Ù„Ù†Ù‘ÙØ³ÙØ§Û¤Ø¡ÙÛ—  Ù‚ÙÙ„Ù Ø§Ù„Ù„Ù‘Ù°Ù‡Ù ÙŠÙÙÙ’ØªÙÙŠÙ’ÙƒÙ...\n",
      "ğŸ‡®ğŸ‡© Mereka meminta fatwa kepada engkau (Nabi Muhammad) tentang perempuan. Katakanlah, â€œAllah memberi fat...\n",
      "ğŸ’¬ TAFSIR: Dan mereka meminta fatwa, yaitu penjelasan hukum, kepadamu, wahai Muhammad, tentang berbagai persoal...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 3] Skor: 0.7350\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 8\n",
      "ğŸ‡¸ğŸ‡¦ ÙˆÙØ§ÙØ°ÙØ§ Ø­ÙØ¶ÙØ±Ù Ø§Ù„Ù’Ù‚ÙØ³Ù’Ù…ÙØ©Ù Ø§ÙÙˆÙ„ÙÙˆØ§ Ø§Ù„Ù’Ù‚ÙØ±Ù’Ø¨Ù°Ù‰ ÙˆÙØ§Ù„Ù’ÙŠÙØªÙ°Ù…Ù°Ù‰ Ùˆ...\n",
      "ğŸ‡®ğŸ‡© Apabila (saat) pembagian itu hadir beberapa kerabat,144) anak-anak yatim, dan orang-orang miskin, be...\n",
      "ğŸ’¬ TAFSIR: Setelah menjelaskan ketentuan hak warisan bagi kaum perempuan, maka pada ayat ini Allah memberi peri...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cari_random_forest(\"Kisah sapi betina bani israil\")\n",
    "cari_random_forest(\"Hukum warisan bagi perempuan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fc86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
