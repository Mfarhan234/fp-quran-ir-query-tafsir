{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5ece15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ MEMUAT ENGINE LIGHTGBM...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset FULL_COMPLETE tidak ditemukan!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# --- 2. LOAD DATA & INDEX (SAMA SEPERTI SEBELUMNYA) ---\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Kita tetap pakai data dan embedding yang sama, cuma ganti \"Juri\"-nya saja.\u001b[39;00m\n\u001b[32m     36\u001b[39m \n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# A. Load Data Teks & Metadata\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(DATA_PATH):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset FULL_COMPLETE tidak ditemukan!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m df_full = pd.read_csv(DATA_PATH)\n\u001b[32m     42\u001b[39m df_full.columns = df_full.columns.str.strip().str.lower()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Dataset FULL_COMPLETE tidak ditemukan!"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# UJI COBA MANUAL: LIGHTGBM (TEMAN)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- 1. KONFIGURASI ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'models')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'dataset_training_FULL_COMPLETE.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "\n",
    "# Path ke model teman Anda (Pastikan file ini ada di folder models)\n",
    "LGBM_PATH = os.path.join(MODEL_DIR, 'lightgbm (2) (1).pkl')\n",
    "\n",
    "# URUTAN FITUR WAJIB (Sesuai isi perut file .pkl teman Anda)\n",
    "LGBM_FEATURES = ['sbert_sim', 'overlap_score', 'jaccard_score', 'bm25_score']\n",
    "\n",
    "print(\"âš™ï¸ MEMUAT ENGINE LIGHTGBM...\")\n",
    "\n",
    "# --- 2. LOAD DATA & INDEX (SAMA SEPERTI SEBELUMNYA) ---\n",
    "# Kita tetap pakai data dan embedding yang sama, cuma ganti \"Juri\"-nya saja.\n",
    "\n",
    "# A. Load Data Teks & Metadata\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(\"Dataset FULL_COMPLETE tidak ditemukan!\")\n",
    "\n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "df_full.columns = df_full.columns.str.strip().str.lower()\n",
    "df_index = df_full.drop_duplicates(subset=['text']).copy()\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "metadata_map = {}\n",
    "for _, row in df_index.iterrows():\n",
    "    key = str(row['text']).strip()\n",
    "    metadata_map[key] = {\n",
    "        'lokasi': row.get('ayat_asal', '?'),\n",
    "        'arabic': row.get('arabic', ''),\n",
    "        'trans': row.get('translation', '')\n",
    "    }\n",
    "\n",
    "print(f\"   -> Data Training: {len(unique_tafsirs)} dokumen.\")\n",
    "\n",
    "# B. Load Embedding Lama (.pt)\n",
    "if not os.path.exists(EMB_FILE):\n",
    "    raise FileNotFoundError(\"File corpus_embeddings.pt tidak ditemukan!\")\n",
    "corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "print(\"   -> Embedding Lama: Teruat.\")\n",
    "\n",
    "# C. Load Model LightGBM (Pickle)\n",
    "print(f\"   -> Memuat Model LightGBM: {os.path.basename(LGBM_PATH)}\")\n",
    "if not os.path.exists(LGBM_PATH):\n",
    "    raise FileNotFoundError(f\"File {LGBM_PATH} tidak ditemukan! Taruh di folder models dulu.\")\n",
    "\n",
    "try:\n",
    "    # Coba load pakai pickle biasa (karena format .pkl)\n",
    "    with open(LGBM_PATH, 'rb') as f:\n",
    "        lgbm_model = pickle.load(f)\n",
    "    print(\"      âœ… Berhasil load via Pickle.\")\n",
    "except:\n",
    "    # Fallback joblib\n",
    "    lgbm_model = joblib.load(LGBM_PATH)\n",
    "    print(\"      âœ… Berhasil load via Joblib.\")\n",
    "\n",
    "# D. Setup SBERT & BM25\n",
    "sbert_path = os.path.join(MODEL_DIR, 'sbert_finetuned_quran')\n",
    "try:\n",
    "    sbert_model = SentenceTransformer(sbert_path, device='cpu', trust_remote_code=True)\n",
    "except:\n",
    "    sbert_model = SentenceTransformer(sbert_path, device='cpu')\n",
    "\n",
    "try: nltk.download('stopwords', quiet=True); stop_words = set(stopwords.words('indonesian'))\n",
    "except: stop_words = set(['yang', 'dan', 'di'])\n",
    "\n",
    "def clean_tokens(text):\n",
    "    text = str(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stop_words]\n",
    "\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "print(\"\\nâœ… SIAP TESTING LIGHTGBM!\")\n",
    "\n",
    "# --- 3. FUNGSI PENCARIAN (KHUSUS LIGHTGBM) ---\n",
    "def cari_pake_lgbm(query_text, threshold=0.1): # LGBM kadang butuh threshold lebih rendah/tinggi tergantung training\n",
    "    print(f\"\\nğŸ” QUERY: '{query_text}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Encode Query\n",
    "    query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0]\n",
    "    \n",
    "    candidates = []\n",
    "    q_toks = clean_tokens(query_text)\n",
    "    \n",
    "    for hit in hits:\n",
    "        idx = hit['corpus_id']\n",
    "        txt = unique_tafsirs[idx]\n",
    "        \n",
    "        # Hitung Fitur\n",
    "        t_toks = corpus_tokens[idx]\n",
    "        sq, st = set(q_toks), set(t_toks)\n",
    "        ov = len(sq & st) / len(sq) if sq else 0\n",
    "        jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "        bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "        \n",
    "        candidates.append({\n",
    "            'text_key': txt,\n",
    "            'sbert_sim': hit['score'],\n",
    "            'bm25_score': bm25_s,\n",
    "            'overlap_score': ov,\n",
    "            'jaccard_score': jac\n",
    "        })\n",
    "    \n",
    "    # 2. Re-Ranking pakai LightGBM\n",
    "    df_cand = pd.DataFrame(candidates)\n",
    "    \n",
    "    # SUSUN KOLOM SESUAI URUTAN MODEL TEMAN ANDA (WAJIB!)\n",
    "    X_pred = df_cand[LGBM_FEATURES]\n",
    "    \n",
    "    # Prediksi\n",
    "    try:\n",
    "        # Jika model klasifikasi (keluar probabilitas 0-1)\n",
    "        scores = lgbm_model.predict_proba(X_pred)[:, 1]\n",
    "    except:\n",
    "        # Jika model regresi (keluar skor mentah)\n",
    "        scores = lgbm_model.predict(X_pred)\n",
    "        \n",
    "    df_cand['final_score'] = scores\n",
    "    \n",
    "    # 3. Tampilkan\n",
    "    results = df_cand[df_cand['final_score'] > threshold].sort_values('final_score', ascending=False)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(\"âŒ Tidak ada hasil (Coba turunkan threshold).\")\n",
    "    else:\n",
    "        print(f\"âœ… Top 3 Hasil (Engine: LightGBM):\\n\")\n",
    "        for i, row in enumerate(results.head(3).iterrows()):\n",
    "            _, data = row\n",
    "            key = str(data['text_key']).strip()\n",
    "            score = data['final_score']\n",
    "            info = metadata_map.get(key)\n",
    "            \n",
    "            print(f\"ğŸ… [RANK {i+1}] Skor: {score:.4f}\")\n",
    "            if info:\n",
    "                print(f\"ğŸ“ {info['lokasi']}\")\n",
    "                print(f\"ğŸ‡¸ğŸ‡¦ {info['arabic'][:50]}...\")\n",
    "                print(f\"ğŸ‡®ğŸ‡© {info['trans'][:100]}...\")\n",
    "            print(f\"ğŸ’¬ {key[:100]}...\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "# --- 4. TEST ---\n",
    "cari_pake_lgbm(\"Kisah sapi betina bani israil\")\n",
    "cari_pake_lgbm(\"Hukum warisan bagi perempuan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8310cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Path berhasil diset.\n",
      "ğŸ“‚ Root Dir: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\n",
      "ğŸ“‚ Model Dir: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\n",
      "ğŸ“‚ Data Dir: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\n",
      "\n",
      "ğŸ” Checking files:\n",
      "   CSV exists: True\n",
      "   Embedding exists: True\n",
      "   LightGBM exists: True\n"
     ]
    }
   ],
   "source": [
    "# --- KONFIGURASI PATH ---\n",
    "# Notebook ada di folder scripts/, jadi naik 1 level ke root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "LGBM_PATH = os.path.join(MODEL_DIR, 'lightgbm (2) (1).pkl') # Model Teman Anda\n",
    "\n",
    "# URUTAN FITUR WAJIB (Sesuai model teman Anda)\n",
    "LGBM_FEATURES = ['sbert_sim', 'overlap_score', 'jaccard_score', 'bm25_score']\n",
    "\n",
    "print(\"âœ… Path berhasil diset.\")\n",
    "print(f\"ğŸ“‚ Root Dir: {ROOT_DIR}\")\n",
    "print(f\"ğŸ“‚ Model Dir: {MODEL_DIR}\")\n",
    "print(f\"ğŸ“‚ Data Dir: {DATA_DIR}\")\n",
    "print(f\"\\nğŸ” Checking files:\")\n",
    "print(f\"   CSV exists: {os.path.exists(CSV_PATH)}\")\n",
    "print(f\"   Embedding exists: {os.path.exists(EMB_FILE)}\")\n",
    "print(f\"   LightGBM exists: {os.path.exists(LGBM_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77bd170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Dataset Lengkap...\n",
      "âœ… Data Termuat: 6031 dokumen tafsir unik.\n",
      "âœ… Metadata Map Siap.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Dataset Lengkap...\")\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"File {CSV_PATH} tidak ditemukan!\")\n",
    "\n",
    "# 1. Baca CSV\n",
    "df_full = pd.read_csv(CSV_PATH)\n",
    "df_full.columns = df_full.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. Ambil Tafsir Unik (Untuk Pencarian)\n",
    "# Urutan ini harus SAMA PERSIS dengan urutan saat corpus_embeddings.pt dibuat\n",
    "df_index = df_full.drop_duplicates(subset=['text']).copy()\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "# 3. Buat Kamus Metadata (Untuk Tampilan Arab/Terjemah)\n",
    "metadata_map = {}\n",
    "for _, row in df_index.iterrows():\n",
    "    key = str(row['text']).strip()\n",
    "    metadata_map[key] = {\n",
    "        'lokasi': row.get('ayat_asal', '?'),\n",
    "        'arabic': row.get('arabic', ''),\n",
    "        'trans': row.get('translation', '')\n",
    "    }\n",
    "\n",
    "print(f\"âœ… Data Termuat: {len(unique_tafsirs)} dokumen tafsir unik.\")\n",
    "print(\"âœ… Metadata Map Siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c47ea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Embedding & SBERT...\n",
      "âœ… Embedding Lama Berhasil Diload: 6031 vektor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT Finetuned Loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Embedding & SBERT...\")\n",
    "\n",
    "# 1. Load Embedding Lama (.pt)\n",
    "if os.path.exists(EMB_FILE):\n",
    "    corpus_embeddings = torch.load(EMB_FILE, map_location=torch.device('cpu'))\n",
    "    print(f\"âœ… Embedding Lama Berhasil Diload: {len(corpus_embeddings)} vektor.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"File corpus_embeddings.pt hilang! Anda harus punya file ini.\")\n",
    "\n",
    "# 2. Load SBERT (Hanya untuk encode query user nanti)\n",
    "SBERT_FOLDER = os.path.join(MODEL_DIR, 'sbert_finetuned_quran')\n",
    "try:\n",
    "    sbert_model = SentenceTransformer(SBERT_FOLDER, device='cpu', trust_remote_code=True)\n",
    "    print(\"âœ… SBERT Finetuned Loaded.\")\n",
    "except:\n",
    "    sbert_model = SentenceTransformer(SBERT_FOLDER, device='cpu')\n",
    "    print(\"âœ… SBERT (Default) Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fed557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Memuat Model LightGBM...\n",
      "âœ… Model dimuat via Joblib.\n",
      "â„¹ï¸ Fitur Model: ['sbert_sim', 'overlap_score', 'jaccard_score', 'bm25_score']\n",
      "âš ï¸ PASTIKAN URUTAN FITUR KITA ADALAH: ['sbert_sim', 'overlap_score', 'jaccard_score', 'bm25_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Memuat Model LightGBM...\")\n",
    "\n",
    "if not os.path.exists(LGBM_PATH):\n",
    "    raise FileNotFoundError(f\"Model {LGBM_PATH} tidak ditemukan.\")\n",
    "\n",
    "try:\n",
    "    # Cara 1: Pakai Pickle biasa\n",
    "    with open(LGBM_PATH, 'rb') as f:\n",
    "        lgbm_model = pickle.load(f)\n",
    "    print(\"âœ… Model dimuat via Pickle.\")\n",
    "except:\n",
    "    try:\n",
    "        # Cara 2: Pakai Joblib\n",
    "        lgbm_model = joblib.load(LGBM_PATH)\n",
    "        print(\"âœ… Model dimuat via Joblib.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Gagal memuat model: {e}\")\n",
    "\n",
    "# Cek Fitur (Opsional, untuk memastikan urutan benar)\n",
    "try:\n",
    "    if hasattr(lgbm_model, 'feature_name_'):\n",
    "        print(f\"â„¹ï¸ Fitur Model: {lgbm_model.feature_name_}\")\n",
    "    elif hasattr(lgbm_model, 'booster_'):\n",
    "        print(f\"â„¹ï¸ Fitur Model: {lgbm_model.booster_.feature_name()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"âš ï¸ PASTIKAN URUTAN FITUR KITA ADALAH: {LGBM_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896597c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Menyiapkan BM25...\n",
      "âœ… BM25 Siap.\n"
     ]
    }
   ],
   "source": [
    "print(\"âš™ï¸ Menyiapkan BM25...\")\n",
    "\n",
    "# Setup Stopwords\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "except:\n",
    "    stop_words = set(['yang', 'dan', 'di'])\n",
    "\n",
    "def clean_tokens(text):\n",
    "    text = str(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stop_words]\n",
    "\n",
    "# Build Index\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "print(\"âœ… BM25 Siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be69a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cari_lightgbm(query_text, threshold=0.1):\n",
    "    print(f\"\\nğŸ” QUERY: '{query_text}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Retrieval (SBERT) - Cari 50 Kandidat Awal\n",
    "    query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0]\n",
    "    \n",
    "    candidates = []\n",
    "    q_toks = clean_tokens(query_text)\n",
    "    \n",
    "    # 2. Hitung Fitur (On-The-Fly)\n",
    "    for hit in hits:\n",
    "        idx = hit['corpus_id']\n",
    "        txt = unique_tafsirs[idx]\n",
    "        t_toks = corpus_tokens[idx]\n",
    "        \n",
    "        # Hitung skor-skor manual\n",
    "        sq, st = set(q_toks), set(t_toks)\n",
    "        ov = len(sq & st) / len(sq) if sq else 0\n",
    "        jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "        bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "        \n",
    "        candidates.append({\n",
    "            'text_key': txt,\n",
    "            'sbert_sim': hit['score'],\n",
    "            'bm25_score': bm25_s,\n",
    "            'overlap_score': ov,\n",
    "            'jaccard_score': jac\n",
    "        })\n",
    "    \n",
    "    # 3. Prediksi Pakai LightGBM\n",
    "    df_cand = pd.DataFrame(candidates)\n",
    "    \n",
    "    # WAJIB: Susun kolom sesuai urutan model teman Anda\n",
    "    X_pred = df_cand[LGBM_FEATURES]\n",
    "    \n",
    "    try:\n",
    "        # Klasifikasi (Probabilitas)\n",
    "        scores = lgbm_model.predict_proba(X_pred)[:, 1]\n",
    "    except:\n",
    "        # Regresi (Skor Langsung)\n",
    "        scores = lgbm_model.predict(X_pred)\n",
    "        \n",
    "    df_cand['final_score'] = scores\n",
    "    \n",
    "    # 4. Filter & Tampilkan\n",
    "    results = df_cand[df_cand['final_score'] > threshold].sort_values('final_score', ascending=False)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(\"âŒ Tidak ada hasil relevan.\")\n",
    "    else:\n",
    "        print(f\"âœ… Top 3 Hasil (Engine: LightGBM):\\n\")\n",
    "        for i, row in enumerate(results.head(3).iterrows()):\n",
    "            _, data = row\n",
    "            key = str(data['text_key']).strip()\n",
    "            score = data['final_score']\n",
    "            \n",
    "            # Ambil Info Lengkap (Arab/Terjemah)\n",
    "            info = metadata_map.get(key)\n",
    "            \n",
    "            print(f\"ğŸ… [RANK {i+1}] Skor: {score:.4f}\")\n",
    "            if info:\n",
    "                print(f\"ğŸ“ {info['lokasi']}\")\n",
    "                print(f\"ğŸ‡¸ğŸ‡¦ {info['arabic'][:60]}...\") # Dipotong dikit biar rapi di print\n",
    "                print(f\"ğŸ‡®ğŸ‡© {info['trans'][:100]}...\")\n",
    "            \n",
    "            print(f\"ğŸ’¬ TAFSIR: {key[:100]}...\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1746ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” QUERY: 'Kisah sapi betina bani israil'\n",
      "============================================================\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "âœ… Top 3 Hasil (Engine: LightGBM):\n",
      "\n",
      "ğŸ… [RANK 1] Skor: 0.8973\n",
      "ğŸ“ QS. á¹¬ÄhÄ : Ayat 96\n",
      "ğŸ‡¸ğŸ‡¦ Ù‚ÙØ§Ù„Ù Ø¨ÙØµÙØ±Ù’ØªÙ Ø¨ÙÙ…ÙØ§ Ù„ÙÙ…Ù’ ÙŠÙØ¨Ù’ØµÙØ±ÙÙˆÙ’Ø§ Ø¨ÙÙ‡Ù– ÙÙÙ‚ÙØ¨ÙØ¶Ù’ØªÙ Ù‚ÙØ¨Ù’Ø¶Ù...\n",
      "ğŸ‡®ğŸ‡© Dia (Samiri) menjawab, â€œAku melihat sesuatu yang tidak mereka lihat. Kemudian, aku ambil segenggam (...\n",
      "ğŸ’¬ TAFSIR: Mendapat pertanyaan dari Nabi Musa, dia menjawab, â€œAku melihat dan mengetahui sesuatu yang tidak mer...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 2] Skor: 0.8831\n",
      "ğŸ“ QS. Al-Aâ€˜rÄf : Ayat 105\n",
      "ğŸ‡¸ğŸ‡¦ Ø­ÙÙ‚ÙÙŠÙ’Ù‚ÙŒ Ø¹ÙÙ„Ù°Ù“Ù‰ Ø§ÙÙ†Ù’ Ù„Ù‘ÙØ§Ù“ Ø§ÙÙ‚ÙÙˆÙ’Ù„Ù Ø¹ÙÙ„ÙÙ‰ Ø§Ù„Ù„Ù‘Ù°Ù‡Ù Ø§ÙÙ„Ù‘ÙØ§ Ø§Ù„Ù’...\n",
      "ğŸ‡®ğŸ‡© Wajib atasku tidak mengatakan (sesuatu) terhadap Allah, kecuali yang hak (benar). Sungguh, aku datan...\n",
      "ğŸ’¬ TAFSIR: Sebagai seorang nabi dan rasul yang bertugas menyampaikan pesan Allah, aku wajib mengatakan yang seb...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 3] Skor: 0.8810\n",
      "ğŸ“ QS. Al-Baqarah  : Ayat 51\n",
      "ğŸ‡¸ğŸ‡¦ ÙˆÙØ§ÙØ°Ù’ ÙˆÙ°Ø¹ÙØ¯Ù’Ù†ÙØ§ Ù…ÙÙˆÙ’Ø³Ù°Ù‰Ù“ Ø§ÙØ±Ù’Ø¨ÙØ¹ÙÙŠÙ’Ù†Ù Ù„ÙÙŠÙ’Ù„ÙØ©Ù‹ Ø«ÙÙ…Ù‘Ù Ø§ØªÙ‘ÙØ®Ù...\n",
      "ğŸ‡®ğŸ‡© (Ingatlah) ketika Kami menjanjikan (petunjuk Taurat) kepada Musa (melalui munajat selama) empat pulu...\n",
      "ğŸ’¬ TAFSIR: Setelah menerima nikmat dalam bentuk penyelamatan dari dua bencana pembunuhan dan tenggelam di Laut ...\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ” QUERY: 'Hukum warisan bagi perempuan'\n",
      "============================================================\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "âœ… Top 3 Hasil (Engine: LightGBM):\n",
      "\n",
      "ğŸ… [RANK 1] Skor: 0.7183\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 11\n",
      "ğŸ‡¸ğŸ‡¦ ÙŠÙÙˆÙ’ØµÙÙŠÙ’ÙƒÙÙ…Ù Ø§Ù„Ù„Ù‘Ù°Ù‡Ù ÙÙÙŠÙ’Ù“ Ø§ÙÙˆÙ’Ù„ÙØ§Ø¯ÙÙƒÙÙ…Ù’ Ù„ÙÙ„Ø°Ù‘ÙÙƒÙØ±Ù Ù…ÙØ«Ù’Ù„Ù Ø­...\n",
      "ğŸ‡®ğŸ‡© Allah mensyariatkan (mewajibkan) kepadamu tentang (pembagian warisan untuk) anak-anakmu, (yaitu) bag...\n",
      "ğŸ’¬ TAFSIR: Setelah ayat sebelumnya menjelaskan dampak orang yang mengabaikan hak orang lain, ayat ini menjelask...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 2] Skor: 0.5632\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 127\n",
      "ğŸ‡¸ğŸ‡¦ ÙˆÙÙŠÙØ³Ù’ØªÙÙÙ’ØªÙÙˆÙ’Ù†ÙÙƒÙ ÙÙÙ‰ Ø§Ù„Ù†Ù‘ÙØ³ÙØ§Û¤Ø¡ÙÛ—  Ù‚ÙÙ„Ù Ø§Ù„Ù„Ù‘Ù°Ù‡Ù ÙŠÙÙÙ’ØªÙÙŠÙ’ÙƒÙ...\n",
      "ğŸ‡®ğŸ‡© Mereka meminta fatwa kepada engkau (Nabi Muhammad) tentang perempuan. Katakanlah, â€œAllah memberi fat...\n",
      "ğŸ’¬ TAFSIR: Dan mereka meminta fatwa, yaitu penjelasan hukum, kepadamu, wahai Muhammad, tentang berbagai persoal...\n",
      "--------------------------------------------------\n",
      "ğŸ… [RANK 3] Skor: 0.5285\n",
      "ğŸ“ QS. An-NisÄ'  : Ayat 8\n",
      "ğŸ‡¸ğŸ‡¦ ÙˆÙØ§ÙØ°ÙØ§ Ø­ÙØ¶ÙØ±Ù Ø§Ù„Ù’Ù‚ÙØ³Ù’Ù…ÙØ©Ù Ø§ÙÙˆÙ„ÙÙˆØ§ Ø§Ù„Ù’Ù‚ÙØ±Ù’Ø¨Ù°Ù‰ ÙˆÙØ§Ù„Ù’ÙŠÙØªÙ°Ù…Ù°Ù‰ Ùˆ...\n",
      "ğŸ‡®ğŸ‡© Apabila (saat) pembagian itu hadir beberapa kerabat,144) anak-anak yatim, dan orang-orang miskin, be...\n",
      "ğŸ’¬ TAFSIR: Setelah menjelaskan ketentuan hak warisan bagi kaum perempuan, maka pada ayat ini Allah memberi peri...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tes Query\n",
    "cari_lightgbm(\"Kisah sapi betina bani israil\")\n",
    "cari_lightgbm(\"Hukum warisan bagi perempuan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e1692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
