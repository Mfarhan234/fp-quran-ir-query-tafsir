{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quran Query-Tafsir Ranking Tutorial\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load tafsir data\n",
    "2. Extract TF-IDF and SBERT features\n",
    "3. Train ranking models (Logistic Regression, SVM, XGBoost)\n",
    "4. Evaluate using MAP, nDCG, MRR, and Recall@K metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.data_loader import TafsirDataLoader\n",
    "from src.features import FeatureExtractor\n",
    "from src.models import RankingModel, train_multiple_models\n",
    "from src.evaluation import RankingMetrics, print_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Tafsir Data\n",
    "\n",
    "We'll create sample data for demonstration. In practice, you would load your own tafsir dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = TafsirDataLoader()\n",
    "\n",
    "# Create sample data (or use loader.load_csv() for real data)\n",
    "data = loader.create_sample_data(n_queries=10, n_docs_per_query=5)\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {data.columns.tolist()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_data, test_data = loader.train_test_split(test_size=0.2)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "Extract TF-IDF features (and optionally SBERT embeddings) from query-document pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor (TF-IDF only for demonstration)\n",
    "feature_extractor = FeatureExtractor(use_tfidf=True, use_sbert=False)\n",
    "\n",
    "# Fit TF-IDF on all text data\n",
    "all_texts = data['query'].tolist() + data['tafsir_text'].tolist()\n",
    "feature_extractor.fit_tfidf(all_texts)\n",
    "\n",
    "print(\"Feature extractor initialized and fitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for training data\n",
    "train_queries = train_data['query'].tolist()\n",
    "train_docs = train_data['tafsir_text'].tolist()\n",
    "train_labels = train_data['relevance'].values\n",
    "\n",
    "X_train = feature_extractor.extract_features(train_queries, train_docs)\n",
    "\n",
    "print(f\"Training feature shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for test data\n",
    "test_queries = test_data['query'].tolist()\n",
    "test_docs = test_data['tafsir_text'].tolist()\n",
    "test_labels = test_data['relevance'].values\n",
    "\n",
    "X_test = feature_extractor.extract_features(test_queries, test_docs)\n",
    "\n",
    "print(f\"Test feature shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Ranking Models\n",
    "\n",
    "Train different models: Logistic Regression, SVM, and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = RankingModel(model_type='logistic_regression')\n",
    "lr_model.fit(X_train, train_labels)\n",
    "print(\"Logistic Regression trained.\")\n",
    "\n",
    "# Train SVM\n",
    "svm_model = RankingModel(model_type='svm')\n",
    "svm_model.fit(X_train, train_labels)\n",
    "print(\"SVM trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost (if installed)\n",
    "try:\n",
    "    xgb_model = RankingModel(model_type='xgboost')\n",
    "    xgb_model.fit(X_train, train_labels)\n",
    "    print(\"XGBoost trained.\")\n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed. Skipping.\")\n",
    "    xgb_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Models\n",
    "\n",
    "Compute ranking metrics: MAP, nDCG, MRR, and Recall@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics calculator\n",
    "metrics_calc = RankingMetrics()\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"=\" * 50)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "lr_metrics = metrics_calc.evaluate_model(\n",
    "    lr_model, feature_extractor, test_data, k_values=[1, 3, 5]\n",
    ")\n",
    "print_metrics(lr_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SVM\n",
    "print(\"=\" * 50)\n",
    "print(\"SVM\")\n",
    "svm_metrics = metrics_calc.evaluate_model(\n",
    "    svm_model, feature_extractor, test_data, k_values=[1, 3, 5]\n",
    ")\n",
    "print_metrics(svm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost (if available)\n",
    "if xgb_model:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"XGBOOST\")\n",
    "    xgb_metrics = metrics_calc.evaluate_model(\n",
    "        xgb_model, feature_extractor, test_data, k_values=[1, 3, 5]\n",
    "    )\n",
    "    print_metrics(xgb_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Model': ['Logistic Regression', 'SVM'],\n",
    "    'MAP': [lr_metrics['MAP'], svm_metrics['MAP']],\n",
    "    'MRR': [lr_metrics['MRR'], svm_metrics['MRR']],\n",
    "    'nDCG@5': [lr_metrics.get('nDCG@5', 0), svm_metrics.get('nDCG@5', 0)],\n",
    "    'Recall@5': [lr_metrics.get('Recall@5', 0), svm_metrics.get('Recall@5', 0)]\n",
    "}\n",
    "\n",
    "if xgb_model:\n",
    "    comparison_data['Model'].append('XGBoost')\n",
    "    comparison_data['MAP'].append(xgb_metrics['MAP'])\n",
    "    comparison_data['MRR'].append(xgb_metrics['MRR'])\n",
    "    comparison_data['nDCG@5'].append(xgb_metrics.get('nDCG@5', 0))\n",
    "    comparison_data['Recall@5'].append(xgb_metrics.get('Recall@5', 0))\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "# lr_model.save('../data/lr_model.joblib')\n",
    "# feature_extractor.save('../data/feature_extractor.joblib')\n",
    "print(\"Models can be saved using model.save() and feature_extractor.save()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example: Search and Rank\n",
    "\n",
    "Demonstrate how to search for relevant tafsir passages given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example search\n",
    "query = \"Apa makna taqwa dalam Islam?\"\n",
    "\n",
    "# Get unique tafsir passages\n",
    "unique_tafsirs = data.drop_duplicates(subset=['tafsir_text'])\n",
    "\n",
    "# Create query-document pairs\n",
    "queries = [query] * len(unique_tafsirs)\n",
    "documents = unique_tafsirs['tafsir_text'].tolist()\n",
    "\n",
    "# Extract features and predict\n",
    "X = feature_extractor.extract_features(queries, documents)\n",
    "scores = lr_model.predict_scores(X)\n",
    "\n",
    "# Rank and display results\n",
    "results = list(zip(documents, scores))\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 5 Results:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (doc, score) in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. (Score: {score:.4f})\")\n",
    "    print(f\"   {doc}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
