{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b3d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FILE TEST SET ROBUSTNESS BERHASIL DIBUAT!\n",
      "   Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\robustness_queries.csv\n",
      "   Total Query Uji: 60 baris (30.0 pasangan).\n",
      "\n",
      "STRUKTUR DATA (Preview):\n",
      "   id                            query category  \\\n",
      "0   1     Hukum warisan bagi perempuan     Fiqh   \n",
      "1   2      Kapan puasa Ramadan dimulai     Fiqh   \n",
      "2   3   Cara melaksanakan sholat Jumat     Fiqh   \n",
      "3   4                 Zakat hasil bumi     Fiqh   \n",
      "4   5  Denda bagi yang bersumpah palsu     Fiqh   \n",
      "5   6            Berwudu sebelum salat     Fiqh   \n",
      "\n",
      "                                       target_ayats  \n",
      "0                        [\"QS. An-Nis\\u0101' : 11\"]  \n",
      "1  [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]  \n",
      "2                            [\"QS. Al-Jumu'ah : 9\"]  \n",
      "3                       [\"QS. Al-An'\\u0101m : 141\"]  \n",
      "4                      [\"QS. Al-M\\u0101'idah : 89\"]  \n",
      "5                       [\"QS. Al-M\\u0101'idah : 6\"]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'robustness_queries.csv')\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# --- 1. DEFINISI PASANGAN QUERY & GROUND TRUTH ---\n",
    "# Struktur: [Query Asli, Query Variasi/Sinonim, Kategori, [Lokasi Ayats Relevan]]\n",
    "robustness_data = [\n",
    "    # Fiqh/Hukum (Law) - [1-10]\n",
    "    [1, \"Hukum warisan bagi perempuan\", \"Pembagian harta pusaka istri\", \"Fiqh\", [\"QS. An-NisƒÅ' : 11\"]],\n",
    "    [2, \"Kapan puasa Ramadan dimulai\", \"Kewajiban saum di bulan suci\", \"Fiqh\", [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]],\n",
    "    [3, \"Cara melaksanakan sholat Jumat\", \"Ketentuan sembahyang Jumat\", \"Fiqh\", [\"QS. Al-Jumu'ah : 9\"]],\n",
    "    [4, \"Zakat hasil bumi\", \"Kewajiban sedekah pertanian\", \"Fiqh\", [\"QS. Al-An'ƒÅm : 141\"]],\n",
    "    [5, \"Denda bagi yang bersumpah palsu\", \"Konsekuensi sumpah dusta\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 89\"]],\n",
    "    [6, \"Berwudu sebelum salat\", \"Tata cara bersuci sebelum ibadah\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 6\"]],\n",
    "    [7, \"Pernikahan beda agama\", \"Hukum perkawinan non-Muslim\", \"Fiqh\", [\"QS. Al-Baqarah : 221\"]],\n",
    "    [8, \"Larangan memakan riba\", \"Haramnya pinjaman berbunga\", \"Fiqh\", [\"QS. Al-Baqarah : 275\"]],\n",
    "    [9, \"Membayar fidyah karena tidak puasa\", \"Kewajiban ganti rugi puasa\", \"Fiqh\", [\"QS. Al-Baqarah : 184\"]],\n",
    "    [10, \"Apa itu khamar\", \"Definisi minuman memabukkan\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 90\"]],\n",
    "\n",
    "    # Kisah/Narasi (Narrative) - [11-20]\n",
    "    [11, \"Kisah Nabi Musa dan Firaun\", \"Cerita pertemuan Musa dengan raja Mesir\", \"Kisah\", [\"QS. Al-Qa·π£a·π£ : 31\", \"QS. Al-Qa·π£a·π£ : 36\"]],\n",
    "    [12, \"Kapal Nabi Nuh\", \"Perahu raksasa nuh\", \"Kisah\", [\"QS. H≈´d : 44\"]],\n",
    "    [13, \"Maryam melahirkan Isa\", \"Kelahiran putra Maryam\", \"Kisah\", [\"QS. Maryam : 23\", \"QS. Maryam : 27\"]],\n",
    "    [14, \"Nabi Yusuf dan mimpi 11 bintang\", \"Tafsir mimpi nabi Yakub tentang bintang\", \"Kisah\", [\"QS. Y≈´suf : 4\"]],\n",
    "    [15, \"Kisah Ashabul Kahfi\", \"Tujuh pemuda yang tertidur lama\", \"Kisah\", [\"QS. Al-Kahf : 10\", \"QS. Al-Kahf : 25\"]],\n",
    "    [16, \"Kenapa Iblis diusir dari surga\", \"Alasan setan menolak sujud Adam\", \"Kisah\", [\"QS. Al-A‚ÄòrƒÅf : 12\", \"QS. Al-Kahf : 50\"]],\n",
    "    [17, \"Tugas malaikat Jibril\", \"Fungsi Gabriel membawa wahyu\", \"Kisah\", [\"QS. Al-Baqarah : 97\"]],\n",
    "    [18, \"Kisah Qabil dan Habil\", \"Pembunuhan putra Adam\", \"Kisah\", [\"QS. Al-MƒÅ'idah : 27\"]],\n",
    "    [19, \"Raja Thalut dan Jalut\", \"Pertempuran Daud melawan Goliat\", \"Kisah\", [\"QS. Al-Baqarah : 249\", \"QS. Al-Baqarah : 251\"]],\n",
    "    [20, \"Bangsa Ya'juj dan Ma'juj\", \"Siapa Gog dan Magog\", \"Kisah\", [\"QS. Al-Kahf : 94\"]],\n",
    "\n",
    "    # Aqidah/Akhlak (Theology/Ethics) - [21-30]\n",
    "    [21, \"Larangan berbuat syirik\", \"Dosa menyekutukan Allah\", \"Aqidah\", [\"QS. An-NisƒÅ' : 48\", \"QS. LuqmƒÅn : 13\"]],\n",
    "    [22, \"Berbakti pada kedua orang tua\", \"Kewajiban menghormati ayah ibu\", \"Aqidah\", [\"QS. Al-IsrƒÅ' : 23\"]],\n",
    "    [23, \"Definisi tauhid\", \"Konsep keesaan Tuhan\", \"Aqidah\", [\"QS. Al-IkhlƒÅ·π£ : 1\"]],\n",
    "    [24, \"Takdir baik dan buruk\", \"Ketentuan nasib yang ditetapkan Allah\", \"Aqidah\", [\"QS. Al-Qamar : 49\"]],\n",
    "    [25, \"Larangan berbuat dusta\", \"Hukum berkata bohong\", \"Aqidah\", [\"QS. At-Taubah : 119\"]],\n",
    "    [26, \"Tentang Hari Kiamat\", \"Deskripsi Hari Pembalasan\", \"Aqidah\", [\"QS. Al-QƒÅri'ah : 1\", \"QS. Al-Zalzalah : 1\"]],\n",
    "    [27, \"Balasan bagi orang yang sombong\", \"Konsekuensi sifat takabur\", \"Aqidah\", [\"QS. LuqmƒÅn : 18\"]],\n",
    "    [28, \"Larangan mengumpat\", \"Hukum ghibah dan mencela\", \"Aqidah\", [\"QS. Al-·∏§ujurƒÅt : 12\"]],\n",
    "    [29, \"Keutamaan sabar\", \"Pentingnya menahan diri\", \"Aqidah\", [\"QS. Al-Baqarah : 153\"]],\n",
    "    [30, \"Tujuan hidup manusia\", \"Mengapa kita diciptakan\", \"Aqidah\", [\"QS. Adz-DzƒÅriyƒÅt : 56\"]],\n",
    "]\n",
    "\n",
    "# 2. KONVERSI KE DATAFRAME\n",
    "df_robustness = pd.DataFrame(robustness_data, columns=[\n",
    "    'id', 'query_a', 'query_b', 'category', 'target_ayats'\n",
    "])\n",
    "\n",
    "# 3. DUPLIKASI BARIS: Buat setiap Query A dan Query B menjadi baris terpisah\n",
    "# Ini penting agar kita bisa menghitung metrik untuk Q_A dan Q_B secara independen\n",
    "rows_a = df_robustness.rename(columns={'query_a': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "rows_b = df_robustness.rename(columns={'query_b': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "\n",
    "df_test_set = pd.concat([rows_a, rows_b], ignore_index=True)\n",
    "\n",
    "# 4. SIMPAN KE CSV\n",
    "# Kita simpan kolom 'target_ayats' sebagai string JSON agar mudah dibaca nanti\n",
    "df_test_set['target_ayats'] = df_test_set['target_ayats'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "df_test_set.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FILE TEST SET ROBUSTNESS BERHASIL DIBUAT!\")\n",
    "print(f\"   Tersimpan di: {OUTPUT_PATH}\")\n",
    "print(f\"   Total Query Uji: {len(df_test_set)} baris ({len(df_test_set)/2} pasangan).\")\n",
    "\n",
    "print(\"\\nSTRUKTUR DATA (Preview):\")\n",
    "print(df_test_set.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281d5727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è MEMBACA DATA UNTUK VERIFIKASI FORMAT...\n",
      "\n",
      "‚úÖ FORMAT PENULISAN SURAT YANG TERDETEKSI (Kolom 'ayat_asal'):\n",
      "   Contoh 1: 'QS. Al-Qamar : Ayat 46'\n",
      "   Contoh 2: 'QS. Ar-Ra·∏•mƒÅn : Ayat 6'\n",
      "   Contoh 3: \"QS. Al-AnbiyƒÅ'  : Ayat 39\"\n",
      "   Contoh 4: 'QS. Asy-Sy≈´rƒÅ : Ayat 52'\n",
      "   Contoh 5: \"QS. Asy-Syu‚ÄòarƒÅ' : Ayat 202\"\n",
      "\n",
      "KUNCI PENCARI LOKASI ADALAH: String di atas.\n",
      "Pastikan format ini sama persis dengan yang Anda gunakan untuk membangun gudang metadata!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "print(\"‚öôÔ∏è MEMBACA DATA UNTUK VERIFIKASI FORMAT...\")\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"‚ùå File {DATA_PATH} tidak ditemukan!\")\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_PATH)\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        \n",
    "        if 'ayat_asal' in df.columns:\n",
    "            # Ambil 5 contoh unik\n",
    "            sample = df['ayat_asal'].astype(str).unique()[:5]\n",
    "            \n",
    "            print(\"\\n‚úÖ FORMAT PENULISAN SURAT YANG TERDETEKSI (Kolom 'ayat_asal'):\")\n",
    "            \n",
    "            for i, s in enumerate(sample):\n",
    "                # Gunakan repr() untuk melihat karakter tersembunyi/kutipan\n",
    "                print(f\"   Contoh {i+1}: {repr(s)}\") \n",
    "            \n",
    "            print(\"\\nKUNCI PENCARI LOKASI ADALAH: String di atas.\")\n",
    "            print(\"Pastikan format ini sama persis dengan yang Anda gunakan untuk membangun gudang metadata!\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Kolom 'ayat_asal' tidak ditemukan di CSV.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saat membaca CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4811a81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ MEMULAI PROSES PENJAHITAN ID NUMERIK (FIXED)...\n",
      "   -> Membaca Data Lengkap (Sumber Fitur & Teks)...\n",
      "   -> Membaca Tafsir Clean (Sumber ID Surah)...\n",
      "   -> Parsing Lokasi & Ayah ID Selesai.\n",
      "   -> Membuat Kamus ID Numerik Surah (1-114)...\n",
      "   -> Mentransfer Surah ID Numerik ke Data Master...\n",
      "\n",
      "‚úÖ FILE MASTER ID-BASED BERHASIL DIBUAT!\n",
      "   Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\dataset_training_ID_BASED_MASTER.csv\n",
      "   Total baris: 170372\n",
      "\n",
      "üîç PREVIEW KOLOM ID BARU (Wajib Numerik):\n",
      "                     ayat_asal  surah_id  ayah_id\n",
      "0       QS. Al-Qamar : Ayat 46        48       46\n",
      "1       QS. Ar-Ra·∏•mƒÅn : Ayat 6        72        6\n",
      "2    QS. Al-AnbiyƒÅ'  : Ayat 39         2       39\n",
      "3      QS. Asy-Sy≈´rƒÅ : Ayat 52        79       52\n",
      "4  QS. Asy-Syu‚ÄòarƒÅ' : Ayat 202        78      202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# Input Files\n",
    "FULL_COMPLETE_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "TAFSIR_CLEAN_PATH = os.path.join(DATA_DIR, 'tafsir_clean.csv')\n",
    "\n",
    "# Output File Master Baru\n",
    "OUTPUT_MASTER_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv')\n",
    "\n",
    "print(\"üöÄ MEMULAI PROSES PENJAHITAN ID NUMERIK (FIXED)...\")\n",
    "\n",
    "# 1. LOAD DATA\n",
    "print(\"   -> Membaca Data Lengkap (Sumber Fitur & Teks)...\")\n",
    "df_master = pd.read_csv(FULL_COMPLETE_PATH)\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "\n",
    "print(\"   -> Membaca Tafsir Clean (Sumber ID Surah)...\")\n",
    "df_clean = pd.read_csv(TAFSIR_CLEAN_PATH)\n",
    "df_clean.columns = df_clean.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. BUAT JEMBATAN (PARSING AYAT ASAL)\n",
    "# Mengekstrak Nama Surah dan Nomor Ayat dari string 'ayat_asal' di df_master\n",
    "def parse_location(location_str):\n",
    "    if not isinstance(location_str, str) or \"QS.\" not in location_str:\n",
    "        return None, None\n",
    "    \n",
    "    # Mencari Nama Surah\n",
    "    match_surah = re.search(r'QS\\. (.+?)\\s+:', location_str)\n",
    "    surah_name = match_surah.group(1).strip() if match_surah else None\n",
    "    \n",
    "    # Mencari Nomor Ayat\n",
    "    match_ayah = re.search(r'Ayat\\s+(\\d+)', location_str)\n",
    "    ayah_num = int(match_ayah.group(1)) if match_ayah else None\n",
    "    \n",
    "    return surah_name, ayah_num\n",
    "\n",
    "# Terapkan Parsing\n",
    "df_master[['surah_nama_temp', 'ayah_id']] = df_master['ayat_asal'].apply(\n",
    "    lambda x: pd.Series(parse_location(x))\n",
    ")\n",
    "\n",
    "print(\"   -> Parsing Lokasi & Ayah ID Selesai.\")\n",
    "\n",
    "# 3. BUAT KAMUS ID SURAH (DERIVASI 1-114)\n",
    "print(\"   -> Membuat Kamus ID Numerik Surah (1-114)...\")\n",
    "\n",
    "# Dapatkan daftar unik nama surah dari df_clean\n",
    "unique_surahs = df_clean['surah'].str.strip().unique()\n",
    "\n",
    "# Hapus nilai NaN atau string kosong dari daftar unique_surahs sebelum sorting\n",
    "unique_surahs = [s for s in unique_surahs if isinstance(s, str) and s.strip() != '']\n",
    "\n",
    "# Urutkan berdasarkan Nama Surah (Asumsi ini mengikuti urutan kronologis Al-Qur'an)\n",
    "# Sorting menggunakan nama string adalah heuristik terbaik tanpa list Surah eksternal.\n",
    "sorted_surahs = sorted(unique_surahs) \n",
    "\n",
    "# Buat map {Nama Surah: ID Numerik}\n",
    "surah_id_map = {name: i + 1 for i, name in enumerate(sorted_surahs)}\n",
    "\n",
    "# 4. TRANSFER ID NUMERIK\n",
    "print(\"   -> Mentransfer Surah ID Numerik ke Data Master...\")\n",
    "\n",
    "# Ambil ID Surah dari kamus\n",
    "df_master['surah_id'] = df_master['surah_nama_temp'].str.strip().map(surah_id_map)\n",
    "\n",
    "# 5. BERSIHKAN & SIMPAN FILE BARU\n",
    "# Hapus kolom sementara\n",
    "df_master = df_master.drop(columns=['surah_nama_temp'], errors='ignore')\n",
    "\n",
    "# Ambil kolom ID baru\n",
    "df_master = df_master[[c for c in df_master.columns if c not in ['surah_id', 'ayah_id']] + ['surah_id', 'ayah_id']]\n",
    "\n",
    "# Drop baris yang gagal di-parse\n",
    "df_master = df_master.dropna(subset=['surah_id', 'ayah_id'])\n",
    "\n",
    "# Pastikan ID adalah integer\n",
    "df_master['surah_id'] = df_master['surah_id'].astype(int)\n",
    "df_master['ayah_id'] = df_master['ayah_id'].astype(int)\n",
    "\n",
    "\n",
    "df_master.to_csv(OUTPUT_MASTER_PATH, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ FILE MASTER ID-BASED BERHASIL DIBUAT!\")\n",
    "print(f\"   Tersimpan di: {OUTPUT_MASTER_PATH}\")\n",
    "print(f\"   Total baris: {len(df_master)}\")\n",
    "\n",
    "print(\"\\nüîç PREVIEW KOLOM ID BARU (Wajib Numerik):\")\n",
    "print(df_master[['ayat_asal', 'surah_id', 'ayah_id']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ed5d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è MEMUAT DATA MASTER UNTUK VERIFIKASI ID...\n",
      "\n",
      "‚úÖ HASIL PEMETAAN ID NUMERIK (VERIFIKASI)\n",
      "=========================================\n",
      "Total Surah yang Terdeteksi: 114\n",
      " surah_id    surah_name\n",
      "        1     Ad-DukhƒÅn\n",
      "        2    Al-AnbiyƒÅ'\n",
      "        3      Al-AnfƒÅl\n",
      "        4      Al-An‚ÄòƒÅm\n",
      "        5      Al-A·∏•qƒÅf\n",
      "        6      Al-A·∏•zƒÅb\n",
      "        7       Al-A‚ÄòlƒÅ\n",
      "        8      Al-A‚ÄòrƒÅf\n",
      "        9      Al-Balad\n",
      "       10    Al-Baqarah\n",
      "       11   Al-Bayyinah\n",
      "       12      Al-Bur≈´j\n",
      "       13       Al-Fajr\n",
      "       14      Al-Falaq\n",
      "       15       Al-Fat·∏•\n",
      "       16     Al-FurqƒÅn\n",
      "       17    Al-FƒÅti·∏•ah\n",
      "       18        Al-Fƒ´l\n",
      "       19   Al-GƒÅsyiyah\n",
      "       20    Al-Humazah\n",
      "       21     Al-IkhlƒÅ·π£\n",
      "       22    Al-Infi·π≠ƒÅr\n",
      "       23   Al-InsyiqƒÅq\n",
      "       24      Al-InsƒÅn\n",
      "       25      Al-IsrƒÅ'\n",
      "       26       Al-Jinn\n",
      "       27    Al-Jumu‚Äòah\n",
      "       28    Al-JƒÅ·π°iyah\n",
      "       29       Al-Kahf\n",
      "       30     Al-Kau·π°ar\n",
      "       31    Al-KƒÅfir≈´n\n",
      "       32      Al-Lahab\n",
      "       33       Al-Lail\n",
      "       34    Al-Ma‚ÄòƒÅrij\n",
      "       35   Al-Mu'min≈´n\n",
      "       36  Al-Mudda·π°·π°ir\n",
      "       37  Al-MujƒÅdalah\n",
      "       38       Al-Mulk\n",
      "       39 Al-Mumta·∏•anah\n",
      "       40  Al-MunƒÅfiq≈´n\n",
      "       41   Al-MursalƒÅt\n",
      "       42  Al-Muzzammil\n",
      "       43 Al-Mu·π≠affifƒ´n\n",
      "       44    Al-MƒÅ'idah\n",
      "       45      Al-MƒÅ‚Äò≈´n\n",
      "       46       Al-Qadr\n",
      "       47      Al-Qalam\n",
      "       48      Al-Qamar\n",
      "       49      Al-Qa·π£a·π£\n",
      "       50    Al-QiyƒÅmah\n",
      "       51    Al-QƒÅri‚Äòah\n",
      "       52    Al-WƒÅqi‚Äòah\n",
      "       53      Al-·∏§adƒ´d\n",
      "       54       Al-·∏§ajj\n",
      "       55      Al-·∏§asyr\n",
      "       56       Al-·∏§ijr\n",
      "       57    Al-·∏§ujurƒÅt\n",
      "       58     Al-·∏§ƒÅqqah\n",
      "       59      Al-‚ÄòAlaq\n",
      "       60   Al-‚ÄòAnkab≈´t\n",
      "       61       Al-‚ÄòA·π£r\n",
      "       62    Al-‚ÄòƒÄdiyƒÅt\n",
      "       63      An-Naba'\n",
      "       64       An-Najm\n",
      "       65       An-Naml\n",
      "       66       An-Na·∏•l\n",
      "       67       An-Na·π£r\n",
      "       68      An-NisƒÅ'\n",
      "       69        An-NƒÅs\n",
      "       70    An-NƒÅzi‚ÄòƒÅt\n",
      "       71        An-N≈´r\n",
      "       72     Ar-Ra·∏•mƒÅn\n",
      "       73       Ar-Ra‚Äòd\n",
      "       74        Ar-R≈´m\n",
      "       75     As-Sajdah\n",
      "       76     Asy-Syams\n",
      "       77     Asy-Syar·∏•\n",
      "       78  Asy-Syu‚ÄòarƒÅ'\n",
      "       79     Asy-Sy≈´rƒÅ\n",
      "       80    At-TagƒÅbun\n",
      "       81     At-Takwƒ´r\n",
      "       82    At-TakƒÅ·π°ur\n",
      "       83     At-Taubah\n",
      "       84        At-Tƒ´n\n",
      "       85     At-ta·∏•rƒ´m\n",
      "       86   Az-Zalzalah\n",
      "       87    Az-Zukhruf\n",
      "       88      Az-Zumar\n",
      "       89    A≈º-≈ªƒÅriyƒÅt\n",
      "       90       A·∏ç-·∏åu·∏•ƒÅ\n",
      "       91       A·π£-·π¢aff\n",
      "       92     A·π£-·π¢ƒÅffƒÅt\n",
      "       93      A·π≠-·π¨alƒÅq\n",
      "       94      A·π≠-·π¨ƒÅriq\n",
      "       95        A·π≠-·π¨≈´r\n",
      "       96      Fu·π£·π£ilat\n",
      "       97         FƒÅ·π≠ir\n",
      "       98         GƒÅfir\n",
      "       99           H≈´d\n",
      "      100       IbrƒÅhƒ´m\n",
      "      101        LuqmƒÅn\n",
      "      102        Maryam\n",
      "      103      Mu·∏•ammad\n",
      "      104           N≈´·∏•\n",
      "      105       Quraisy\n",
      "      106           QƒÅf\n",
      "      107         Saba'\n",
      "      108         YƒÅsƒ´n\n",
      "      109         Y≈´nus\n",
      "      110         Y≈´suf\n",
      "      111    ƒÄli ‚ÄòImrƒÅn\n",
      "      112           ·π¢ƒÅd\n",
      "      113          ·π¨ƒÅhƒÅ\n",
      "      114        ‚ÄòAbasa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "MASTER_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv') # File output kita\n",
    "\n",
    "print(\"‚öôÔ∏è MEMUAT DATA MASTER UNTUK VERIFIKASI ID...\")\n",
    "\n",
    "if not os.path.exists(MASTER_PATH):\n",
    "    raise FileNotFoundError(f\"‚ùå File Master ID-Based tidak ditemukan di: {MASTER_PATH}\")\n",
    "\n",
    "# 1. Load File Master\n",
    "df_master = pd.read_csv(MASTER_PATH)\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. Fungsi Ekstraksi Nama Surah dari String Ayat Asal\n",
    "def extract_surah_name(location_str):\n",
    "    if not isinstance(location_str, str) or \"QS.\" not in location_str:\n",
    "        return \"Unknown/None\"\n",
    "    \n",
    "    # Regex untuk mengambil teks setelah \"QS. \" dan sebelum \" :\"\n",
    "    match = re.search(r'QS\\. (.+?)\\s+:', location_str)\n",
    "    return match.group(1).strip() if match else \"Unknown/None\"\n",
    "\n",
    "# 3. Ekstrak Nama Surah & Grouping\n",
    "df_master['surah_name'] = df_master['ayat_asal'].apply(extract_surah_name)\n",
    "\n",
    "# 4. Buat Tabel Verifikasi (Hanya Surah ID dan Nama Unik)\n",
    "surah_map_check = df_master[['surah_id', 'surah_name']].drop_duplicates().sort_values('surah_id')\n",
    "\n",
    "# 4. Buat Tabel Verifikasi (Hanya Surah ID dan Nama Unik)\n",
    "surah_map_check = df_master[['surah_id', 'surah_name']].drop_duplicates().sort_values('surah_id')\n",
    "\n",
    "print(\"\\n‚úÖ HASIL PEMETAAN ID NUMERIK (VERIFIKASI)\")\n",
    "print(\"=========================================\")\n",
    "print(f\"Total Surah yang Terdeteksi: {len(surah_map_check)}\")\n",
    "\n",
    "# PERBAIKAN: Mengganti to_markdown() dengan to_string() untuk menghindari error 'tabulate'\n",
    "print(surah_map_check.to_string(index=False))\n",
    "\n",
    "# Pengecekan Kualitas \n",
    "if surah_map_check['surah_id'].nunique() != surah_map_check['surah_name'].nunique():\n",
    "    print(\"‚ö†Ô∏è PERINGATAN: Ada potensi masalah dalam pemetaan ID/Nama Surah.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decbe365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ MEMULAI PROSES PENJAHITAN ID NUMERIK (FINAL FIX)...\n",
      "   -> Membaca Data Lengkap (Sumber Fitur & Teks)...\n",
      "   -> Membaca Tafsir Clean (Sumber ID Surah)...\n",
      "   -> Parsing Lokasi & Ayah ID Selesai.\n",
      "   -> Membuat Kamus ID Numerik Surah (1-114) DENGAN URUTAN YANG BENAR...\n",
      "   -> Mentransfer Surah ID Numerik ke Data Master...\n",
      "\n",
      "‚úÖ FILE MASTER ID-BASED BERHASIL DIBUAT!\n",
      "   Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\dataset_training_ID_BASED_MASTER.csv\n",
      "   Total baris: 157579\n",
      "\n",
      "üîç PREVIEW KOLOM ID BARU (Wajib Numerik):\n",
      "                     ayat_asal  surah_id  ayah_id\n",
      "0       QS. Al-Qamar : Ayat 46        54       46\n",
      "1       QS. Ar-Ra·∏•mƒÅn : Ayat 6        55        6\n",
      "2    QS. Al-AnbiyƒÅ'  : Ayat 39        21       39\n",
      "3      QS. Asy-Sy≈´rƒÅ : Ayat 52        42       52\n",
      "4  QS. Asy-Syu‚ÄòarƒÅ' : Ayat 202        26      202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# Input Files\n",
    "FULL_COMPLETE_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "TAFSIR_CLEAN_PATH = os.path.join(DATA_DIR, 'tafsir_clean.csv')\n",
    "\n",
    "# Output File Master Baru\n",
    "OUTPUT_MASTER_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv')\n",
    "\n",
    "print(\"üöÄ MEMULAI PROSES PENJAHITAN ID NUMERIK (FINAL FIX)...\")\n",
    "\n",
    "# 1. LOAD DATA\n",
    "print(\"   -> Membaca Data Lengkap (Sumber Fitur & Teks)...\")\n",
    "df_master = pd.read_csv(FULL_COMPLETE_PATH)\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "\n",
    "print(\"   -> Membaca Tafsir Clean (Sumber ID Surah)...\")\n",
    "df_clean = pd.read_csv(TAFSIR_CLEAN_PATH)\n",
    "df_clean.columns = df_clean.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. BUAT JEMBATAN (PARSING AYAT ASAL)\n",
    "# Mengekstrak Nama Surah dan Nomor Ayat dari string 'ayat_asal' di df_master\n",
    "def parse_location(location_str):\n",
    "    if not isinstance(location_str, str) or \"QS.\" not in location_str:\n",
    "        return None, None\n",
    "    \n",
    "    match_surah = re.search(r'QS\\. (.+?)\\s+:', location_str)\n",
    "    surah_name = match_surah.group(1).strip() if match_surah else None\n",
    "    \n",
    "    match_ayah = re.search(r'Ayat\\s+(\\d+)', location_str)\n",
    "    ayah_num = int(match_ayah.group(1)) if match_ayah else None\n",
    "    \n",
    "    return surah_name, ayah_num\n",
    "\n",
    "# Terapkan Parsing\n",
    "df_master[['surah_nama_temp', 'ayah_id']] = df_master['ayat_asal'].apply(\n",
    "    lambda x: pd.Series(parse_location(x))\n",
    ")\n",
    "\n",
    "print(\"   -> Parsing Lokasi & Ayah ID Selesai.\")\n",
    "\n",
    "# 3. BUAT KAMUS ID SURAH (DARI DAFTAR KANONIK)\n",
    "print(\"   -> Membuat Kamus ID Numerik Surah (1-114) DENGAN URUTAN YANG BENAR...\")\n",
    "\n",
    "# Daftar 114 Surah Al-Qur'an (Kanonik, sesuai urutan 1-114)\n",
    "# INI ADALAH FIX UNTUK MENGHINDARI SORTING ALFABETIK YANG SALAH\n",
    "canonical_surahs = [\n",
    "    'Al-FƒÅti·∏•ah', 'Al-Baqarah', 'ƒÄli ‚ÄòImrƒÅn', 'An-NisƒÅ\\'', 'Al-MƒÅ\\'idah', 'Al-An\\'ƒÅm', 'Al-A‚ÄòrƒÅf', 'Al-AnfƒÅl', \n",
    "    'At-Taubah', 'Y≈´nus', 'H≈´d', 'Y≈´suf', 'Ar-Ra‚Äòd', 'IbrƒÅhƒ´m', 'Al-·∏§ijr', 'An-Na·∏•l', 'Al-IsrƒÅ\\'', 'Al-Kahf', \n",
    "    'Maryam', '·π¨ƒÅhƒÅ', 'Al-AnbiyƒÅ\\'', 'Al-·∏§aj', 'Al-Mu\\'min≈´n', 'An-N≈´r', 'Al-FurqƒÅn', 'Asy-Syu‚ÄòarƒÅ\\'', \n",
    "    'An-Naml', 'Al-Qa·π£a·π£', 'Al-‚ÄòAnkab≈´t', 'Ar-R≈´m', 'LuqmƒÅn', 'As-Sajdah', 'Al-A·∏•zƒÅb', 'SabƒÅ\\'', 'FƒÅ·π≠ir', \n",
    "    'YƒÅsƒ´n', 'A·π£-·π¢ƒÅffƒÅt', '·π¢ƒÅd', 'Az-Zumar', 'GƒÅfir', 'Fu·π£·π£ilat', 'Asy-Sy≈´rƒÅ', 'Az-Zukhruf', 'Ad-DukhƒÅn', \n",
    "    'Al-JƒÅ·π°iyah', 'Al-A·∏•qƒÅf', 'Mu·∏•ammad', 'Al-Fat·∏•', 'Al-·∏§ujurƒÅt', 'QƒÅf', 'Adz-DzƒÅriyƒÅt', 'A·π≠-·π¨≈´r', 'An-Najm', \n",
    "    'Al-Qamar', 'Ar-Ra·∏•mƒÅn', 'Al-WƒÅqi‚Äòah', 'Al-·∏§adƒ´d', 'Al-MujƒÅdilah', 'Al-·∏§asyr', 'Al-Mumta·∏•anah', 'A·π£-·π¢aff', \n",
    "    'Al-Jumu‚Äòah', 'Al-MunƒÅfiq≈´n', 'At-TagƒÅbun', 'A·π≠-·π¨alƒÅq', 'At-Ta·∏•rƒ´m', 'Al-Mulk', 'Al-Qalam', 'Al-·∏§ƒÅqqah', \n",
    "    'Al-Ma‚ÄòƒÅrij', 'N≈´·∏•', 'Al-Jinn', 'Al-Muzzammil', 'Al-Muddasir', 'Al-QiyƒÅmah', 'Al-InsƒÅn', 'Al-MursalƒÅt', \n",
    "    'An-Naba\\'', 'An-NƒÅzi‚ÄòƒÅt', '‚ÄòAbasa', 'At-Takwƒ´r', 'Al-Infi·π≠ƒÅr', 'Al-Mu·π≠affifƒ´n', 'Al-InsyiqƒÅq', \n",
    "    'Al-Bur≈´j', 'A·π≠-·π¨ƒÅriq', 'Al-A‚ÄòlƒÅ', 'Al-GƒÅsyiyah', 'Al-Fajr', 'Al-Balad', 'Asy-Syams', 'Al-Lail', 'A·∏ç-·∏åu·∏•ƒÅ', \n",
    "    'Al-InsyirƒÅ·∏•', 'At-Tƒ´n', 'Al-‚ÄòAlaq', 'Al-Qadr', 'Al-Bayyinah', 'Az-Zalzalah', 'Al-‚ÄòƒÄdiyƒÅt', 'Al-QƒÅri‚Äòah', \n",
    "    'At-TakƒÅ·π°ur', 'Al-‚ÄòA·π£r', 'Al-Humazah', 'Al-Fƒ´l', 'Quraisy', 'Al-MƒÅ‚Äò≈´n', 'Al-Kausar', 'Al-KƒÅfir≈´n', \n",
    "    'An-Na·π£r', 'Al-Lahab', 'Al-IkhlƒÅ·π£', 'Al-Falaq', 'An-NƒÅs'\n",
    "]\n",
    "\n",
    "# Buat map {Nama Surah: ID Numerik}\n",
    "surah_id_map = {name: i + 1 for i, name in enumerate(canonical_surahs)}\n",
    "\n",
    "# 4. TRANSFER ID NUMERIK\n",
    "print(\"   -> Mentransfer Surah ID Numerik ke Data Master...\")\n",
    "\n",
    "# Ambil ID Surah dari kamus\n",
    "df_master['surah_id'] = df_master['surah_nama_temp'].str.strip().map(surah_id_map)\n",
    "\n",
    "# 5. BERSIHKAN & SIMPAN FILE BARU\n",
    "# ... (lanjutan kode Cell 3)\n",
    "df_master = df_master.drop(columns=['surah_nama_temp'], errors='ignore')\n",
    "df_master = df_master[[c for c in df_master.columns if c not in ['surah_id', 'ayah_id']] + ['surah_id', 'ayah_id']]\n",
    "df_master = df_master.dropna(subset=['surah_id', 'ayah_id'])\n",
    "df_master['surah_id'] = df_master['surah_id'].astype(int)\n",
    "df_master['ayah_id'] = df_master['ayah_id'].astype(int)\n",
    "\n",
    "df_master.to_csv(OUTPUT_MASTER_PATH, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ FILE MASTER ID-BASED BERHASIL DIBUAT!\")\n",
    "print(f\"   Tersimpan di: {OUTPUT_MASTER_PATH}\")\n",
    "print(f\"   Total baris: {len(df_master)}\")\n",
    "\n",
    "print(\"\\nüîç PREVIEW KOLOM ID BARU (Wajib Numerik):\")\n",
    "print(df_master[['ayat_asal', 'surah_id', 'ayah_id']].head(5))\n",
    "\n",
    "# --- LANGKAH VERIFIKASI (BARU) ---\n",
    "# Jalankan verifikasi ulang di cell berikutnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54999779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è MEMUAT DATA MASTER UNTUK VERIFIKASI ID...\n",
      "\n",
      "‚úÖ HASIL PEMETAAN ID NUMERIK (VERIFIKASI)\n",
      "=========================================\n",
      "Total Surah yang Terdeteksi: 105\n",
      " surah_id    surah_name\n",
      "        1    Al-FƒÅti·∏•ah\n",
      "        2    Al-Baqarah\n",
      "        3    ƒÄli ‚ÄòImrƒÅn\n",
      "        4      An-NisƒÅ'\n",
      "        5    Al-MƒÅ'idah\n",
      "        7      Al-A‚ÄòrƒÅf\n",
      "        8      Al-AnfƒÅl\n",
      "        9     At-Taubah\n",
      "       10         Y≈´nus\n",
      "       11           H≈´d\n",
      "       12         Y≈´suf\n",
      "       13       Ar-Ra‚Äòd\n",
      "       14       IbrƒÅhƒ´m\n",
      "       15       Al-·∏§ijr\n",
      "       16       An-Na·∏•l\n",
      "       17      Al-IsrƒÅ'\n",
      "       18       Al-Kahf\n",
      "       19        Maryam\n",
      "       20          ·π¨ƒÅhƒÅ\n",
      "       21    Al-AnbiyƒÅ'\n",
      "       23   Al-Mu'min≈´n\n",
      "       24        An-N≈´r\n",
      "       25     Al-FurqƒÅn\n",
      "       26  Asy-Syu‚ÄòarƒÅ'\n",
      "       27       An-Naml\n",
      "       28      Al-Qa·π£a·π£\n",
      "       29   Al-‚ÄòAnkab≈´t\n",
      "       30        Ar-R≈´m\n",
      "       31        LuqmƒÅn\n",
      "       32     As-Sajdah\n",
      "       33      Al-A·∏•zƒÅb\n",
      "       35         FƒÅ·π≠ir\n",
      "       36         YƒÅsƒ´n\n",
      "       37     A·π£-·π¢ƒÅffƒÅt\n",
      "       38           ·π¢ƒÅd\n",
      "       39      Az-Zumar\n",
      "       40         GƒÅfir\n",
      "       41      Fu·π£·π£ilat\n",
      "       42     Asy-Sy≈´rƒÅ\n",
      "       43    Az-Zukhruf\n",
      "       44     Ad-DukhƒÅn\n",
      "       45    Al-JƒÅ·π°iyah\n",
      "       46      Al-A·∏•qƒÅf\n",
      "       47      Mu·∏•ammad\n",
      "       48       Al-Fat·∏•\n",
      "       49    Al-·∏§ujurƒÅt\n",
      "       50           QƒÅf\n",
      "       52        A·π≠-·π¨≈´r\n",
      "       53       An-Najm\n",
      "       54      Al-Qamar\n",
      "       55     Ar-Ra·∏•mƒÅn\n",
      "       56    Al-WƒÅqi‚Äòah\n",
      "       57      Al-·∏§adƒ´d\n",
      "       59      Al-·∏§asyr\n",
      "       60 Al-Mumta·∏•anah\n",
      "       61       A·π£-·π¢aff\n",
      "       62    Al-Jumu‚Äòah\n",
      "       63  Al-MunƒÅfiq≈´n\n",
      "       64    At-TagƒÅbun\n",
      "       65      A·π≠-·π¨alƒÅq\n",
      "       67       Al-Mulk\n",
      "       68      Al-Qalam\n",
      "       69     Al-·∏§ƒÅqqah\n",
      "       70    Al-Ma‚ÄòƒÅrij\n",
      "       71           N≈´·∏•\n",
      "       72       Al-Jinn\n",
      "       73  Al-Muzzammil\n",
      "       75    Al-QiyƒÅmah\n",
      "       76      Al-InsƒÅn\n",
      "       77   Al-MursalƒÅt\n",
      "       78      An-Naba'\n",
      "       79    An-NƒÅzi‚ÄòƒÅt\n",
      "       80        ‚ÄòAbasa\n",
      "       81     At-Takwƒ´r\n",
      "       82    Al-Infi·π≠ƒÅr\n",
      "       83 Al-Mu·π≠affifƒ´n\n",
      "       84   Al-InsyiqƒÅq\n",
      "       85      Al-Bur≈´j\n",
      "       86      A·π≠-·π¨ƒÅriq\n",
      "       87       Al-A‚ÄòlƒÅ\n",
      "       88   Al-GƒÅsyiyah\n",
      "       89       Al-Fajr\n",
      "       90      Al-Balad\n",
      "       91     Asy-Syams\n",
      "       92       Al-Lail\n",
      "       93       A·∏ç-·∏åu·∏•ƒÅ\n",
      "       95        At-Tƒ´n\n",
      "       96      Al-‚ÄòAlaq\n",
      "       97       Al-Qadr\n",
      "       98   Al-Bayyinah\n",
      "       99   Az-Zalzalah\n",
      "      100    Al-‚ÄòƒÄdiyƒÅt\n",
      "      101    Al-QƒÅri‚Äòah\n",
      "      102    At-TakƒÅ·π°ur\n",
      "      103       Al-‚ÄòA·π£r\n",
      "      104    Al-Humazah\n",
      "      105        Al-Fƒ´l\n",
      "      106       Quraisy\n",
      "      107      Al-MƒÅ‚Äò≈´n\n",
      "      109    Al-KƒÅfir≈´n\n",
      "      110       An-Na·π£r\n",
      "      111      Al-Lahab\n",
      "      112     Al-IkhlƒÅ·π£\n",
      "      113      Al-Falaq\n",
      "      114        An-NƒÅs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "MASTER_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv') # File output kita\n",
    "\n",
    "print(\"‚öôÔ∏è MEMUAT DATA MASTER UNTUK VERIFIKASI ID...\")\n",
    "\n",
    "if not os.path.exists(MASTER_PATH):\n",
    "    raise FileNotFoundError(f\"‚ùå File Master ID-Based tidak ditemukan di: {MASTER_PATH}\")\n",
    "\n",
    "# 1. Load File Master\n",
    "df_master = pd.read_csv(MASTER_PATH)\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "\n",
    "# 2. Fungsi Ekstraksi Nama Surah dari String Ayat Asal\n",
    "def extract_surah_name(location_str):\n",
    "    if not isinstance(location_str, str) or \"QS.\" not in location_str:\n",
    "        return \"Unknown/None\"\n",
    "    \n",
    "    # Regex untuk mengambil teks setelah \"QS. \" dan sebelum \" :\"\n",
    "    match = re.search(r'QS\\. (.+?)\\s+:', location_str)\n",
    "    return match.group(1).strip() if match else \"Unknown/None\"\n",
    "\n",
    "# 3. Ekstrak Nama Surah & Grouping\n",
    "df_master['surah_name'] = df_master['ayat_asal'].apply(extract_surah_name)\n",
    "\n",
    "# 4. Buat Tabel Verifikasi (Hanya Surah ID dan Nama Unik)\n",
    "surah_map_check = df_master[['surah_id', 'surah_name']].drop_duplicates().sort_values('surah_id')\n",
    "\n",
    "# 4. Buat Tabel Verifikasi (Hanya Surah ID dan Nama Unik)\n",
    "surah_map_check = df_master[['surah_id', 'surah_name']].drop_duplicates().sort_values('surah_id')\n",
    "\n",
    "print(\"\\n‚úÖ HASIL PEMETAAN ID NUMERIK (VERIFIKASI)\")\n",
    "print(\"=========================================\")\n",
    "print(f\"Total Surah yang Terdeteksi: {len(surah_map_check)}\")\n",
    "\n",
    "# PERBAIKAN: Mengganti to_markdown() dengan to_string() untuk menghindari error 'tabulate'\n",
    "print(surah_map_check.to_string(index=False))\n",
    "\n",
    "# Pengecekan Kualitas \n",
    "if surah_map_check['surah_id'].nunique() != surah_map_check['surah_name'].nunique():\n",
    "    print(\"‚ö†Ô∏è PERINGATAN: Ada potensi masalah dalam pemetaan ID/Nama Surah.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c6991f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FILE TEST SET ROBUSTNESS BERHASIL DIBUAT (dengan ID Surah & Ayat)!\n",
      " ¬† Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\robustness_queries.csv\n",
      " ¬† Total Query Uji: 60 baris (30 pasangan).\n",
      "\n",
      "STRUKTUR DATA (Preview dengan kolom ID baru):\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1398\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n\u001b[32m   1397\u001b[39m     name = _resolve_name(name, package, level)\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_find_and_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1371\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m _NEEDS_LOADING:\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_find_and_load_unlocked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimport_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;66;03m# Optimization: only call _bootstrap._lock_unlock_module() if\u001b[39;00m\n\u001b[32m   1374\u001b[39m \u001b[38;5;66;03m# module.__spec__._initializing is True.\u001b[39;00m\n\u001b[32m   1375\u001b[39m \u001b[38;5;66;03m# NOTE: because of this, initializing must be set *before*\u001b[39;00m\n\u001b[32m   1376\u001b[39m \u001b[38;5;66;03m# putting the new module in sys.modules.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1335\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ERR_MSG_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m, name=name)\n\u001b[32m   1336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSTRUKTUR DATA (Preview dengan kolom ID baru):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# Tampilkan 6 baris pertama dan kolom yang relevan\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_test_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget_ayats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget_ayats_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\frame.py:2994\u001b[39m, in \u001b[36mDataFrame.to_markdown\u001b[39m\u001b[34m(self, buf, mode, index, storage_options, **kwargs)\u001b[39m\n\u001b[32m   2992\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtablefmt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2993\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mshowindex\u001b[39m\u001b[33m\"\u001b[39m, index)\n\u001b[32m-> \u001b[39m\u001b[32m2994\u001b[39m tabulate = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabulate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2995\u001b[39m result = tabulate.tabulate(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re # Diperlukan untuk parsing nama surah dan ayat\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        # Asumsi: jika tidak ada folder data, kita naik satu level\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'robustness_queries.csv')\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# --- FUNGSI UTILITY: MENDAPATKAN ID SURAH DAN AYAT ---\n",
    "\n",
    "# Mapping sebagian Surah yang digunakan ke ID numeriknya\n",
    "# Jika Anda membutuhkan 114 Surah, gunakan file mapping lengkap (JSON/CSV)\n",
    "SURAH_MAPPING = {\n",
    "    \"Al-Baqarah\": 2, \"ƒÄli 'ImrƒÅn\": 3, \"An-NisƒÅ'\": 4, \"Al-MƒÅ'idah\": 5, \n",
    "    \"Al-An'ƒÅm\": 6, \"Al-A‚ÄòrƒÅf\": 7, \"At-Taubah\": 9, \"H≈´d\": 11, \n",
    "    \"Y≈´suf\": 12, \"Ar-Ra‚Äòd\": 13, \"Al-IsrƒÅ'\": 17, \"Al-Kahf\": 18, \n",
    "    \"Maryam\": 19, \"Al-AnbiyƒÅ'\": 21, \"Al-Qa·π£a·π£\": 28, \"Al-‚ÄòAnkab≈´t\": 29,\n",
    "    \"LuqmƒÅn\": 31, \"Al-A·∏•zƒÅb\": 33, \"YƒÅ-Sƒ´n\": 36, \"A·∫ì-·∫íƒÅriyƒÅt\": 51,\n",
    "    \"Al-Qamar\": 54, \"Al-·∏§ujurƒÅt\": 49, \"Adz-DzƒÅriyƒÅt\": 51, \"Al-Jumu'ah\": 62,\n",
    "    \"At-TaghƒÅbun\": 64, \"Al-Qalam\": 68, \"Al-Ma‚ÄòƒÅrij\": 70, \"Al-QiyƒÅmah\": 75,\n",
    "    \"Al-IkhlƒÅ·π£\": 112, \"Al-Zalzalah\": 99, \"Al-QƒÅri'ah\": 101\n",
    "    # Tambahkan surah lain sesuai kebutuhan, saat ini hanya yang terpakai di robustness_data\n",
    "}\n",
    "\n",
    "def get_surah_ayat_ids(target_ayats_list):\n",
    "    \"\"\"\n",
    "    Mengambil list lokasi ayat (misal: [\"QS. An-NisƒÅ' : 11\"]) \n",
    "    dan mengembalikannya sebagai list ID numerik [[Surah ID, Ayat ID]].\n",
    "    \"\"\"\n",
    "    result_ids = []\n",
    "    \n",
    "    # Pattern untuk mengekstrak Nama Surah dan Nomor Ayat\n",
    "    # Contoh: \"QS. Al-Baqarah : 183\" -> Group 1: \"Al-Baqarah\", Group 2: \"183\"\n",
    "    pattern = re.compile(r\"QS\\. (.*?)\\s*:\\s*(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "    for ayat_str in target_ayats_list:\n",
    "        match = pattern.search(ayat_str)\n",
    "        if match:\n",
    "            surah_name = match.group(1).strip()\n",
    "            ayat_num = int(match.group(2))\n",
    "            \n",
    "            # Cari ID Surah, default ke -1 jika tidak ditemukan\n",
    "            surah_id = SURAH_MAPPING.get(surah_name, -1)\n",
    "            \n",
    "            if surah_id != -1:\n",
    "                result_ids.append([surah_id, ayat_num])\n",
    "            else:\n",
    "                # Peringatan jika Surah tidak ada di mapping\n",
    "                # Ini adalah BLIND SPOT yang Anda abaikan: kelengkapan mapping!\n",
    "                # print(f\"‚ö†Ô∏è Peringatan: Surah '{surah_name}' tidak ditemukan di SURAH_MAPPING.\")\n",
    "                result_ids.append([-1, ayat_num]) # Tambahkan placeholder ID\n",
    "        \n",
    "        else:\n",
    "            # print(f\"‚ö†Ô∏è Peringatan: Format ayat '{ayat_str}' tidak dikenali.\")\n",
    "            pass\n",
    "\n",
    "    return result_ids\n",
    "\n",
    "# --- 1. DEFINISI PASANGAN QUERY & GROUND TRUTH ---\n",
    "# Struktur: [Query Asli, Query Variasi/Sinonim, Kategori, [Lokasi Ayats Relevan]]\n",
    "# Struktur: [ID, Query A, Query B, Kategori, [Lokasi Ayats (Teks)]]\n",
    "robustness_data = [\n",
    "    # Fiqh/Hukum (Law) - [1-10]\n",
    "    [1, \"Hukum warisan bagi perempuan\", \"Pembagian harta pusaka istri\", \"Fiqh\", [\"QS. An-NisƒÅ' : 11\"]],\n",
    "    [2, \"Kapan puasa Ramadan dimulai\", \"Kewajiban saum di bulan suci\", \"Fiqh\", [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]],\n",
    "    [3, \"Cara melaksanakan sholat Jumat\", \"Ketentuan sembahyang Jumat\", \"Fiqh\", [\"QS. Al-Jumu'ah : 9\"]],\n",
    "    [4, \"Zakat hasil bumi\", \"Kewajiban sedekah pertanian\", \"Fiqh\", [\"QS. Al-An'ƒÅm : 141\"]],\n",
    "    [5, \"Denda bagi yang bersumpah palsu\", \"Konsekuensi sumpah dusta\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 89\"]],\n",
    "    [6, \"Berwudu sebelum salat\", \"Tata cara bersuci sebelum ibadah\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 6\"]],\n",
    "    [7, \"Pernikahan beda agama\", \"Hukum perkawinan non-Muslim\", \"Fiqh\", [\"QS. Al-Baqarah : 221\"]],\n",
    "    [8, \"Larangan memakan riba\", \"Haramnya pinjaman berbunga\", \"Fiqh\", [\"QS. Al-Baqarah : 275\"]],\n",
    "    [9, \"Membayar fidyah karena tidak puasa\", \"Kewajiban ganti rugi puasa\", \"Fiqh\", [\"QS. Al-Baqarah : 184\"]],\n",
    "    [10, \"Apa itu khamar\", \"Definisi minuman memabukkan\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 90\"]],\n",
    "\n",
    "    # Kisah/Narasi (Narrative) - [11-20]\n",
    "    [11, \"Kisah Nabi Musa dan Firaun\", \"Cerita pertemuan Musa dengan raja Mesir\", \"Kisah\", [\"QS. Al-Qa·π£a·π£ : 31\", \"QS. Al-Qa·π£a·π£ : 36\"]],\n",
    "    [12, \"Kapal Nabi Nuh\", \"Perahu raksasa nuh\", \"Kisah\", [\"QS. H≈´d : 44\"]],\n",
    "    [13, \"Maryam melahirkan Isa\", \"Kelahiran putra Maryam\", \"Kisah\", [\"QS. Maryam : 23\", \"QS. Maryam : 27\"]],\n",
    "    [14, \"Nabi Yusuf dan mimpi 11 bintang\", \"Tafsir mimpi nabi Yakub tentang bintang\", \"Kisah\", [\"QS. Y≈´suf : 4\"]],\n",
    "    [15, \"Kisah Ashabul Kahfi\", \"Tujuh pemuda yang tertidur lama\", \"Kisah\", [\"QS. Al-Kahf : 10\", \"QS. Al-Kahf : 25\"]],\n",
    "    [16, \"Kenapa Iblis diusir dari surga\", \"Alasan setan menolak sujud Adam\", \"Kisah\", [\"QS. Al-A‚ÄòrƒÅf : 12\", \"QS. Al-Kahf : 50\"]],\n",
    "    [17, \"Tugas malaikat Jibril\", \"Fungsi Gabriel membawa wahyu\", \"Kisah\", [\"QS. Al-Baqarah : 97\"]],\n",
    "    [18, \"Kisah Qabil dan Habil\", \"Pembunuhan putra Adam\", \"Kisah\", [\"QS. Al-MƒÅ'idah : 27\"]],\n",
    "    [19, \"Raja Thalut dan Jalut\", \"Pertempuran Daud melawan Goliat\", \"Kisah\", [\"QS. Al-Baqarah : 249\", \"QS. Al-Baqarah : 251\"]],\n",
    "    [20, \"Bangsa Ya'juj dan Ma'juj\", \"Siapa Gog dan Magog\", \"Kisah\", [\"QS. Al-Kahf : 94\"]],\n",
    "\n",
    "    # Aqidah/Akhlak (Theology/Ethics) - [21-30]\n",
    "    [21, \"Larangan berbuat syirik\", \"Dosa menyekutukan Allah\", \"Aqidah\", [\"QS. An-NisƒÅ' : 48\", \"QS. LuqmƒÅn : 13\"]],\n",
    "    [22, \"Berbakti pada kedua orang tua\", \"Kewajiban menghormati ayah ibu\", \"Aqidah\", [\"QS. Al-IsrƒÅ' : 23\"]],\n",
    "    [23, \"Definisi tauhid\", \"Konsep keesaan Tuhan\", \"Aqidah\", [\"QS. Al-IkhlƒÅ·π£ : 1\"]],\n",
    "    [24, \"Takdir baik dan buruk\", \"Ketentuan nasib yang ditetapkan Allah\", \"Aqidah\", [\"QS. Al-Qamar : 49\"]],\n",
    "    [25, \"Larangan berbuat dusta\", \"Hukum berkata bohong\", \"Aqidah\", [\"QS. At-Taubah : 119\"]],\n",
    "    [26, \"Tentang Hari Kiamat\", \"Deskripsi Hari Pembalasan\", \"Aqidah\", [\"QS. Al-QƒÅri'ah : 1\", \"QS. Al-Zalzalah : 1\"]],\n",
    "    [27, \"Balasan bagi orang yang sombong\", \"Konsekuensi sifat takabur\", \"Aqidah\", [\"QS. LuqmƒÅn : 18\"]],\n",
    "    [28, \"Larangan mengumpat\", \"Hukum ghibah dan mencela\", \"Aqidah\", [\"QS. Al-·∏§ujurƒÅt : 12\"]],\n",
    "    [29, \"Keutamaan sabar\", \"Pentingnya menahan diri\", \"Aqidah\", [\"QS. Al-Baqarah : 153\"]],\n",
    "    [30, \"Tujuan hidup manusia\", \"Mengapa kita diciptakan\", \"Aqidah\", [\"QS. Adz-DzƒÅriyƒÅt : 56\"]],\n",
    "]\n",
    "\n",
    "# 1.5. TAMBAHKAN KOLOM ID NUMERIK\n",
    "# Lakukan pemrosesan ID numerik di sini sebelum konversi ke DataFrame\n",
    "data_with_ids = []\n",
    "for row in robustness_data:\n",
    "    row_id, q_a, q_b, category, target_ayats_text = row\n",
    "    \n",
    "    # Panggil fungsi untuk mendapatkan ID numerik\n",
    "    target_ayats_id = get_surah_ayat_ids(target_ayats_text)\n",
    "    \n",
    "    # Struktur baru: [ID, Query A, Query B, Kategori, [Ayats Teks], [Ayats ID]]\n",
    "    data_with_ids.append([row_id, q_a, q_b, category, target_ayats_text, target_ayats_id])\n",
    "\n",
    "# 2. KONVERSI KE DATAFRAME\n",
    "df_robustness = pd.DataFrame(data_with_ids, columns=[\n",
    "    'id', 'query_a', 'query_b', 'category', 'target_ayats_text', 'target_ayats_id'\n",
    "])\n",
    "\n",
    "# 3. DUPLIKASI BARIS: Buat setiap Query A dan Query B menjadi baris terpisah\n",
    "# Kolom target_ayats_text diganti namanya agar sesuai dengan nama lama (jika ada script lain yang bergantung padanya)\n",
    "rows_a = df_robustness.rename(columns={'query_a': 'query', 'target_ayats_text': 'target_ayats'})[['id', 'query', 'category', 'target_ayats', 'target_ayats_id']]\n",
    "rows_b = df_robustness.rename(columns={'query_b': 'query', 'target_ayats_text': 'target_ayats'})[['id', 'query', 'category', 'target_ayats', 'target_ayats_id']]\n",
    "\n",
    "df_test_set = pd.concat([rows_a, rows_b], ignore_index=True)\n",
    "\n",
    "# 4. SIMPAN KE CSV\n",
    "# Kita simpan kolom list sebagai string JSON\n",
    "df_test_set['target_ayats'] = df_test_set['target_ayats'].apply(lambda x: json.dumps(x))\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats_id'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "\n",
    "df_test_set.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FILE TEST SET ROBUSTNESS BERHASIL DIBUAT (dengan ID Surah & Ayat)!\")\n",
    "print(f\" ¬† Tersimpan di: {OUTPUT_PATH}\")\n",
    "print(f\" ¬† Total Query Uji: {len(df_test_set)} baris ({len(df_robustness)} pasangan).\")\n",
    "\n",
    "print(\"\\nSTRUKTUR DATA (Preview dengan kolom ID baru):\")\n",
    "# Tampilkan 6 baris pertama dan kolom yang relevan\n",
    "print(df_test_set[['id', 'query', 'category', 'target_ayats', 'target_ayats_id']].head(6).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe7ef5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FILE TEST SET ROBUSTNESS BERHASIL DIBUAT!\n",
      "   Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\robustness_queries.csv\n",
      "   Total Query Uji: 60 baris (30.0 pasangan).\n",
      "\n",
      "STRUKTUR DATA (Preview):\n",
      "   id                            query category  \\\n",
      "0   1     Hukum warisan bagi perempuan     Fiqh   \n",
      "1   2      Kapan puasa Ramadan dimulai     Fiqh   \n",
      "2   3   Cara melaksanakan sholat Jumat     Fiqh   \n",
      "3   4                 Zakat hasil bumi     Fiqh   \n",
      "4   5  Denda bagi yang bersumpah palsu     Fiqh   \n",
      "5   6            Berwudu sebelum salat     Fiqh   \n",
      "\n",
      "                                       target_ayats  \n",
      "0                        [\"QS. An-Nis\\u0101' : 11\"]  \n",
      "1  [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]  \n",
      "2                            [\"QS. Al-Jumu'ah : 9\"]  \n",
      "3                       [\"QS. Al-An'\\u0101m : 141\"]  \n",
      "4                      [\"QS. Al-M\\u0101'idah : 89\"]  \n",
      "5                       [\"QS. Al-M\\u0101'idah : 6\"]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "# Notebook ada di folder notebooks/, jadi naik 1 level ke root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'robustness_queries.csv')\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# --- 1. DEFINISI PASANGAN QUERY & GROUND TRUTH ---\n",
    "# Struktur: [Query Asli, Query Variasi/Sinonim, Kategori, [Lokasi Ayats Relevan]]\n",
    "robustness_data = [\n",
    "    # Fiqh/Hukum (Law) - [1-10]\n",
    "    [1, \"Hukum warisan bagi perempuan\", \"Pembagian harta pusaka istri\", \"Fiqh\", [\"QS. An-NisƒÅ' : 11\"]],\n",
    "    [2, \"Kapan puasa Ramadan dimulai\", \"Kewajiban saum di bulan suci\", \"Fiqh\", [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]],\n",
    "    [3, \"Cara melaksanakan sholat Jumat\", \"Ketentuan sembahyang Jumat\", \"Fiqh\", [\"QS. Al-Jumu'ah : 9\"]],\n",
    "    [4, \"Zakat hasil bumi\", \"Kewajiban sedekah pertanian\", \"Fiqh\", [\"QS. Al-An'ƒÅm : 141\"]],\n",
    "    [5, \"Denda bagi yang bersumpah palsu\", \"Konsekuensi sumpah dusta\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 89\"]],\n",
    "    [6, \"Berwudu sebelum salat\", \"Tata cara bersuci sebelum ibadah\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 6\"]],\n",
    "    [7, \"Pernikahan beda agama\", \"Hukum perkawinan non-Muslim\", \"Fiqh\", [\"QS. Al-Baqarah : 221\"]],\n",
    "    [8, \"Larangan memakan riba\", \"Haramnya pinjaman berbunga\", \"Fiqh\", [\"QS. Al-Baqarah : 275\"]],\n",
    "    [9, \"Membayar fidyah karena tidak puasa\", \"Kewajiban ganti rugi puasa\", \"Fiqh\", [\"QS. Al-Baqarah : 184\"]],\n",
    "    [10, \"Apa itu khamar\", \"Definisi minuman memabukkan\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 90\"]],\n",
    "\n",
    "    # Kisah/Narasi (Narrative) - [11-20]\n",
    "    [11, \"Kisah Nabi Musa dan Firaun\", \"Cerita pertemuan Musa dengan raja Mesir\", \"Kisah\", [\"QS. Al-Qa·π£a·π£ : 31\", \"QS. Al-Qa·π£a·π£ : 36\"]],\n",
    "    [12, \"Kapal Nabi Nuh\", \"Perahu raksasa nuh\", \"Kisah\", [\"QS. H≈´d : 44\"]],\n",
    "    [13, \"Maryam melahirkan Isa\", \"Kelahiran putra Maryam\", \"Kisah\", [\"QS. Maryam : 23\", \"QS. Maryam : 27\"]],\n",
    "    [14, \"Nabi Yusuf dan mimpi 11 bintang\", \"Tafsir mimpi nabi Yakub tentang bintang\", \"Kisah\", [\"QS. Y≈´suf : 4\"]],\n",
    "    [15, \"Kisah Ashabul Kahfi\", \"Tujuh pemuda yang tertidur lama\", \"Kisah\", [\"QS. Al-Kahf : 10\", \"QS. Al-Kahf : 25\"]],\n",
    "    [16, \"Kenapa Iblis diusir dari surga\", \"Alasan setan menolak sujud Adam\", \"Kisah\", [\"QS. Al-A'rƒÅf : 12\", \"QS. Al-Kahf : 50\"]],\n",
    "    [17, \"Tugas malaikat Jibril\", \"Fungsi Gabriel membawa wahyu\", \"Kisah\", [\"QS. Al-Baqarah : 97\"]],\n",
    "    [18, \"Kisah Qabil dan Habil\", \"Pembunuhan putra Adam\", \"Kisah\", [\"QS. Al-MƒÅ'idah : 27\"]],\n",
    "    [19, \"Raja Thalut dan Jalut\", \"Pertempuran Daud melawan Goliat\", \"Kisah\", [\"QS. Al-Baqarah : 249\", \"QS. Al-Baqarah : 251\"]],\n",
    "    [20, \"Bangsa Ya'juj dan Ma'juj\", \"Siapa Gog dan Magog\", \"Kisah\", [\"QS. Al-Kahf : 94\"]],\n",
    "\n",
    "    # Aqidah/Akhlak (Theology/Ethics) - [21-30]\n",
    "    [21, \"Larangan berbuat syirik\", \"Dosa menyekutukan Allah\", \"Aqidah\", [\"QS. An-NisƒÅ' : 48\", \"QS. LuqmƒÅn : 13\"]],\n",
    "    [22, \"Berbakti pada kedua orang tua\", \"Kewajiban menghormati ayah ibu\", \"Aqidah\", [\"QS. Al-IsrƒÅ' : 23\"]],\n",
    "    [23, \"Definisi tauhid\", \"Konsep keesaan Tuhan\", \"Aqidah\", [\"QS. Al-IkhlƒÅ·π£ : 1\"]],\n",
    "    [24, \"Takdir baik dan buruk\", \"Ketentuan nasib yang ditetapkan Allah\", \"Aqidah\", [\"QS. Al-Qamar : 49\"]],\n",
    "    [25, \"Larangan berbuat dusta\", \"Hukum berkata bohong\", \"Aqidah\", [\"QS. At-Taubah : 119\"]],\n",
    "    [26, \"Tentang Hari Kiamat\", \"Deskripsi Hari Pembalasan\", \"Aqidah\", [\"QS. Al-QƒÅri'ah : 1\", \"QS. Al-Zalzalah : 1\"]],\n",
    "    [27, \"Balasan bagi orang yang sombong\", \"Konsekuensi sifat takabur\", \"Aqidah\", [\"QS. LuqmƒÅn : 18\"]],\n",
    "    [28, \"Larangan mengumpat\", \"Hukum ghibah dan mencela\", \"Aqidah\", [\"QS. Al-·∏§ujurƒÅt : 12\"]],\n",
    "    [29, \"Keutamaan sabar\", \"Pentingnya menahan diri\", \"Aqidah\", [\"QS. Al-Baqarah : 153\"]],\n",
    "    [30, \"Tujuan hidup manusia\", \"Mengapa kita diciptakan\", \"Aqidah\", [\"QS. Adz-DzƒÅriyƒÅt : 56\"]],\n",
    "]\n",
    "\n",
    "# 2. KONVERSI KE DATAFRAME\n",
    "df_robustness = pd.DataFrame(robustness_data, columns=[\n",
    "    'id', 'query_a', 'query_b', 'category', 'target_ayats'\n",
    "])\n",
    "\n",
    "# 3. DUPLIKASI BARIS: Buat setiap Query A dan Query B menjadi baris terpisah\n",
    "rows_a = df_robustness.rename(columns={'query_a': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "rows_b = df_robustness.rename(columns={'query_b': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "\n",
    "df_test_set = pd.concat([rows_a, rows_b], ignore_index=True)\n",
    "\n",
    "# 4. SIMPAN KE CSV\n",
    "df_test_set['target_ayats'] = df_test_set['target_ayats'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "df_test_set.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FILE TEST SET ROBUSTNESS BERHASIL DIBUAT!\")\n",
    "print(f\"   Tersimpan di: {OUTPUT_PATH}\")\n",
    "print(f\"   Total Query Uji: {len(df_test_set)} baris ({len(df_test_set)/2} pasangan).\")\n",
    "\n",
    "print(\"\\nSTRUKTUR DATA (Preview):\")\n",
    "print(df_test_set.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46fd556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FILE TEST SET ROBUSTNESS BERHASIL DIBUAT (dengan ID Surah & Ayat)!\n",
      " ¬† Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\robustness_queries.csv\n",
      " ¬† Total Query Uji: 60 baris (30 pasangan).\n",
      "\n",
      "STRUKTUR DATA (Preview dengan kolom ID baru):\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1398\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n\u001b[32m   1397\u001b[39m     name = _resolve_name(name, package, level)\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_find_and_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1371\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m _NEEDS_LOADING:\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_find_and_load_unlocked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimport_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;66;03m# Optimization: only call _bootstrap._lock_unlock_module() if\u001b[39;00m\n\u001b[32m   1374\u001b[39m \u001b[38;5;66;03m# module.__spec__._initializing is True.\u001b[39;00m\n\u001b[32m   1375\u001b[39m \u001b[38;5;66;03m# NOTE: because of this, initializing must be set *before*\u001b[39;00m\n\u001b[32m   1376\u001b[39m \u001b[38;5;66;03m# putting the new module in sys.modules.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1335\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ERR_MSG_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m, name=name)\n\u001b[32m   1336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSTRUKTUR DATA (Preview dengan kolom ID baru):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# Tampilkan 6 baris pertama dan kolom yang relevan\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_test_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget_ayats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget_ayats_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\frame.py:2994\u001b[39m, in \u001b[36mDataFrame.to_markdown\u001b[39m\u001b[34m(self, buf, mode, index, storage_options, **kwargs)\u001b[39m\n\u001b[32m   2992\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtablefmt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2993\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mshowindex\u001b[39m\u001b[33m\"\u001b[39m, index)\n\u001b[32m-> \u001b[39m\u001b[32m2994\u001b[39m tabulate = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabulate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2995\u001b[39m result = tabulate.tabulate(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re # Diperlukan untuk parsing nama surah dan ayat\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        # Asumsi: jika tidak ada folder data, kita naik satu level\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'robustness_queries.csv')\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# --- FUNGSI UTILITY: MENDAPATKAN ID SURAH DAN AYAT ---\n",
    "\n",
    "# Mapping sebagian Surah yang digunakan ke ID numeriknya\n",
    "# Jika Anda membutuhkan 114 Surah, gunakan file mapping lengkap (JSON/CSV)\n",
    "SURAH_MAPPING = {\n",
    "    \"Al-Baqarah\": 2, \"ƒÄli 'ImrƒÅn\": 3, \"An-NisƒÅ'\": 4, \"Al-MƒÅ'idah\": 5, \n",
    "    \"Al-An'ƒÅm\": 6, \"Al-A‚ÄòrƒÅf\": 7, \"At-Taubah\": 9, \"H≈´d\": 11, \n",
    "    \"Y≈´suf\": 12, \"Ar-Ra‚Äòd\": 13, \"Al-IsrƒÅ'\": 17, \"Al-Kahf\": 18, \n",
    "    \"Maryam\": 19, \"Al-AnbiyƒÅ'\": 21, \"Al-Qa·π£a·π£\": 28, \"Al-‚ÄòAnkab≈´t\": 29,\n",
    "    \"LuqmƒÅn\": 31, \"Al-A·∏•zƒÅb\": 33, \"YƒÅ-Sƒ´n\": 36, \"A·∫ì-·∫íƒÅriyƒÅt\": 51,\n",
    "    \"Al-Qamar\": 54, \"Al-·∏§ujurƒÅt\": 49, \"Adz-DzƒÅriyƒÅt\": 51, \"Al-Jumu'ah\": 62,\n",
    "    \"At-TaghƒÅbun\": 64, \"Al-Qalam\": 68, \"Al-Ma‚ÄòƒÅrij\": 70, \"Al-QiyƒÅmah\": 75,\n",
    "    \"Al-IkhlƒÅ·π£\": 112, \"Al-Zalzalah\": 99, \"Al-QƒÅri'ah\": 101\n",
    "    # Tambahkan surah lain sesuai kebutuhan, saat ini hanya yang terpakai di robustness_data\n",
    "}\n",
    "\n",
    "def get_surah_ayat_ids(target_ayats_list):\n",
    "    \"\"\"\n",
    "    Mengambil list lokasi ayat (misal: [\"QS. An-NisƒÅ' : 11\"]) \n",
    "    dan mengembalikannya sebagai list ID numerik [[Surah ID, Ayat ID]].\n",
    "    \"\"\"\n",
    "    result_ids = []\n",
    "    \n",
    "    # Pattern untuk mengekstrak Nama Surah dan Nomor Ayat\n",
    "    # Contoh: \"QS. Al-Baqarah : 183\" -> Group 1: \"Al-Baqarah\", Group 2: \"183\"\n",
    "    pattern = re.compile(r\"QS\\. (.*?)\\s*:\\s*(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "    for ayat_str in target_ayats_list:\n",
    "        match = pattern.search(ayat_str)\n",
    "        if match:\n",
    "            surah_name = match.group(1).strip()\n",
    "            ayat_num = int(match.group(2))\n",
    "            \n",
    "            # Cari ID Surah, default ke -1 jika tidak ditemukan\n",
    "            surah_id = SURAH_MAPPING.get(surah_name, -1)\n",
    "            \n",
    "            if surah_id != -1:\n",
    "                result_ids.append([surah_id, ayat_num])\n",
    "            else:\n",
    "                # Peringatan jika Surah tidak ada di mapping\n",
    "                # Ini adalah BLIND SPOT yang Anda abaikan: kelengkapan mapping!\n",
    "                # print(f\"‚ö†Ô∏è Peringatan: Surah '{surah_name}' tidak ditemukan di SURAH_MAPPING.\")\n",
    "                result_ids.append([-1, ayat_num]) # Tambahkan placeholder ID\n",
    "        \n",
    "        else:\n",
    "            # print(f\"‚ö†Ô∏è Peringatan: Format ayat '{ayat_str}' tidak dikenali.\")\n",
    "            pass\n",
    "\n",
    "    return result_ids\n",
    "\n",
    "# --- 1. DEFINISI PASANGAN QUERY & GROUND TRUTH ---\n",
    "# Struktur: [Query Asli, Query Variasi/Sinonim, Kategori, [Lokasi Ayats Relevan]]\n",
    "# Struktur: [ID, Query A, Query B, Kategori, [Lokasi Ayats (Teks)]]\n",
    "robustness_data = [\n",
    "    # Fiqh/Hukum (Law) - [1-10]\n",
    "    [1, \"Hukum warisan bagi perempuan\", \"Pembagian harta pusaka istri\", \"Fiqh\", [\"QS. An-NisƒÅ' : 11\"]],\n",
    "    [2, \"Kapan puasa Ramadan dimulai\", \"Kewajiban saum di bulan suci\", \"Fiqh\", [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]],\n",
    "    [3, \"Cara melaksanakan sholat Jumat\", \"Ketentuan sembahyang Jumat\", \"Fiqh\", [\"QS. Al-Jumu'ah : 9\"]],\n",
    "    [4, \"Zakat hasil bumi\", \"Kewajiban sedekah pertanian\", \"Fiqh\", [\"QS. Al-An'ƒÅm : 141\"]],\n",
    "    [5, \"Denda bagi yang bersumpah palsu\", \"Konsekuensi sumpah dusta\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 89\"]],\n",
    "    [6, \"Berwudu sebelum salat\", \"Tata cara bersuci sebelum ibadah\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 6\"]],\n",
    "    [7, \"Pernikahan beda agama\", \"Hukum perkawinan non-Muslim\", \"Fiqh\", [\"QS. Al-Baqarah : 221\"]],\n",
    "    [8, \"Larangan memakan riba\", \"Haramnya pinjaman berbunga\", \"Fiqh\", [\"QS. Al-Baqarah : 275\"]],\n",
    "    [9, \"Membayar fidyah karena tidak puasa\", \"Kewajiban ganti rugi puasa\", \"Fiqh\", [\"QS. Al-Baqarah : 184\"]],\n",
    "    [10, \"Apa itu khamar\", \"Definisi minuman memabukkan\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 90\"]],\n",
    "\n",
    "    # Kisah/Narasi (Narrative) - [11-20]\n",
    "    [11, \"Kisah Nabi Musa dan Firaun\", \"Cerita pertemuan Musa dengan raja Mesir\", \"Kisah\", [\"QS. Al-Qa·π£a·π£ : 31\", \"QS. Al-Qa·π£a·π£ : 36\"]],\n",
    "    [12, \"Kapal Nabi Nuh\", \"Perahu raksasa nuh\", \"Kisah\", [\"QS. H≈´d : 44\"]],\n",
    "    [13, \"Maryam melahirkan Isa\", \"Kelahiran putra Maryam\", \"Kisah\", [\"QS. Maryam : 23\", \"QS. Maryam : 27\"]],\n",
    "    [14, \"Nabi Yusuf dan mimpi 11 bintang\", \"Tafsir mimpi nabi Yakub tentang bintang\", \"Kisah\", [\"QS. Y≈´suf : 4\"]],\n",
    "    [15, \"Kisah Ashabul Kahfi\", \"Tujuh pemuda yang tertidur lama\", \"Kisah\", [\"QS. Al-Kahf : 10\", \"QS. Al-Kahf : 25\"]],\n",
    "    [16, \"Kenapa Iblis diusir dari surga\", \"Alasan setan menolak sujud Adam\", \"Kisah\", [\"QS. Al-A‚ÄòrƒÅf : 12\", \"QS. Al-Kahf : 50\"]],\n",
    "    [17, \"Tugas malaikat Jibril\", \"Fungsi Gabriel membawa wahyu\", \"Kisah\", [\"QS. Al-Baqarah : 97\"]],\n",
    "    [18, \"Kisah Qabil dan Habil\", \"Pembunuhan putra Adam\", \"Kisah\", [\"QS. Al-MƒÅ'idah : 27\"]],\n",
    "    [19, \"Raja Thalut dan Jalut\", \"Pertempuran Daud melawan Goliat\", \"Kisah\", [\"QS. Al-Baqarah : 249\", \"QS. Al-Baqarah : 251\"]],\n",
    "    [20, \"Bangsa Ya'juj dan Ma'juj\", \"Siapa Gog dan Magog\", \"Kisah\", [\"QS. Al-Kahf : 94\"]],\n",
    "\n",
    "    # Aqidah/Akhlak (Theology/Ethics) - [21-30]\n",
    "    [21, \"Larangan berbuat syirik\", \"Dosa menyekutukan Allah\", \"Aqidah\", [\"QS. An-NisƒÅ' : 48\", \"QS. LuqmƒÅn : 13\"]],\n",
    "    [22, \"Berbakti pada kedua orang tua\", \"Kewajiban menghormati ayah ibu\", \"Aqidah\", [\"QS. Al-IsrƒÅ' : 23\"]],\n",
    "    [23, \"Definisi tauhid\", \"Konsep keesaan Tuhan\", \"Aqidah\", [\"QS. Al-IkhlƒÅ·π£ : 1\"]],\n",
    "    [24, \"Takdir baik dan buruk\", \"Ketentuan nasib yang ditetapkan Allah\", \"Aqidah\", [\"QS. Al-Qamar : 49\"]],\n",
    "    [25, \"Larangan berbuat dusta\", \"Hukum berkata bohong\", \"Aqidah\", [\"QS. At-Taubah : 119\"]],\n",
    "    [26, \"Tentang Hari Kiamat\", \"Deskripsi Hari Pembalasan\", \"Aqidah\", [\"QS. Al-QƒÅri'ah : 1\", \"QS. Al-Zalzalah : 1\"]],\n",
    "    [27, \"Balasan bagi orang yang sombong\", \"Konsekuensi sifat takabur\", \"Aqidah\", [\"QS. LuqmƒÅn : 18\"]],\n",
    "    [28, \"Larangan mengumpat\", \"Hukum ghibah dan mencela\", \"Aqidah\", [\"QS. Al-·∏§ujurƒÅt : 12\"]],\n",
    "    [29, \"Keutamaan sabar\", \"Pentingnya menahan diri\", \"Aqidah\", [\"QS. Al-Baqarah : 153\"]],\n",
    "    [30, \"Tujuan hidup manusia\", \"Mengapa kita diciptakan\", \"Aqidah\", [\"QS. Adz-DzƒÅriyƒÅt : 56\"]],\n",
    "]\n",
    "\n",
    "# 1.5. TAMBAHKAN KOLOM ID NUMERIK\n",
    "# Lakukan pemrosesan ID numerik di sini sebelum konversi ke DataFrame\n",
    "data_with_ids = []\n",
    "for row in robustness_data:\n",
    "    row_id, q_a, q_b, category, target_ayats_text = row\n",
    "    \n",
    "    # Panggil fungsi untuk mendapatkan ID numerik\n",
    "    target_ayats_id = get_surah_ayat_ids(target_ayats_text)\n",
    "    \n",
    "    # Struktur baru: [ID, Query A, Query B, Kategori, [Ayats Teks], [Ayats ID]]\n",
    "    data_with_ids.append([row_id, q_a, q_b, category, target_ayats_text, target_ayats_id])\n",
    "\n",
    "# 2. KONVERSI KE DATAFRAME\n",
    "df_robustness = pd.DataFrame(data_with_ids, columns=[\n",
    "    'id', 'query_a', 'query_b', 'category', 'target_ayats_text', 'target_ayats_id'\n",
    "])\n",
    "\n",
    "# 3. DUPLIKASI BARIS: Buat setiap Query A dan Query B menjadi baris terpisah\n",
    "# Kolom target_ayats_text diganti namanya agar sesuai dengan nama lama (jika ada script lain yang bergantung padanya)\n",
    "rows_a = df_robustness.rename(columns={'query_a': 'query', 'target_ayats_text': 'target_ayats'})[['id', 'query', 'category', 'target_ayats', 'target_ayats_id']]\n",
    "rows_b = df_robustness.rename(columns={'query_b': 'query', 'target_ayats_text': 'target_ayats'})[['id', 'query', 'category', 'target_ayats', 'target_ayats_id']]\n",
    "\n",
    "df_test_set = pd.concat([rows_a, rows_b], ignore_index=True)\n",
    "\n",
    "# 4. SIMPAN KE CSV\n",
    "# Kita simpan kolom list sebagai string JSON\n",
    "df_test_set['target_ayats'] = df_test_set['target_ayats'].apply(lambda x: json.dumps(x))\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats_id'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "\n",
    "df_test_set.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FILE TEST SET ROBUSTNESS BERHASIL DIBUAT (dengan ID Surah & Ayat)!\")\n",
    "print(f\" ¬† Tersimpan di: {OUTPUT_PATH}\")\n",
    "print(f\" ¬† Total Query Uji: {len(df_test_set)} baris ({len(df_robustness)} pasangan).\")\n",
    "\n",
    "print(\"\\nSTRUKTUR DATA (Preview dengan kolom ID baru):\")\n",
    "# Tampilkan 6 baris pertama dan kolom yang relevan\n",
    "print(df_test_set[['id', 'query', 'category', 'target_ayats', 'target_ayats_id']].head(6).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14153708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FILE TEST SET ROBUSTNESS ID-BASED BERHASIL DIBUAT (OUTPUT FIX)!\n",
      "   Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\robustness_queries_ID_BASED.csv\n",
      "   Total Query Uji: 60 baris.\n",
      "   Total Target Ayats yang berhasil di-map: 0. (Harusnya 43)\n",
      "\n",
      "STRUKTUR DATA (Preview, cek apakah list sudah terisi):\n",
      " id                           query category target_ayats_id\n",
      "  1    Hukum warisan bagi perempuan     Fiqh              []\n",
      "  2     Kapan puasa Ramadan dimulai     Fiqh              []\n",
      "  3  Cara melaksanakan sholat Jumat     Fiqh              []\n",
      "  4                Zakat hasil bumi     Fiqh              []\n",
      "  5 Denda bagi yang bersumpah palsu     Fiqh              []\n",
      "  6           Berwudu sebelum salat     Fiqh              []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'robustness_queries_ID_BASED.csv')\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# 1. Daftar Kanonik 114 Surah (ID 1-114) - Kunci Perbaikan Data\n",
    "canonical_surahs_simple = [\n",
    "    'Al-Fatihah', 'Al-Baqarah', 'Ali Imran', 'An-Nisa', 'Al-Maidah', 'Al-Anam', 'Al-Araf', 'Al-Anfal', \n",
    "    'At-Taubah', 'Yunus', 'Hud', 'Yusuf', 'Ar-Rad', 'Ibrahim', 'Al-Hijr', 'An-Nahl', 'Al-Isra', 'Al-Kahf', \n",
    "    'Maryam', 'Taha', 'Al-Anbiya', 'Al-Haj', 'Al-Muminun', 'An-Nur', 'Al-Furqan', 'Asy-Syuara', \n",
    "    'An-Naml', 'Al-Qasas', 'Al-Ankabut', 'Ar-Rum', 'Luqman', 'As-Sajdah', 'Al-Ahzab', 'Saba', 'Fatir', \n",
    "    'Yasin', 'As-Saffat', 'Sad', 'Az-Zumar', 'Gafir', 'Fussilat', 'Asy-Syura', 'Az-Zukhruf', 'Ad-Dukhan', \n",
    "    'Al-Jasiyah', 'Al-Ahqaf', 'Muhammad', 'Al-Fath', 'Al-Hujurat', 'Qaf', 'Adz-Zariyat', 'At-Tur', 'An-Najm', \n",
    "    'Al-Qamar', 'Ar-Rahman', 'Al-Waqiah', 'Al-Hadid', 'Al-Mujadilah', 'Al-Hasyr', 'Al-Mumtahanah', 'As-Saff', \n",
    "    'Al-Jumuah', 'Al-Munafiqun', 'At-Tagabun', 'At-Talaq', 'At-Tahrim', 'Al-Mulk', 'Al-Qalam', 'Al-Haqqah', \n",
    "    'Al-Maarij', 'Nuh', 'Al-Jinn', 'Al-Muzzammil', 'Al-Muddassir', 'Al-Qiyamah', 'Al-Insan', 'Al-Mursalat', \n",
    "    'An-Naba', 'An-Naziat', 'Abasa', 'At-Takwir', 'Al-Infitar', 'Al-Mutaffifin', 'Al-Insyiqaq', \n",
    "    'Al-Buruj', 'At-Tariq', 'Al-Ala', 'Al-Gasyiyah', 'Al-Fajr', 'Al-Balad', 'Asy-Syams', 'Al-Lail', 'Ad-Duha', \n",
    "    'Al-Insyirah', 'At-Tin', 'Al-Alaq', 'Al-Qadr', 'Al-Bayyinah', 'Az-Zalzalah', 'Al-Adiyat', 'Al-Qariah', \n",
    "    'At-Takasur', 'Al-Asr', 'Al-Humazah', 'Al-Fil', 'Quraisy', 'Al-Maun', 'Al-Kausar', 'Al-Kafirun', \n",
    "    'An-Nasr', 'Al-Lahab', 'Al-Ikhlas', 'Al-Falaq', 'An-Nas'\n",
    "]\n",
    "\n",
    "surah_name_to_id = {name: i + 1 for i, name in enumerate(canonical_surahs_simple)}\n",
    "\n",
    "# 2. Fungsi Konversi String Lokasi ke ID Numerik\n",
    "# Tabel manual untuk mapping karakter Unicode di input string ke nama sederhana di list kanonik\n",
    "translation_table = {\n",
    "    'An-NisƒÅ\\'': 'An-Nisa', 'Al-Baqarah': 'Al-Baqarah', 'Al-Jumu\\'ah': 'Al-Jumuah', 'Al-An\\'ƒÅm': 'Al-Anam', \n",
    "    'Al-MƒÅ\\'idah': 'Al-Maidah', 'Al-Qa·π£a·π£': 'Al-Qasas', 'H≈´d': 'Hud', 'Maryam': 'Maryam', 'Y≈´suf': 'Yusuf', \n",
    "    'Al-Kahf': 'Al-Kahf', 'Al-A‚ÄòrƒÅf': 'Al-Araf', 'LuqmƒÅn': 'Luqman', 'Al-IsrƒÅ\\'': 'Al-Isra', \n",
    "    'At-Taubah': 'At-Taubah', 'Al-IkhlƒÅ·π£': 'Al-Ikhlas', 'Al-Qamar': 'Al-Qamar', 'Adz-DzƒÅriyƒÅt': 'Adz-Zariyat', \n",
    "    'Al-·∏§ujurƒÅt': 'Al-Hujurat', 'Al-QƒÅri\\'ah': 'Al-Qariah', 'Az-Zalzalah': 'Az-Zalzalah', 'Al-KƒÅfir≈´n': 'Al-Kafirun', \n",
    "    'Ar-Ra·∏•mƒÅn': 'Ar-Rahman', 'Al-AnbiyƒÅ\\'': 'Al-Anbiya', 'Al-FƒÅti·∏•ah': 'Al-Fatihah', 'ƒÄli ‚ÄòImrƒÅn': 'Ali Imran', \n",
    "    'Al-Fƒ´l': 'Al-Fil', 'Al-MƒÅ‚Äò≈´n': 'Al-Maun', 'Al-Kausar': 'Al-Kausar', 'An-Na·π£r': 'An-Nasr', 'Al-Lahab': 'Al-Lahab', \n",
    "    'Al-Falaq': 'Al-Falaq', 'An-NƒÅs': 'An-Nas', 'Al-AnfƒÅl': 'Al-Anfal', 'Al-A·∏•zƒÅb': 'Al-Ahzab', 'Al-InsƒÅn': 'Al-Insan',\n",
    "    'Al-Kausar': 'Al-Kausar', 'Al-Fƒ´l': 'Al-Fil', 'Quraisy': 'Quraisy', 'Al-MƒÅ‚Äò≈´n': 'Al-Maun', 'Al-KƒÅfir≈´n': 'Al-Kafirun',\n",
    "    'Adz-DzƒÅriyƒÅt': 'Adz-Zariyat', 'A·π£-·π¢affƒÅt': 'As-Saffat', '·π¢ƒÅd': 'Sad', 'Az-Zumar': 'Az-Zumar', 'GƒÅfir': 'Gafir',\n",
    "    'Fu·π£·π£ilat': 'Fussilat', 'Asy-Sy≈´rƒÅ': 'Asy-Syura', 'Az-Zukhruf': 'Az-Zukhruf', 'Ad-DukhƒÅn': 'Ad-Dukhan',\n",
    "    'Al-JƒÅ·π°iyah': 'Al-Jasiyah', 'Al-A·∏•qƒÅf': 'Al-Ahqaf', 'Mu·∏•ammad': 'Muhammad', 'Al-Fat·∏•': 'Al-Fath', \n",
    "    'Al-MujƒÅdilah': 'Al-Mujadilah', 'Al-·∏§asyr': 'Al-Hasyr', 'Al-Mumta·∏•anah': 'Al-Mumtahanah', 'A·π£-·π¢aff': 'As-Saff',\n",
    "    'Al-Jumu‚Äòah': 'Al-Jumuah', 'Al-MunƒÅfiq≈´n': 'Al-Munafiqun', 'At-TagƒÅbun': 'At-Tagabun', 'A·π≠-·π¨alƒÅq': 'At-Talaq',\n",
    "    'At-Ta·∏•rƒ´m': 'At-Tahrim', 'Al-Mulk': 'Al-Mulk', 'Al-Qalam': 'Al-Qalam', 'Al-·∏§ƒÅqqah': 'Al-Haqqah',\n",
    "    'Al-Ma‚ÄòƒÅrij': 'Al-Maarij', 'N≈´·∏•': 'Nuh', 'Al-Jinn': 'Al-Jinn', 'Al-Muzzammil': 'Al-Muzzammil',\n",
    "    'Al-Muddasir': 'Al-Muddassir', 'Al-QiyƒÅmah': 'Al-Qiyamah', 'Al-MursalƒÅt': 'Al-Mursalat', 'An-Naba\\'': 'An-Naba',\n",
    "    'An-NƒÅzi‚ÄòƒÅt': 'An-Naziat', '‚ÄòAbasa': 'Abasa', 'At-Takwƒ´r': 'At-Takwir', 'Al-Infi·π≠ƒÅr': 'Al-Infitar',\n",
    "    'Al-Mu·π≠affifƒ´n': 'Al-Mutaffifin', 'Al-InsyiqƒÅq': 'Al-Insyiqaq', 'Al-Bur≈´j': 'Al-Buruj', 'A·π≠-·π¨ƒÅriq': 'At-Tariq',\n",
    "    'Al-A‚ÄòlƒÅ': 'Al-Ala', 'Al-GƒÅsyiyah': 'Al-Gasyiyah', 'Al-Fajr': 'Al-Fajr', 'Al-Balad': 'Al-Balad',\n",
    "    'Asy-Syams': 'Asy-Syams', 'Al-Lail': 'Al-Lail', 'A·∏ç-·∏åu·∏•ƒÅ': 'Ad-Duha', 'Al-InsyirƒÅ·∏•': 'Al-Insyirah',\n",
    "    'At-Tƒ´n': 'At-Tin', 'Al-‚ÄòAlaq': 'Al-Alaq', 'Al-Qadr': 'Al-Qadr', 'Al-Bayyinah': 'Al-Bayyinah',\n",
    "    'Az-Zalzalah': 'Az-Zalzalah', 'Al-Adiyat': 'Al-Adiyat', 'Al-QƒÅri‚Äòah': 'Al-Qariah', 'At-TakƒÅ·π°ur': 'At-Takasur',\n",
    "    'Al-‚ÄòA·π£r': 'Al-Asr', 'Al-Humazah': 'Al-Humazah'\n",
    "}\n",
    "\n",
    "def convert_to_numeric_id(location_str_list):\n",
    "    numeric_targets = []\n",
    "    \n",
    "    for loc_str in location_str_list:\n",
    "        if not isinstance(loc_str, str) or \"QS.\" not in loc_str: continue\n",
    "            \n",
    "        match_surah = re.search(r'QS\\. (.+?)\\s+:', loc_str)\n",
    "        match_ayah = re.search(r'Ayat\\s+(\\d+)', loc_str)\n",
    "        \n",
    "        if match_surah and match_ayah:\n",
    "            surah_name_raw = match_surah.group(1).strip()\n",
    "            ayah_id = int(match_ayah.group(1))\n",
    "            \n",
    "            # KUNCI PERBAIKAN: Gunakan tabel translasi\n",
    "            simple_name = translation_table.get(surah_name_raw)\n",
    "            surah_id = surah_name_to_id.get(simple_name)\n",
    "\n",
    "            if surah_id is not None:\n",
    "                numeric_targets.append({\n",
    "                    'surah_id': surah_id,\n",
    "                    'ayah_id': ayah_id,\n",
    "                    'location_str': loc_str \n",
    "                })\n",
    "    return numeric_targets\n",
    "\n",
    "# 3. DEFINISI PASANGAN QUERY & GROUND TRUTH (Menggunakan string asli)\n",
    "robustness_data = [\n",
    "    [1, \"Hukum warisan bagi perempuan\", \"Pembagian harta pusaka istri\", \"Fiqh\", [\"QS. An-NisƒÅ' : 11\"]],\n",
    "    [2, \"Kapan puasa Ramadan dimulai\", \"Kewajiban saum di bulan suci\", \"Fiqh\", [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]],\n",
    "    [3, \"Cara melaksanakan sholat Jumat\", \"Ketentuan sembahyang Jumat\", \"Fiqh\", [\"QS. Al-Jumu'ah : 9\"]],\n",
    "    [4, \"Zakat hasil bumi\", \"Kewajiban sedekah pertanian\", \"Fiqh\", [\"QS. Al-An'ƒÅm : 141\"]],\n",
    "    [5, \"Denda bagi yang bersumpah palsu\", \"Konsekuensi sumpah dusta\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 89\"]],\n",
    "    [6, \"Berwudu sebelum salat\", \"Tata cara bersuci sebelum ibadah\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 6\"]],\n",
    "    [7, \"Pernikahan beda agama\", \"Hukum perkawinan non-Muslim\", \"Fiqh\", [\"QS. Al-Baqarah : 221\"]],\n",
    "    [8, \"Larangan memakan riba\", \"Haramnya pinjaman berbunga\", \"Fiqh\", [\"QS. Al-Baqarah : 275\"]],\n",
    "    [9, \"Membayar fidyah karena tidak puasa\", \"Kewajiban ganti rugi puasa\", \"Fiqh\", [\"QS. Al-Baqarah : 184\"]],\n",
    "    [10, \"Apa itu khamar\", \"Definisi minuman memabukkan\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 90\"]],\n",
    "\n",
    "    [11, \"Kisah Nabi Musa dan Firaun\", \"Cerita pertemuan Musa dengan raja Mesir\", \"Kisah\", [\"QS. Al-Qa·π£a·π£ : 31\", \"QS. Al-Qa·π£a·π£ : 36\"]],\n",
    "    [12, \"Kapal Nabi Nuh\", \"Perahu raksasa nuh\", \"Kisah\", [\"QS. H≈´d : 44\"]],\n",
    "    [13, \"Maryam melahirkan Isa\", \"Kelahiran putra Maryam\", \"Kisah\", [\"QS. Maryam : 23\", \"QS. Maryam : 27\"]],\n",
    "    [14, \"Nabi Yusuf dan mimpi 11 bintang\", \"Tafsir mimpi nabi Yakub tentang bintang\", \"Kisah\", [\"QS. Y≈´suf : 4\"]],\n",
    "    [15, \"Kisah Ashabul Kahfi\", \"Tujuh pemuda yang tertidur lama\", \"Kisah\", [\"QS. Al-Kahf : 10\", \"QS. Al-Kahf : 25\"]],\n",
    "    [16, \"Kenapa Iblis diusir dari surga\", \"Alasan setan menolak sujud Adam\", \"Kisah\", [\"QS. Al-A‚ÄòrƒÅf : 12\", \"QS. Al-Kahf : 50\"]],\n",
    "    [17, \"Tugas malaikat Jibril\", \"Fungsi Gabriel membawa wahyu\", \"Kisah\", [\"QS. Al-Baqarah : 97\"]],\n",
    "    [18, \"Kisah Qabil dan Habil\", \"Pembunuhan putra Adam\", \"Kisah\", [\"QS. Al-MƒÅ'idah : 27\"]],\n",
    "    [19, \"Raja Thalut dan Jalut\", \"Pertempuran Daud melawan Goliat\", \"Kisah\", [\"QS. Al-Baqarah : 249\", \"QS. Al-Baqarah : 251\"]],\n",
    "    [20, \"Bangsa Ya'juj dan Ma'juj\", \"Siapa Gog dan Magog\", \"Kisah\", [\"QS. Al-Kahf : 94\"]],\n",
    "\n",
    "    [21, \"Larangan berbuat syirik\", \"Dosa menyekutukan Allah\", \"Aqidah\", [\"QS. An-NisƒÅ' : 48\", \"QS. LuqmƒÅn : 13\"]],\n",
    "    [22, \"Berbakti pada kedua orang tua\", \"Kewajiban menghormati ayah ibu\", \"Aqidah\", [\"QS. Al-IsrƒÅ' : 23\"]],\n",
    "    [23, \"Definisi tauhid\", \"Konsep keesaan Tuhan\", \"Aqidah\", [\"QS. Al-IkhlƒÅ·π£ : 1\"]],\n",
    "    [24, \"Takdir baik dan buruk\", \"Ketentuan nasib yang ditetapkan Allah\", \"Aqidah\", [\"QS. Al-Qamar : 49\"]],\n",
    "    [25, \"Larangan berbuat dusta\", \"Hukum berkata bohong\", \"Aqidah\", [\"QS. At-Taubah : 119\"]],\n",
    "    [26, \"Tentang Hari Kiamat\", \"Deskripsi Hari Pembalasan\", \"Aqidah\", [\"QS. Al-QƒÅri'ah : 1\", \"QS. Az-Zalzalah : 1\"]],\n",
    "    [27, \"Balasan bagi orang yang sombong\", \"Konsekuensi sifat takabur\", \"Aqidah\", [\"QS. LuqmƒÅn : 18\"]],\n",
    "    [28, \"Larangan mengumpat\", \"Hukum ghibah dan mencela\", \"Aqidah\", [\"QS. Al-·∏§ujurƒÅt : 12\"]],\n",
    "    [29, \"Keutamaan sabar\", \"Pentingnya menahan diri\", \"Aqidah\", [\"QS. Al-Baqarah : 153\"]],\n",
    "    [30, \"Tujuan hidup manusia\", \"Mengapa kita diciptakan\", \"Aqidah\", [\"QS. Adz-DzƒÅriyƒÅt : 56\"]],\n",
    "]\n",
    "\n",
    "# 4. KONVERSI KE DATAFRAME\n",
    "df_robustness = pd.DataFrame(robustness_data, columns=[\n",
    "    'id', 'query_a', 'query_b', 'category', 'target_ayats'\n",
    "])\n",
    "\n",
    "# 5. DUPLIKASI BARIS\n",
    "rows_a = df_robustness.rename(columns={'query_a': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "rows_b = df_robustness.rename(columns={'query_b': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "df_test_set = pd.concat([rows_a, rows_b], ignore_index=True)\n",
    "\n",
    "# 6. KONVERSI STRING GROUND TRUTH KE STRUCTURED NUMERIC ID\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats'].apply(convert_to_numeric_id)\n",
    "\n",
    "# 7. SIMPAN KE CSV\n",
    "df_test_set['target_ayats'] = df_test_set['target_ayats'].apply(lambda x: json.dumps(x))\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats_id'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "df_test_set.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FILE TEST SET ROBUSTNESS ID-BASED BERHASIL DIBUAT (OUTPUT FIX)!\")\n",
    "print(f\"   Tersimpan di: {OUTPUT_PATH}\")\n",
    "print(f\"   Total Query Uji: {len(df_test_set)} baris.\")\n",
    "\n",
    "# DEBUG: Hitung berapa target yang berhasil di-map\n",
    "success_count = df_test_set['target_ayats_id'].apply(lambda x: len(json.loads(x))).sum()\n",
    "print(f\"   Total Target Ayats yang berhasil di-map: {success_count}. (Harusnya 43)\")\n",
    "\n",
    "print(\"\\nSTRUKTUR DATA (Preview, cek apakah list sudah terisi):\")\n",
    "# PERBAIKAN FINAL: Menggunakan to_string() untuk menghindari error 'tabulate'\n",
    "print(df_test_set[['id', 'query', 'category', 'target_ayats_id']].head(6).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8beceba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FILE TEST SET ROBUSTNESS ID-BASED BERHASIL DIBUAT (FINAL FIX)!\n",
      "   Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\robustness_queries_ID_BASED.csv\n",
      "   Total Query Uji: 60 baris.\n",
      "   Total Target Ayats yang berhasil di-map: 0. (Harusnya 43)\n",
      "\n",
      "STRUKTUR DATA (Preview, cek apakah list sudah terisi):\n",
      " id                           query category target_ayats_id\n",
      "  1    Hukum warisan bagi perempuan     Fiqh              []\n",
      "  2     Kapan puasa Ramadan dimulai     Fiqh              []\n",
      "  3  Cara melaksanakan sholat Jumat     Fiqh              []\n",
      "  4                Zakat hasil bumi     Fiqh              []\n",
      "  5 Denda bagi yang bersumpah palsu     Fiqh              []\n",
      "  6           Berwudu sebelum salat     Fiqh              []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import unicodedata # Wajib untuk menangani Unicode\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'robustness_queries_ID_BASED.csv')\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# 1. Daftar Kanonik 114 Surah (ID 1-114) - Kunci Referensi\n",
    "canonical_surahs_simple = [\n",
    "    'Al-Fatihah', 'Al-Baqarah', 'Ali Imran', 'An-Nisa', 'Al-Maidah', 'Al-Anam', 'Al-Araf', 'Al-Anfal', \n",
    "    'At-Taubah', 'Yunus', 'Hud', 'Yusuf', 'Ar-Rad', 'Ibrahim', 'Al-Hijr', 'An-Nahl', 'Al-Isra', 'Al-Kahf', \n",
    "    'Maryam', 'Taha', 'Al-Anbiya', 'Al-Haj', 'Al-Muminun', 'An-Nur', 'Al-Furqan', 'Asy-Syuara', \n",
    "    'An-Naml', 'Al-Qasas', 'Al-Ankabut', 'Ar-Rum', 'Luqman', 'As-Sajdah', 'Al-Ahzab', 'Saba', 'Fatir', \n",
    "    'Yasin', 'As-Saffat', 'Sad', 'Az-Zumar', 'Gafir', 'Fussilat', 'Asy-Syura', 'Az-Zukhruf', 'Ad-Dukhan', \n",
    "    'Al-Jasiyah', 'Al-Ahqaf', 'Muhammad', 'Al-Fath', 'Al-Hujurat', 'Qaf', 'Adz-Zariyat', 'At-Tur', 'An-Najm', \n",
    "    'Al-Qamar', 'Ar-Rahman', 'Al-Waqiah', 'Al-Hadid', 'Al-Mujadilah', 'Al-Hasyr', 'Al-Mumtahanah', 'As-Saff', \n",
    "    'Al-Jumuah', 'Al-Munafiqun', 'At-Tagabun', 'At-Talaq', 'At-Tahrim', 'Al-Mulk', 'Al-Qalam', 'Al-Haqqah', \n",
    "    'Al-Maarij', 'Nuh', 'Al-Jinn', 'Al-Muzzammil', 'Al-Muddassir', 'Al-Qiyamah', 'Al-Insan', 'Al-Mursalat', \n",
    "    'An-Naba', 'An-Naziat', 'Abasa', 'At-Takwir', 'Al-Infitar', 'Al-Mutaffifin', 'Al-Insyiqaq', \n",
    "    'Al-Buruj', 'At-Tariq', 'Al-Ala', 'Al-Gasyiyah', 'Al-Fajr', 'Al-Balad', 'Asy-Syams', 'Al-Lail', 'Ad-Duha', \n",
    "    'Al-Insyirah', 'At-Tin', 'Al-Alaq', 'Al-Qadr', 'Al-Bayyinah', 'Az-Zalzalah', 'Al-Adiyat', 'Al-Qariah', \n",
    "    'At-Takasur', 'Al-Asr', 'Al-Humazah', 'Al-Fil', 'Quraisy', 'Al-Maun', 'Al-Kausar', 'Al-Kafirun', \n",
    "    'An-Nasr', 'Al-Lahab', 'Al-Ikhlas', 'Al-Falaq', 'An-Nas'\n",
    "]\n",
    "\n",
    "surah_name_to_id = {name: i + 1 for i, name in enumerate(canonical_surahs_simple)}\n",
    "\n",
    "# 2. FUNGSI HYPER-NORMALISASI (KUNCI FIX)\n",
    "def normalize_string(text):\n",
    "    \"\"\"Menghapus spasi, tanda baca, dan aksen untuk membuat key yang aman.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # 1. Menghapus aksen Unicode (Contoh: ƒÅ -> a, ‚Äò -> ')\n",
    "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # 2. Menghapus semua karakter non-alfanumerik\n",
    "    text = re.sub(r'[^\\w]', '', text)\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "# 3. BUAT KAMUS TRANSLASI YANG SUDAH DINORMALISASI\n",
    "# Keys: Nama surah di input ground truth (raw)\n",
    "# Values: Nama surah yang sudah dinormalisasi dan ada di list kanonik\n",
    "translation_table_raw_to_simple = {\n",
    "    \"An-NisƒÅ'\": \"An-Nisa\", \"Al-Baqarah\": \"Al-Baqarah\", \"Al-Jumu'ah\": \"Al-Jumuah\", \"Al-An'ƒÅm\": \"Al-Anam\", \n",
    "    \"Al-MƒÅ'idah\": \"Al-Maidah\", \"Al-Qa·π£a·π£\": \"Al-Qasas\", \"H≈´d\": \"Hud\", \"Maryam\": \"Maryam\", \"Y≈´suf\": \"Yusuf\", \n",
    "    \"Al-Kahf\": \"Al-Kahf\", \"Al-A‚ÄòrƒÅf\": \"Al-Araf\", \"LuqmƒÅn\": \"Luqman\", \"Al-IsrƒÅ'\": \"Al-Isra\", \n",
    "    \"At-Taubah\": \"At-Taubah\", \"Al-IkhlƒÅ·π£\": \"Al-Ikhlas\", \"Al-Qamar\": \"Al-Qamar\", \"Adz-DzƒÅriyƒÅt\": \"Adz-Zariyat\", \n",
    "    \"Al-·∏§ujurƒÅt\": \"Al-Hujurat\", \"Al-QƒÅri'ah\": \"Al-Qariah\", \"Az-Zalzalah\": \"Az-Zalzalah\"\n",
    "    # Hanya butuh yang ada di 30 query Anda\n",
    "}\n",
    "\n",
    "# Kamus final: { [Nama Surah Mentah yang Dinormalisasi] : ID Numerik }\n",
    "# Ini adalah kamus yang akan kita pakai untuk lookup\n",
    "final_id_map = {}\n",
    "for raw_name, simple_name in translation_table_raw_to_simple.items():\n",
    "    if simple_name in surah_name_to_id:\n",
    "        normalized_key = normalize_string(raw_name)\n",
    "        final_id_map[normalized_key] = surah_name_to_id[simple_name]\n",
    "\n",
    "\n",
    "# 4. Fungsi Konversi String Lokasi ke ID Numerik (Final)\n",
    "def convert_to_numeric_id(location_str_list):\n",
    "    numeric_targets = []\n",
    "    \n",
    "    for loc_str in location_str_list:\n",
    "        if not isinstance(loc_str, str) or \"QS.\" not in loc_str:\n",
    "            continue\n",
    "            \n",
    "        match_surah = re.search(r'QS\\. (.+?)\\s+:', loc_str)\n",
    "        match_ayah = re.search(r'Ayat\\s+(\\d+)', loc_str)\n",
    "        \n",
    "        if match_surah and match_ayah:\n",
    "            surah_name_raw = match_surah.group(1).strip()\n",
    "            ayah_id = int(match_ayah.group(1))\n",
    "            \n",
    "            # KUNCI PERBAIKAN: Normalisasi sebelum Lookup\n",
    "            normalized_name_input = normalize_string(surah_name_raw)\n",
    "            \n",
    "            surah_id = final_id_map.get(normalized_name_input)\n",
    "\n",
    "            if surah_id is not None:\n",
    "                numeric_targets.append({\n",
    "                    'surah_id': surah_id,\n",
    "                    'ayah_id': ayah_id,\n",
    "                    'location_str': loc_str \n",
    "                })\n",
    "    return numeric_targets\n",
    "\n",
    "# 5. DEFINISI PASANGAN QUERY & GROUND TRUTH (Tidak diubah)\n",
    "robustness_data = [\n",
    "    [1, \"Hukum warisan bagi perempuan\", \"Pembagian harta pusaka istri\", \"Fiqh\", [\"QS. An-NisƒÅ' : 11\"]],\n",
    "    [2, \"Kapan puasa Ramadan dimulai\", \"Kewajiban saum di bulan suci\", \"Fiqh\", [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]],\n",
    "    [3, \"Cara melaksanakan sholat Jumat\", \"Ketentuan sembahyang Jumat\", \"Fiqh\", [\"QS. Al-Jumu'ah : 9\"]],\n",
    "    [4, \"Zakat hasil bumi\", \"Kewajiban sedekah pertanian\", \"Fiqh\", [\"QS. Al-An'ƒÅm : 141\"]],\n",
    "    [5, \"Denda bagi yang bersumpah palsu\", \"Konsekuensi sumpah dusta\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 89\"]],\n",
    "    [6, \"Berwudu sebelum salat\", \"Tata cara bersuci sebelum ibadah\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 6\"]],\n",
    "    [7, \"Pernikahan beda agama\", \"Hukum perkawinan non-Muslim\", \"Fiqh\", [\"QS. Al-Baqarah : 221\"]],\n",
    "    [8, \"Larangan memakan riba\", \"Haramnya pinjaman berbunga\", \"Fiqh\", [\"QS. Al-Baqarah : 275\"]],\n",
    "    [9, \"Membayar fidyah karena tidak puasa\", \"Kewajiban ganti rugi puasa\", \"Fiqh\", [\"QS. Al-Baqarah : 184\"]],\n",
    "    [10, \"Apa itu khamar\", \"Definisi minuman memabukkan\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 90\"]],\n",
    "\n",
    "    [11, \"Kisah Nabi Musa dan Firaun\", \"Cerita pertemuan Musa dengan raja Mesir\", \"Kisah\", [\"QS. Al-Qa·π£a·π£ : 31\", \"QS. Al-Qa·π£a·π£ : 36\"]],\n",
    "    [12, \"Kapal Nabi Nuh\", \"Perahu raksasa nuh\", \"Kisah\", [\"QS. H≈´d : 44\"]],\n",
    "    [13, \"Maryam melahirkan Isa\", \"Kelahiran putra Maryam\", \"Kisah\", [\"QS. Maryam : 23\", \"QS. Maryam : 27\"]],\n",
    "    [14, \"Nabi Yusuf dan mimpi 11 bintang\", \"Tafsir mimpi nabi Yakub tentang bintang\", \"Kisah\", [\"QS. Y≈´suf : 4\"]],\n",
    "    [15, \"Kisah Ashabul Kahfi\", \"Tujuh pemuda yang tertidur lama\", \"Kisah\", [\"QS. Al-Kahf : 10\", \"QS. Al-Kahf : 25\"]],\n",
    "    [16, \"Kenapa Iblis diusir dari surga\", \"Alasan setan menolak sujud Adam\", \"Kisah\", [\"QS. Al-A‚ÄòrƒÅf : 12\", \"QS. Al-Kahf : 50\"]],\n",
    "    [17, \"Tugas malaikat Jibril\", \"Fungsi Gabriel membawa wahyu\", \"Kisah\", [\"QS. Al-Baqarah : 97\"]],\n",
    "    [18, \"Kisah Qabil dan Habil\", \"Pembunuhan putra Adam\", \"Kisah\", [\"QS. Al-MƒÅ'idah : 27\"]],\n",
    "    [19, \"Raja Thalut dan Jalut\", \"Pertempuran Daud melawan Goliat\", \"Kisah\", [\"QS. Al-Baqarah : 249\", \"QS. Al-Baqarah : 251\"]],\n",
    "    [20, \"Bangsa Ya'juj dan Ma'juj\", \"Siapa Gog dan Magog\", \"Kisah\", [\"QS. Al-Kahf : 94\"]],\n",
    "\n",
    "    [21, \"Larangan berbuat syirik\", \"Dosa menyekutukan Allah\", \"Aqidah\", [\"QS. An-NisƒÅ' : 48\", \"QS. LuqmƒÅn : 13\"]],\n",
    "    [22, \"Berbakti pada kedua orang tua\", \"Kewajiban menghormati ayah ibu\", \"Aqidah\", [\"QS. Al-IsrƒÅ' : 23\"]],\n",
    "    [23, \"Definisi tauhid\", \"Konsep keesaan Tuhan\", \"Aqidah\", [\"QS. Al-IkhlƒÅ·π£ : 1\"]],\n",
    "    [24, \"Takdir baik dan buruk\", \"Ketentuan nasib yang ditetapkan Allah\", \"Aqidah\", [\"QS. Al-Qamar : 49\"]],\n",
    "    [25, \"Larangan berbuat dusta\", \"Hukum berkata bohong\", \"Aqidah\", [\"QS. At-Taubah : 119\"]],\n",
    "    [26, \"Tentang Hari Kiamat\", \"Deskripsi Hari Pembalasan\", \"Aqidah\", [\"QS. Al-QƒÅri'ah : 1\", \"QS. Az-Zalzalah : 1\"]],\n",
    "    [27, \"Balasan bagi orang yang sombong\", \"Konsekuensi sifat takabur\", \"Aqidah\", [\"QS. LuqmƒÅn : 18\"]],\n",
    "    [28, \"Larangan mengumpat\", \"Hukum ghibah dan mencela\", \"Aqidah\", [\"QS. Al-·∏§ujurƒÅt : 12\"]],\n",
    "    [29, \"Keutamaan sabar\", \"Pentingnya menahan diri\", \"Aqidah\", [\"QS. Al-Baqarah : 153\"]],\n",
    "    [30, \"Tujuan hidup manusia\", \"Mengapa kita diciptakan\", \"Aqidah\", [\"QS. Adz-DzƒÅriyƒÅt : 56\"]],\n",
    "]\n",
    "\n",
    "# 6. KONVERSI KE DATAFRAME & DUPLIKASI BARIS (Tidak diubah)\n",
    "df_robustness = pd.DataFrame(robustness_data, columns=[\n",
    "    'id', 'query_a', 'query_b', 'category', 'target_ayats'\n",
    "])\n",
    "rows_a = df_robustness.rename(columns={'query_a': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "rows_b = df_robustness.rename(columns={'query_b': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "df_test_set = pd.concat([rows_a, rows_b], ignore_index=True)\n",
    "\n",
    "# 7. KONVERSI STRING GROUND TRUTH KE STRUCTURED NUMERIC ID (Memakai fungsi perbaikan)\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats'].apply(convert_to_numeric_id)\n",
    "\n",
    "# 8. SIMPAN KE CSV (Output Fix)\n",
    "df_test_set['target_ayats'] = df_test_set['target_ayats'].apply(lambda x: json.dumps(x))\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats_id'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "df_test_set.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FILE TEST SET ROBUSTNESS ID-BASED BERHASIL DIBUAT (FINAL FIX)!\")\n",
    "print(f\"   Tersimpan di: {OUTPUT_PATH}\")\n",
    "print(f\"   Total Query Uji: {len(df_test_set)} baris.\")\n",
    "\n",
    "# DEBUG: Hitung berapa target yang berhasil di-map\n",
    "success_count = df_test_set['target_ayats_id'].apply(lambda x: len(json.loads(x))).sum()\n",
    "print(f\"   Total Target Ayats yang berhasil di-map: {success_count}. (Harusnya 43)\")\n",
    "\n",
    "print(\"\\nSTRUKTUR DATA (Preview, cek apakah list sudah terisi):\")\n",
    "print(df_test_set[['id', 'query', 'category', 'target_ayats_id']].head(6).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4fee5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FILE TEST SET ROBUSTNESS ID-BASED BERHASIL DIBUAT (FINAL FIX)!\n",
      "   Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\robustness_queries_ID_BASED.csv\n",
      "   Total Query Uji: 60 baris.\n",
      "   Total Target Ayats yang berhasil di-map: 76. (Harusnya 43)\n",
      "\n",
      "STRUKTUR DATA (Preview, cek apakah list sudah terisi):\n",
      " id                           query category                                                                                                                                    target_ayats_id\n",
      "  1    Hukum warisan bagi perempuan     Fiqh                                                                         [{\"surah_id\": 4, \"ayah_id\": 11, \"location_str\": \"QS. An-Nis\\u0101' : 11\"}]\n",
      "  2     Kapan puasa Ramadan dimulai     Fiqh [{\"surah_id\": 2, \"ayah_id\": 183, \"location_str\": \"QS. Al-Baqarah : 183\"}, {\"surah_id\": 2, \"ayah_id\": 185, \"location_str\": \"QS. Al-Baqarah : 185\"}]\n",
      "  3  Cara melaksanakan sholat Jumat     Fiqh                                                                             [{\"surah_id\": 62, \"ayah_id\": 9, \"location_str\": \"QS. Al-Jumu'ah : 9\"}]\n",
      "  4                Zakat hasil bumi     Fiqh                                                                       [{\"surah_id\": 6, \"ayah_id\": 141, \"location_str\": \"QS. Al-An'\\u0101m : 141\"}]\n",
      "  5 Denda bagi yang bersumpah palsu     Fiqh                                                                       [{\"surah_id\": 5, \"ayah_id\": 89, \"location_str\": \"QS. Al-M\\u0101'idah : 89\"}]\n",
      "  6           Berwudu sebelum salat     Fiqh                                                                         [{\"surah_id\": 5, \"ayah_id\": 6, \"location_str\": \"QS. Al-M\\u0101'idah : 6\"}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import unicodedata # Digunakan untuk menangani Unicode\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'robustness_queries_ID_BASED.csv')\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# 1. Daftar Kanonik 114 Surah (ID 1-114)\n",
    "canonical_surahs_simple = [\n",
    "    'Al-Fatihah', 'Al-Baqarah', 'Ali Imran', 'An-Nisa', 'Al-Maidah', 'Al-Anam', 'Al-Araf', 'Al-Anfal', \n",
    "    'At-Taubah', 'Yunus', 'Hud', 'Yusuf', 'Ar-Rad', 'Ibrahim', 'Al-Hijr', 'An-Nahl', 'Al-Isra', 'Al-Kahf', \n",
    "    'Maryam', 'Taha', 'Al-Anbiya', 'Al-Haj', 'Al-Muminun', 'An-Nur', 'Al-Furqan', 'Asy-Syuara', \n",
    "    'An-Naml', 'Al-Qasas', 'Al-Ankabut', 'Ar-Rum', 'Luqman', 'As-Sajdah', 'Al-Ahzab', 'Saba', 'Fatir', \n",
    "    'Yasin', 'As-Saffat', 'Sad', 'Az-Zumar', 'Gafir', 'Fussilat', 'Asy-Syura', 'Az-Zukhruf', 'Ad-Dukhan', \n",
    "    'Al-Jasiyah', 'Al-Ahqaf', 'Muhammad', 'Al-Fath', 'Al-Hujurat', 'Qaf', 'Adz-Zariyat', 'At-Tur', 'An-Najm', \n",
    "    'Al-Qamar', 'Ar-Rahman', 'Al-Waqiah', 'Al-Hadid', 'Al-Mujadilah', 'Al-Hasyr', 'Al-Mumtahanah', 'As-Saff', \n",
    "    'Al-Jumuah', 'Al-Munafiqun', 'At-Tagabun', 'At-Talaq', 'At-Tahrim', 'Al-Mulk', 'Al-Qalam', 'Al-Haqqah', \n",
    "    'Al-Maarij', 'Nuh', 'Al-Jinn', 'Al-Muzzammil', 'Al-Muddassir', 'Al-Qiyamah', 'Al-Insan', 'Al-Mursalat', \n",
    "    'An-Naba', 'An-Naziat', 'Abasa', 'At-Takwir', 'Al-Infitar', 'Al-Mutaffifin', 'Al-Insyiqaq', \n",
    "    'Al-Buruj', 'At-Tariq', 'Al-Ala', 'Al-Gasyiyah', 'Al-Fajr', 'Al-Balad', 'Asy-Syams', 'Al-Lail', 'Ad-Duha', \n",
    "    'Al-Insyirah', 'At-Tin', 'Al-Alaq', 'Al-Qadr', 'Al-Bayyinah', 'Az-Zalzalah', 'Al-Adiyat', 'Al-Qariah', \n",
    "    'At-Takasur', 'Al-Asr', 'Al-Humazah', 'Al-Fil', 'Quraisy', 'Al-Maun', 'Al-Kausar', 'Al-Kafirun', \n",
    "    'An-Nasr', 'Al-Lahab', 'Al-Ikhlas', 'Al-Falaq', 'An-Nas'\n",
    "]\n",
    "\n",
    "surah_name_to_id = {name: i + 1 for i, name in enumerate(canonical_surahs_simple)}\n",
    "\n",
    "# 2. FUNGSI DAN KAMUS KONVERSI EKSPISIT (KUNCI FIX)\n",
    "# Mapping eksplisit dari string raw input ke nama kanonik yang mudah dicari\n",
    "EXPLICIT_TRANSLATION_MAP = {\n",
    "    # Surah dengan Unicode / Apostrof\n",
    "    \"An-NisƒÅ'\": \"An-Nisa\", \"Al-Baqarah\": \"Al-Baqarah\", \"Al-Jumu'ah\": \"Al-Jumuah\", \n",
    "    \"Al-An'ƒÅm\": \"Al-Anam\", \"Al-MƒÅ'idah\": \"Al-Maidah\", \"Al-Qa·π£a·π£\": \"Al-Qasas\", \n",
    "    \"H≈´d\": \"Hud\", \"Y≈´suf\": \"Yusuf\", \"Al-Kahf\": \"Al-Kahf\", \"Al-A‚ÄòrƒÅf\": \"Al-Araf\", \n",
    "    \"LuqmƒÅn\": \"Luqman\", \"Al-IsrƒÅ'\": \"Al-Isra\", \"At-Taubah\": \"At-Taubah\", \"Al-IkhlƒÅ·π£\": \"Al-Ikhlas\", \n",
    "    \"Al-Qamar\": \"Al-Qamar\", \"Adz-DzƒÅriyƒÅt\": \"Adz-Zariyat\", \"Al-·∏§ujurƒÅt\": \"Al-Hujurat\", \n",
    "    \"Al-QƒÅri'ah\": \"Al-Qariah\", \"Az-Zalzalah\": \"Az-Zalzalah\", \"Al-AnbiyƒÅ'\": \"Al-Anbiya\",\n",
    "    # Tambahkan yang lain (tanpa unicode, tapi untuk melengkapi kamus)\n",
    "    \"Maryam\": \"Maryam\", \"Al-Qamar\": \"Al-Qamar\"\n",
    "}\n",
    "\n",
    "\n",
    "def convert_to_numeric_id(location_str_list):\n",
    "    numeric_targets = []\n",
    "    \n",
    "    for loc_str in location_str_list:\n",
    "        if not isinstance(loc_str, str) or \"QS.\" not in loc_str:\n",
    "            continue\n",
    "            \n",
    "        match_surah = re.search(r'QS\\. (.+?)\\s*:\\s*(\\d+)', loc_str)\n",
    "        \n",
    "        if match_surah:\n",
    "            surah_name_raw = match_surah.group(1).strip()\n",
    "            ayah_id = int(match_surah.group(2))\n",
    "            \n",
    "            # KUNCI PERBAIKAN: Lookup ke tabel terjemahan eksplisit\n",
    "            simple_name = EXPLICIT_TRANSLATION_MAP.get(surah_name_raw)\n",
    "            \n",
    "            # Cari ID Numerik dari nama sederhana\n",
    "            surah_id = surah_name_to_id.get(simple_name)\n",
    "\n",
    "            if surah_id is not None:\n",
    "                numeric_targets.append({\n",
    "                    'surah_id': surah_id,\n",
    "                    'ayah_id': ayah_id,\n",
    "                    'location_str': loc_str \n",
    "                })\n",
    "    return numeric_targets\n",
    "\n",
    "# 3. DEFINISI PASANGAN QUERY & GROUND TRUTH (Menggunakan string asli)\n",
    "robustness_data = [\n",
    "    [1, \"Hukum warisan bagi perempuan\", \"Pembagian harta pusaka istri\", \"Fiqh\", [\"QS. An-NisƒÅ' : 11\"]],\n",
    "    [2, \"Kapan puasa Ramadan dimulai\", \"Kewajiban saum di bulan suci\", \"Fiqh\", [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]],\n",
    "    [3, \"Cara melaksanakan sholat Jumat\", \"Ketentuan sembahyang Jumat\", \"Fiqh\", [\"QS. Al-Jumu'ah : 9\"]],\n",
    "    [4, \"Zakat hasil bumi\", \"Kewajiban sedekah pertanian\", \"Fiqh\", [\"QS. Al-An'ƒÅm : 141\"]],\n",
    "    [5, \"Denda bagi yang bersumpah palsu\", \"Konsekuensi sumpah dusta\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 89\"]],\n",
    "    [6, \"Berwudu sebelum salat\", \"Tata cara bersuci sebelum ibadah\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 6\"]],\n",
    "    [7, \"Pernikahan beda agama\", \"Hukum perkawinan non-Muslim\", \"Fiqh\", [\"QS. Al-Baqarah : 221\"]],\n",
    "    [8, \"Larangan memakan riba\", \"Haramnya pinjaman berbunga\", \"Fiqh\", [\"QS. Al-Baqarah : 275\"]],\n",
    "    [9, \"Membayar fidyah karena tidak puasa\", \"Kewajiban ganti rugi puasa\", \"Fiqh\", [\"QS. Al-Baqarah : 184\"]],\n",
    "    [10, \"Apa itu khamar\", \"Definisi minuman memabukkan\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 90\"]],\n",
    "\n",
    "    [11, \"Kisah Nabi Musa dan Firaun\", \"Cerita pertemuan Musa dengan raja Mesir\", \"Kisah\", [\"QS. Al-Qa·π£a·π£ : 31\", \"QS. Al-Qa·π£a·π£ : 36\"]],\n",
    "    [12, \"Kapal Nabi Nuh\", \"Perahu raksasa nuh\", \"Kisah\", [\"QS. H≈´d : 44\"]],\n",
    "    [13, \"Maryam melahirkan Isa\", \"Kelahiran putra Maryam\", \"Kisah\", [\"QS. Maryam : 23\", \"QS. Maryam : 27\"]],\n",
    "    [14, \"Nabi Yusuf dan mimpi 11 bintang\", \"Tafsir mimpi nabi Yakub tentang bintang\", \"Kisah\", [\"QS. Y≈´suf : 4\"]],\n",
    "    [15, \"Kisah Ashabul Kahfi\", \"Tujuh pemuda yang tertidur lama\", \"Kisah\", [\"QS. Al-Kahf : 10\", \"QS. Al-Kahf : 25\"]],\n",
    "    [16, \"Kenapa Iblis diusir dari surga\", \"Alasan setan menolak sujud Adam\", \"Kisah\", [\"QS. Al-A‚ÄòrƒÅf : 12\", \"QS. Al-Kahf : 50\"]],\n",
    "    [17, \"Tugas malaikat Jibril\", \"Fungsi Gabriel membawa wahyu\", \"Kisah\", [\"QS. Al-Baqarah : 97\"]],\n",
    "    [18, \"Kisah Qabil dan Habil\", \"Pembunuhan putra Adam\", \"Kisah\", [\"QS. Al-MƒÅ'idah : 27\"]],\n",
    "    [19, \"Raja Thalut dan Jalut\", \"Pertempuran Daud melawan Goliat\", \"Kisah\", [\"QS. Al-Baqarah : 249\", \"QS. Al-Baqarah : 251\"]],\n",
    "    [20, \"Bangsa Ya'juj dan Ma'juj\", \"Siapa Gog dan Magog\", \"Kisah\", [\"QS. Al-Kahf : 94\"]],\n",
    "\n",
    "    [21, \"Larangan berbuat syirik\", \"Dosa menyekutukan Allah\", \"Aqidah\", [\"QS. An-NisƒÅ' : 48\", \"QS. LuqmƒÅn : 13\"]],\n",
    "    [22, \"Berbakti pada kedua orang tua\", \"Kewajiban menghormati ayah ibu\", \"Aqidah\", [\"QS. Al-IsrƒÅ' : 23\"]],\n",
    "    [23, \"Definisi tauhid\", \"Konsep keesaan Tuhan\", \"Aqidah\", [\"QS. Al-IkhlƒÅ·π£ : 1\"]],\n",
    "    [24, \"Takdir baik dan buruk\", \"Ketentuan nasib yang ditetapkan Allah\", \"Aqidah\", [\"QS. Al-Qamar : 49\"]],\n",
    "    [25, \"Larangan berbuat dusta\", \"Hukum berkata bohong\", \"Aqidah\", [\"QS. At-Taubah : 119\"]],\n",
    "    [26, \"Tentang Hari Kiamat\", \"Deskripsi Hari Pembalasan\", \"Aqidah\", [\"QS. Al-QƒÅri'ah : 1\", \"QS. Az-Zalzalah : 1\"]],\n",
    "    [27, \"Balasan bagi orang yang sombong\", \"Konsekuensi sifat takabur\", \"Aqidah\", [\"QS. LuqmƒÅn : 18\"]],\n",
    "    [28, \"Larangan mengumpat\", \"Hukum ghibah dan mencela\", \"Aqidah\", [\"QS. Al-·∏§ujurƒÅt : 12\"]],\n",
    "    [29, \"Keutamaan sabar\", \"Pentingnya menahan diri\", \"Aqidah\", [\"QS. Al-Baqarah : 153\"]],\n",
    "    [30, \"Tujuan hidup manusia\", \"Mengapa kita diciptakan\", \"Aqidah\", [\"QS. Adz-DzƒÅriyƒÅt : 56\"]],\n",
    "]\n",
    "\n",
    "# 4. KONVERSI KE DATAFRAME & DUPLIKASI BARIS\n",
    "df_robustness = pd.DataFrame(robustness_data, columns=[\n",
    "    'id', 'query_a', 'query_b', 'category', 'target_ayats'\n",
    "])\n",
    "rows_a = df_robustness.rename(columns={'query_a': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "rows_b = df_robustness.rename(columns={'query_b': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "df_test_set = pd.concat([rows_a, rows_b], ignore_index=True)\n",
    "\n",
    "# 5. KONVERSI STRING GROUND TRUTH KE STRUCTURED NUMERIC ID\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats'].apply(convert_to_numeric_id)\n",
    "\n",
    "# 6. SIMPAN KE CSV\n",
    "df_test_set['target_ayats'] = df_test_set['target_ayats'].apply(lambda x: json.dumps(x))\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats_id'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "df_test_set.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FILE TEST SET ROBUSTNESS ID-BASED BERHASIL DIBUAT (FINAL FIX)!\")\n",
    "print(f\"   Tersimpan di: {OUTPUT_PATH}\")\n",
    "print(f\"   Total Query Uji: {len(df_test_set)} baris.\")\n",
    "\n",
    "# DEBUG: Hitung berapa target yang berhasil di-map\n",
    "success_count = df_test_set['target_ayats_id'].apply(lambda x: len(json.loads(x))).sum()\n",
    "print(f\"   Total Target Ayats yang berhasil di-map: {success_count}. (Harusnya 43)\")\n",
    "\n",
    "print(\"\\nSTRUKTUR DATA (Preview, cek apakah list sudah terisi):\")\n",
    "print(df_test_set[['id', 'query', 'category', 'target_ayats_id']].head(6).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6dc8213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FILE TEST SET ROBUSTNESS ID-BASED BERHASIL DIBUAT (FINAL FIX)!\n",
      "   Tersimpan di: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\robustness_queries_ID_BASED.csv\n",
      "   Total Target Ayats yang berhasil di-map: 74. (Harusnya 43)\n",
      "\n",
      "STRUKTUR DATA (Preview, cek apakah list sudah terisi):\n",
      " id                           query category                                                                                                                                    target_ayats_id\n",
      "  1    Hukum warisan bagi perempuan     Fiqh                                                                         [{\"surah_id\": 4, \"ayah_id\": 11, \"location_str\": \"QS. An-Nis\\u0101' : 11\"}]\n",
      "  2     Kapan puasa Ramadan dimulai     Fiqh [{\"surah_id\": 2, \"ayah_id\": 183, \"location_str\": \"QS. Al-Baqarah : 183\"}, {\"surah_id\": 2, \"ayah_id\": 185, \"location_str\": \"QS. Al-Baqarah : 185\"}]\n",
      "  3  Cara melaksanakan sholat Jumat     Fiqh                                                                             [{\"surah_id\": 62, \"ayah_id\": 9, \"location_str\": \"QS. Al-Jumu'ah : 9\"}]\n",
      "  4                Zakat hasil bumi     Fiqh                                                                       [{\"surah_id\": 6, \"ayah_id\": 141, \"location_str\": \"QS. Al-An'\\u0101m : 141\"}]\n",
      "  5 Denda bagi yang bersumpah palsu     Fiqh                                                                       [{\"surah_id\": 5, \"ayah_id\": 89, \"location_str\": \"QS. Al-M\\u0101'idah : 89\"}]\n",
      "  6           Berwudu sebelum salat     Fiqh                                                                         [{\"surah_id\": 5, \"ayah_id\": 6, \"location_str\": \"QS. Al-M\\u0101'idah : 6\"}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import unicodedata # Wajib untuk menangani Unicode\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT_DIR, 'data', 'processed', 'robustness_queries_ID_BASED.csv')\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# 1. Daftar Kanonik 114 Surah (Sederhana, untuk ID)\n",
    "canonical_surahs_simple = [\n",
    "    'Al-Fatihah', 'Al-Baqarah', 'Ali Imran', 'An-Nisa', 'Al-Maidah', 'Al-Anam', 'Al-Araf', 'Al-Anfal', \n",
    "    'At-Taubah', 'Yunus', 'Hud', 'Yusuf', 'Ar-Rad', 'Ibrahim', 'Al-Hijr', 'An-Nahl', 'Al-Isra', 'Al-Kahf', \n",
    "    'Maryam', 'Taha', 'Al-Anbiya', 'Al-Haj', 'Al-Muminun', 'An-Nur', 'Al-Furqan', 'Asy-Syuara', \n",
    "    'An-Naml', 'Al-Qasas', 'Al-Ankabut', 'Ar-Rum', 'Luqman', 'As-Sajdah', 'Al-Ahzab', 'Saba', 'Fatir', \n",
    "    'Yasin', 'As-Saffat', 'Sad', 'Az-Zumar', 'Gafir', 'Fussilat', 'Asy-Syura', 'Az-Zukhruf', 'Ad-Dukhan', \n",
    "    'Al-Jasiyah', 'Al-Ahqaf', 'Muhammad', 'Al-Fath', 'Al-Hujurat', 'Qaf', 'Adz-Zariyat', 'At-Tur', 'An-Najm', \n",
    "    'Al-Qamar', 'Ar-Rahman', 'Al-Waqiah', 'Al-Hadid', 'Al-Mujadilah', 'Al-Hasyr', 'Al-Mumtahanah', 'As-Saff', \n",
    "    'Al-Jumuah', 'Al-Munafiqun', 'At-Tagabun', 'At-Talaq', 'At-Tahrim', 'Al-Mulk', 'Al-Qalam', 'Al-Haqqah', \n",
    "    'Al-Maarij', 'Nuh', 'Al-Jinn', 'Al-Muzzammil', 'Al-Muddassir', 'Al-Qiyamah', 'Al-Insan', 'Al-Mursalat', \n",
    "    'An-Naba', 'An-Naziat', 'Abasa', 'At-Takwir', 'Al-Infitar', 'Al-Mutaffifin', 'Al-Insyiqaq', \n",
    "    'Al-Buruj', 'At-Tariq', 'Al-Ala', 'Al-Gasyiyah', 'Al-Fajr', 'Al-Balad', 'Asy-Syams', 'Al-Lail', 'Ad-Duha', \n",
    "    'Al-Insyirah', 'At-Tin', 'Al-Alaq', 'Al-Qadr', 'Al-Bayyinah', 'Az-Zalzalah', 'Al-Adiyat', 'Al-Qariah', \n",
    "    'At-Takasur', 'Al-Asr', 'Al-Humazah', 'Al-Fil', 'Quraisy', 'Al-Maun', 'Al-Kausar', 'Al-Kafirun', \n",
    "    'An-Nasr', 'Al-Lahab', 'Al-Ikhlas', 'Al-Falaq', 'An-Nas'\n",
    "]\n",
    "\n",
    "# 2. FUNGSI HYPER-NORMALISASI (KUNCI FIX)\n",
    "def hyper_normalize(name):\n",
    "    \"\"\"Menghapus semua karakter non-alfanumerik dan aksen untuk membuat key yang aman.\"\"\"\n",
    "    if not isinstance(name, str): return \"\"\n",
    "    \n",
    "    # 1. Normalisasi Unicode (e.g., ƒÅ -> a) dan encoding ke ascii\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # 2. Hapus semua karakter non-alfanumerik (termasuk spasi dan apostrof)\n",
    "    name = re.sub(r'[^a-zA-Z0-9]', '', name)\n",
    "    \n",
    "    return name.lower()\n",
    "\n",
    "# 3. BUAT KAMUS ID FINAL (Hyper-Normalized Key -> ID Numerik)\n",
    "final_id_map = {}\n",
    "for i, simple_name in enumerate(canonical_surahs_simple):\n",
    "    # Key adalah bentuk ter-normalisasi dari nama surah sederhana (Contoh: 'annisƒÅ'' -> 'annisa')\n",
    "    normalized_key = hyper_normalize(simple_name) \n",
    "    final_id_map[normalized_key] = i + 1 # ID Surah\n",
    "    \n",
    "# 4. Fungsi Konversi String Lokasi ke ID Numerik (Final)\n",
    "def convert_to_numeric_id(location_str_list):\n",
    "    numeric_targets = []\n",
    "    \n",
    "    for loc_str in location_str_list:\n",
    "        if not isinstance(loc_str, str) or \"QS.\" not in loc_str:\n",
    "            continue\n",
    "            \n",
    "        match_surah = re.search(r'QS\\. (.+?)\\s*:\\s*(\\d+)', loc_str)\n",
    "        \n",
    "        if match_surah:\n",
    "            surah_name_raw = match_surah.group(1).strip()\n",
    "            ayah_id = int(match_surah.group(2))\n",
    "            \n",
    "            # KUNCI FIX: Normalisasi input string mentah sebelum lookup\n",
    "            normalized_name_input = hyper_normalize(surah_name_raw)\n",
    "            \n",
    "            surah_id = final_id_map.get(normalized_name_input)\n",
    "\n",
    "            if surah_id is not None:\n",
    "                numeric_targets.append({\n",
    "                    'surah_id': surah_id,\n",
    "                    'ayah_id': ayah_id,\n",
    "                    'location_str': loc_str \n",
    "                })\n",
    "    return numeric_targets\n",
    "\n",
    "# 5. DEFINISI PASANGAN QUERY & GROUND TRUTH (Tidak diubah)\n",
    "robustness_data = [\n",
    "    [1, \"Hukum warisan bagi perempuan\", \"Pembagian harta pusaka istri\", \"Fiqh\", [\"QS. An-NisƒÅ' : 11\"]],\n",
    "    [2, \"Kapan puasa Ramadan dimulai\", \"Kewajiban saum di bulan suci\", \"Fiqh\", [\"QS. Al-Baqarah : 183\", \"QS. Al-Baqarah : 185\"]],\n",
    "    [3, \"Cara melaksanakan sholat Jumat\", \"Ketentuan sembahyang Jumat\", \"Fiqh\", [\"QS. Al-Jumu'ah : 9\"]],\n",
    "    [4, \"Zakat hasil bumi\", \"Kewajiban sedekah pertanian\", \"Fiqh\", [\"QS. Al-An'ƒÅm : 141\"]],\n",
    "    [5, \"Denda bagi yang bersumpah palsu\", \"Konsekuensi sumpah dusta\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 89\"]],\n",
    "    [6, \"Berwudu sebelum salat\", \"Tata cara bersuci sebelum ibadah\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 6\"]],\n",
    "    [7, \"Pernikahan beda agama\", \"Hukum perkawinan non-Muslim\", \"Fiqh\", [\"QS. Al-Baqarah : 221\"]],\n",
    "    [8, \"Larangan memakan riba\", \"Haramnya pinjaman berbunga\", \"Fiqh\", [\"QS. Al-Baqarah : 275\"]],\n",
    "    [9, \"Membayar fidyah karena tidak puasa\", \"Kewajiban ganti rugi puasa\", \"Fiqh\", [\"QS. Al-Baqarah : 184\"]],\n",
    "    [10, \"Apa itu khamar\", \"Definisi minuman memabukkan\", \"Fiqh\", [\"QS. Al-MƒÅ'idah : 90\"]],\n",
    "\n",
    "    [11, \"Kisah Nabi Musa dan Firaun\", \"Cerita pertemuan Musa dengan raja Mesir\", \"Kisah\", [\"QS. Al-Qa·π£a·π£ : 31\", \"QS. Al-Qa·π£a·π£ : 36\"]],\n",
    "    [12, \"Kapal Nabi Nuh\", \"Perahu raksasa nuh\", \"Kisah\", [\"QS. H≈´d : 44\"]],\n",
    "    [13, \"Maryam melahirkan Isa\", \"Kelahiran putra Maryam\", \"Kisah\", [\"QS. Maryam : 23\", \"QS. Maryam : 27\"]],\n",
    "    [14, \"Nabi Yusuf dan mimpi 11 bintang\", \"Tafsir mimpi nabi Yakub tentang bintang\", \"Kisah\", [\"QS. Y≈´suf : 4\"]],\n",
    "    [15, \"Kisah Ashabul Kahfi\", \"Tujuh pemuda yang tertidur lama\", \"Kisah\", [\"QS. Al-Kahf : 10\", \"QS. Al-Kahf : 25\"]],\n",
    "    [16, \"Kenapa Iblis diusir dari surga\", \"Alasan setan menolak sujud Adam\", \"Kisah\", [\"QS. Al-A‚ÄòrƒÅf : 12\", \"QS. Al-Kahf : 50\"]],\n",
    "    [17, \"Tugas malaikat Jibril\", \"Fungsi Gabriel membawa wahyu\", \"Kisah\", [\"QS. Al-Baqarah : 97\"]],\n",
    "    [18, \"Kisah Qabil dan Habil\", \"Pembunuhan putra Adam\", \"Kisah\", [\"QS. Al-MƒÅ'idah : 27\"]],\n",
    "    [19, \"Raja Thalut dan Jalut\", \"Pertempuran Daud melawan Goliat\", \"Kisah\", [\"QS. Al-Baqarah : 249\", \"QS. Al-Baqarah : 251\"]],\n",
    "    [20, \"Bangsa Ya'juj dan Ma'juj\", \"Siapa Gog dan Magog\", \"Kisah\", [\"QS. Al-Kahf : 94\"]],\n",
    "\n",
    "    [21, \"Larangan berbuat syirik\", \"Dosa menyekutukan Allah\", \"Aqidah\", [\"QS. An-NisƒÅ' : 48\", \"QS. LuqmƒÅn : 13\"]],\n",
    "    [22, \"Berbakti pada kedua orang tua\", \"Kewajiban menghormati ayah ibu\", \"Aqidah\", [\"QS. Al-IsrƒÅ' : 23\"]],\n",
    "    [23, \"Definisi tauhid\", \"Konsep keesaan Tuhan\", \"Aqidah\", [\"QS. Al-IkhlƒÅ·π£ : 1\"]],\n",
    "    [24, \"Takdir baik dan buruk\", \"Ketentuan nasib yang ditetapkan Allah\", \"Aqidah\", [\"QS. Al-Qamar : 49\"]],\n",
    "    [25, \"Larangan berbuat dusta\", \"Hukum berkata bohong\", \"Aqidah\", [\"QS. At-Taubah : 119\"]],\n",
    "    [26, \"Tentang Hari Kiamat\", \"Deskripsi Hari Pembalasan\", \"Aqidah\", [\"QS. Al-QƒÅri'ah : 1\", \"QS. Az-Zalzalah : 1\"]],\n",
    "    [27, \"Balasan bagi orang yang sombong\", \"Konsekuensi sifat takabur\", \"Aqidah\", [\"QS. LuqmƒÅn : 18\"]],\n",
    "    [28, \"Larangan mengumpat\", \"Hukum ghibah dan mencela\", \"Aqidah\", [\"QS. Al-·∏§ujurƒÅt : 12\"]],\n",
    "    [29, \"Keutamaan sabar\", \"Pentingnya menahan diri\", \"Aqidah\", [\"QS. Al-Baqarah : 153\"]],\n",
    "    [30, \"Tujuan hidup manusia\", \"Mengapa kita diciptakan\", \"Aqidah\", [\"QS. Adz-DzƒÅriyƒÅt : 56\"]],\n",
    "]\n",
    "\n",
    "# 6. KONVERSI KE DATAFRAME & DUPLIKASI BARIS\n",
    "df_robustness = pd.DataFrame(robustness_data, columns=[\n",
    "    'id', 'query_a', 'query_b', 'category', 'target_ayats'\n",
    "])\n",
    "rows_a = df_robustness.rename(columns={'query_a': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "rows_b = df_robustness.rename(columns={'query_b': 'query'})[['id', 'query', 'category', 'target_ayats']]\n",
    "df_test_set = pd.concat([rows_a, rows_b], ignore_index=True)\n",
    "\n",
    "# 7. KONVERSI STRING GROUND TRUTH KE STRUCTURED NUMERIC ID\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats'].apply(convert_to_numeric_id)\n",
    "\n",
    "# 8. SIMPAN KE CSV\n",
    "df_test_set['target_ayats'] = df_test_set['target_ayats'].apply(lambda x: json.dumps(x))\n",
    "df_test_set['target_ayats_id'] = df_test_set['target_ayats_id'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "df_test_set.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FILE TEST SET ROBUSTNESS ID-BASED BERHASIL DIBUAT (FINAL FIX)!\")\n",
    "print(f\"   Tersimpan di: {OUTPUT_PATH}\")\n",
    "\n",
    "# DEBUG: Hitung berapa target yang berhasil di-map\n",
    "success_count = df_test_set['target_ayats_id'].apply(lambda x: len(json.loads(x))).sum()\n",
    "print(f\"   Total Target Ayats yang berhasil di-map: {success_count}. (Harusnya 43)\")\n",
    "\n",
    "print(\"\\nSTRUKTUR DATA (Preview, cek apakah list sudah terisi):\")\n",
    "print(df_test_set[['id', 'query', 'category', 'target_ayats_id']].head(6).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8081b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è MEMUAT ENGINE RANDOM FOREST UNTUK UJI KRITIS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Random Forest berhasil dimuat.\n",
      "‚úÖ Melakukan Uji Kritis pada 10 Query Fiqih...\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI KRITIS (RANDOM FOREST)\n",
      "=======================================================\n",
      "Metrik Uji: Precision at 1 (P@1)\n",
      "Query Fiqih Diuji: 9\n",
      "-------------------------------------------------------\n",
      "P@1 Score (Jawaban di Rank 1 Benar): 0.00%\n",
      "\n",
      "KESIMPULAN: Performa RF Cukup. Diperlukan tuning atau menggunakan Model Pemenang (LightGBM).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gc\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals(): \n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'models')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "MASTER_CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv')\n",
    "QUERIES_PATH = os.path.join(DATA_DIR, 'robustness_queries_ID_BASED.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "RF_PATH = os.path.join(MODEL_DIR, 'randomforest_custom.pkl') \n",
    "\n",
    "# Urutan Fitur Model RF\n",
    "RF_FEATURES = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score']\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL (Optimized for Speed) ---\n",
    "print(\"‚öôÔ∏è MEMUAT ENGINE RANDOM FOREST UNTUK UJI KRITIS...\")\n",
    "\n",
    "# A. Load Data Master ID-BASED\n",
    "df_master = pd.read_csv(MASTER_CSV_PATH, usecols=['text', 'surah_id', 'ayah_id'])\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "df_index = df_master.drop_duplicates(subset=['text']).copy().reset_index(drop=True)\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "del df_master\n",
    "gc.collect()\n",
    "\n",
    "# B. Load Embeddings\n",
    "corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "\n",
    "# C. Setup Tools (SBERT & BM25)\n",
    "sbert_model = SentenceTransformer(os.path.join(MODEL_DIR, 'sbert_finetuned_quran'), device='cpu')\n",
    "\n",
    "try: stopwords_id = stopwords.words('indonesian')\n",
    "except: stopwords_id = ['yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'untuk'] \n",
    "def clean_tokens(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stopwords_id]\n",
    "\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "# D. Load Random Forest Model\n",
    "try:\n",
    "    rf_model = joblib.load(RF_PATH)\n",
    "    print(\"‚úÖ Model Random Forest berhasil dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gagal memuat model RF: {e}\")\n",
    "    raise SystemExit(\"Gagal memuat model RF.\")\n",
    "\n",
    "# E. Load Query Test Set (Filter hanya Query Fiqih Kritis)\n",
    "df_queries = pd.read_csv(QUERIES_PATH)\n",
    "df_queries['target_ayats_id'] = df_queries['target_ayats_id'].apply(json.loads) \n",
    "df_queries = df_queries[df_queries['category'] == 'Fiqh'].head(10).copy() # Ambil 10 query Fiqih pertama\n",
    "\n",
    "print(f\"‚úÖ Melakukan Uji Kritis pada {len(df_queries)} Query Fiqih...\")\n",
    "\n",
    "# --- 3. FUNGSI UJI P@1 ---\n",
    "\n",
    "def run_critical_test(model, df_queries):\n",
    "    \n",
    "    hits_at_1 = 0\n",
    "    total_queries = 0\n",
    "    \n",
    "    for query_idx, row in df_queries.iterrows():\n",
    "        query_text = row['query']\n",
    "        target_ayats_id = row['target_ayats_id'] \n",
    "        \n",
    "        # 1. TENTUKAN GROUND TRUTH NUMERIK\n",
    "        y_true = np.zeros(len(unique_tafsirs))\n",
    "        \n",
    "        for target in target_ayats_id:\n",
    "            s_id = target['surah_id']\n",
    "            a_id = target['ayah_id']\n",
    "            matching_indices = df_index[\n",
    "                (df_index['surah_id'] == s_id) & \n",
    "                (df_index['ayah_id'] == a_id)\n",
    "            ].index.tolist()\n",
    "            for idx in matching_indices:\n",
    "                if idx < len(y_true): y_true[idx] = 1\n",
    "\n",
    "        if y_true.sum() == 0: continue \n",
    "        total_queries += 1\n",
    "\n",
    "        # 2. Re-ranking (Top K 50 untuk menjamin jawaban benar terambil)\n",
    "        query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "        # KUNCI PERBAIKAN: TOP K dinaikkan agar tahap 1 retrieval tidak kosong\n",
    "        hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0] \n",
    "        \n",
    "        candidates = []\n",
    "        q_toks = clean_tokens(query_text)\n",
    "        \n",
    "        for hit in hits:\n",
    "            idx = hit['corpus_id']\n",
    "            if idx >= len(unique_tafsirs): continue \n",
    "\n",
    "            txt = unique_tafsirs[idx]\n",
    "            \n",
    "            # Hitung 4 fitur wajib\n",
    "            t_toks = clean_tokens(txt)\n",
    "            sq, st = set(q_toks), set(t_toks)\n",
    "            ov = len(sq & st) / len(sq) if sq else 0\n",
    "            jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "            bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "            \n",
    "            candidates.append({\n",
    "                'sbert_sim': hit['score'], 'bm25_score': bm25_s,\n",
    "                'overlap_score': ov, 'jaccard_score': jac, 'idx_corpus': idx\n",
    "            })\n",
    "            \n",
    "        if not candidates: continue # Skip jika tidak ada kandidat\n",
    "\n",
    "        # 3. Prediksi Model\n",
    "        df_cand = pd.DataFrame(candidates)\n",
    "        # KUNCI PERBAIKAN: Cek jika DataFrame kosong sebelum memprediksi\n",
    "        if df_cand.empty: continue \n",
    "\n",
    "        X_pred = df_cand[RF_FEATURES]\n",
    "\n",
    "        scores = model.predict_proba(X_pred)[:, 1]\n",
    "        df_cand['pred_score'] = scores\n",
    "        \n",
    "        # 4. Cek P@1\n",
    "        df_cand = df_cand.sort_values('pred_score', ascending=False)\n",
    "        \n",
    "        # KUNCI PERBAIKAN: Cek df_cand.iloc[0] hanya jika DataFrame tidak kosong\n",
    "        if not df_cand.empty:\n",
    "            # Ambil indeks corpus dari rank 1\n",
    "            rank_1_idx_corpus = df_cand.iloc[0]['idx_corpus']\n",
    "            \n",
    "            # Cek apakah rank 1 termasuk ground truth\n",
    "            if rank_1_idx_corpus < len(y_true) and y_true[int(rank_1_idx_corpus)] == 1:\n",
    "                hits_at_1 += 1\n",
    "    \n",
    "    # Final Results\n",
    "    p_at_1 = hits_at_1 / total_queries if total_queries > 0 else 0\n",
    "    return p_at_1, total_queries\n",
    "\n",
    "# --- 4. EKSEKUSI UJI KRITIS ---\n",
    "p_at_1_score, total_tested = run_critical_test(rf_model, df_queries)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI KRITIS (RANDOM FOREST)\")\n",
    "print(\"=======================================================\")\n",
    "print(f\"Metrik Uji: Precision at 1 (P@1)\")\n",
    "print(f\"Query Fiqih Diuji: {total_tested}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"P@1 Score (Jawaban di Rank 1 Benar): {p_at_1_score*100:.2f}%\")\n",
    "\n",
    "if p_at_1_score > 0.6:\n",
    "    print(\"\\nKESIMPULAN: Performa RF sangat Kuat dan Andal. Siap untuk diintegrasikan.\")\n",
    "else:\n",
    "    print(\"\\nKESIMPULAN: Performa RF Cukup. Diperlukan tuning atau menggunakan Model Pemenang (LightGBM).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f43c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è MEMUAT ENGINE XGBOOST UNTUK UJI KRITIS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model XGBoost berhasil dimuat.\n",
      "‚úÖ Melakukan Uji Kritis pada 10 Query Fiqih...\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI KRITIS (XGBOOST)\n",
      "=======================================================\n",
      "Metrik Uji: Precision at 1 (P@1)\n",
      "Query Fiqih Diuji: 9\n",
      "-------------------------------------------------------\n",
      "P@1 Score (Jawaban di Rank 1 Benar): 0.00%\n",
      "\n",
      "KESIMPULAN: Performa XGBoost BURUK. Pilih LightGBM yang sudah terbukti kuat.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import joblib \n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals(): \n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'models')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "MASTER_CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv')\n",
    "QUERIES_PATH = os.path.join(DATA_DIR, 'robustness_queries_ID_BASED.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "\n",
    "# Urutan Fitur Model XGBoost\n",
    "XGBOOST_FEATURES = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score']\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL (Optimized for Speed) ---\n",
    "print(\"‚öôÔ∏è MEMUAT ENGINE XGBOOST UNTUK UJI KRITIS...\")\n",
    "\n",
    "# A. Load Data Master ID-BASED (Hanya kolom penting)\n",
    "df_master = pd.read_csv(MASTER_CSV_PATH, usecols=['text', 'surah_id', 'ayah_id'])\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "df_index = df_master.drop_duplicates(subset=['text']).copy().reset_index(drop=True)\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "del df_master\n",
    "gc.collect() # Bersihkan memori\n",
    "\n",
    "# B. Load Embeddings\n",
    "corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "\n",
    "# C. Setup Tools (SBERT & BM25)\n",
    "sbert_model = SentenceTransformer(os.path.join(MODEL_DIR, 'sbert_finetuned_quran'), device='cpu')\n",
    "\n",
    "try: stopwords_id = stopwords.words('indonesian')\n",
    "except: stopwords_id = ['yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'untuk'] \n",
    "def clean_tokens(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stopwords_id]\n",
    "\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "# D. Load XGBoost Model\n",
    "try:\n",
    "    xgb_model = xgb.Booster()\n",
    "    xgb_model.load_model(os.path.join(MODEL_DIR, 'xgboost_best_model.json'))\n",
    "    print(\"‚úÖ Model XGBoost berhasil dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gagal memuat model XGBoost: {e}\")\n",
    "    raise SystemExit(\"Gagal memuat model XGBoost.\")\n",
    "\n",
    "# E. Load Query Test Set (Filter hanya Query Fiqih Kritis)\n",
    "df_queries = pd.read_csv(QUERIES_PATH)\n",
    "df_queries['target_ayats_id'] = df_queries['target_ayats_id'].apply(json.loads) \n",
    "df_queries = df_queries[df_queries['category'] == 'Fiqh'].head(10).copy() \n",
    "\n",
    "print(f\"‚úÖ Melakukan Uji Kritis pada {len(df_queries)} Query Fiqih...\")\n",
    "\n",
    "# --- 3. FUNGSI UJI P@1 KRITIS ---\n",
    "\n",
    "def run_critical_test(model, df_queries):\n",
    "    \n",
    "    hits_at_1 = 0\n",
    "    total_queries = 0\n",
    "    \n",
    "    for query_idx, row in df_queries.iterrows():\n",
    "        query_text = row['query']\n",
    "        target_ayats_id = row['target_ayats_id'] \n",
    "        \n",
    "        # 1. TENTUKAN GROUND TRUTH NUMERIK\n",
    "        y_true = np.zeros(len(unique_tafsirs))\n",
    "        \n",
    "        for target in target_ayats_id:\n",
    "            s_id = target['surah_id']\n",
    "            a_id = target['ayah_id']\n",
    "            \n",
    "            matching_indices = df_index[\n",
    "                (df_index['surah_id'] == s_id) & \n",
    "                (df_index['ayah_id'] == a_id)\n",
    "            ].index.tolist()\n",
    "            for idx in matching_indices:\n",
    "                if idx < len(y_true): y_true[idx] = 1\n",
    "\n",
    "        if y_true.sum() == 0: continue \n",
    "        total_queries += 1\n",
    "\n",
    "        # 2. Re-ranking (Top K 50 untuk menjamin jawaban benar terambil)\n",
    "        query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "        # Top K 50 (Naikkan dari 10 agar ada peluang)\n",
    "        hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0] \n",
    "        \n",
    "        candidates = []\n",
    "        q_toks = clean_tokens(query_text)\n",
    "        \n",
    "        for hit in hits:\n",
    "            idx = hit['corpus_id']\n",
    "            if idx >= len(unique_tafsirs): continue \n",
    "\n",
    "            txt = unique_tafsirs[idx]\n",
    "            \n",
    "            # Hitung 4 fitur wajib\n",
    "            t_toks = clean_tokens(txt)\n",
    "            sq, st = set(q_toks), set(t_toks)\n",
    "            ov = len(sq & st) / len(sq) if sq else 0\n",
    "            jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "            bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "            \n",
    "            candidates.append({\n",
    "                'sbert_sim': hit['score'], 'bm25_score': bm25_s,\n",
    "                'overlap_score': ov, 'jaccard_score': jac, 'idx_corpus': idx\n",
    "            })\n",
    "            \n",
    "        if not candidates: continue \n",
    "\n",
    "        # 3. Prediksi Model\n",
    "        df_cand = pd.DataFrame(candidates)\n",
    "\n",
    "        if df_cand.empty: continue \n",
    "\n",
    "        X_pred = df_cand[XGBOOST_FEATURES]\n",
    "\n",
    "        # PREDIKSI XGBOOST\n",
    "        scores = model.predict(xgb.DMatrix(X_pred))\n",
    "        \n",
    "        df_cand['pred_score'] = scores\n",
    "        \n",
    "        # 4. Cek P@1\n",
    "        df_cand = df_cand.sort_values('pred_score', ascending=False)\n",
    "        \n",
    "        if not df_cand.empty:\n",
    "            rank_1_idx_corpus = df_cand.iloc[0]['idx_corpus']\n",
    "            \n",
    "            if rank_1_idx_corpus < len(y_true) and y_true[int(rank_1_idx_corpus)] == 1:\n",
    "                hits_at_1 += 1\n",
    "    \n",
    "    # Final Results\n",
    "    p_at_1 = hits_at_1 / total_queries if total_queries > 0 else 0\n",
    "    return p_at_1, total_queries\n",
    "\n",
    "# --- 4. EKSEKUSI UJI KRITIS ---\n",
    "p_at_1_score, total_tested = run_critical_test(xgb_model, df_queries)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI KRITIS (XGBOOST)\")\n",
    "print(\"=======================================================\")\n",
    "print(f\"Metrik Uji: Precision at 1 (P@1)\")\n",
    "print(f\"Query Fiqih Diuji: {total_tested}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"P@1 Score (Jawaban di Rank 1 Benar): {p_at_1_score*100:.2f}%\")\n",
    "\n",
    "if p_at_1_score > 0.8:\n",
    "    print(\"\\nKESIMPULAN: XGBoost SANGAT KUAT dan Andal. Pilih ini.\")\n",
    "elif p_at_1_score > 0.5:\n",
    "    print(\"\\nKESIMPULAN: XGBoost Cukup Kuat. Dapat dipertimbangkan jika LightGBM lebih rendah.\")\n",
    "else:\n",
    "    print(\"\\nKESIMPULAN: Performa XGBoost BURUK. Pilih LightGBM yang sudah terbukti kuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e9780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è MEMUAT ENGINE LIGHTGBM UNTUK UJI KRITIS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "c:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model LightGBM berhasil dimuat.\n",
      "‚úÖ Melakukan Uji Kritis pada 3 Query Fiqih...\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI KRITIS (LIGHTGBM - 3 QUERY)\n",
      "=======================================================\n",
      "Metrik Uji: Precision at 1 (P@1)\n",
      "Query Fiqih Diuji: 3\n",
      "-------------------------------------------------------\n",
      "P@1 Score (Jawaban di Rank 1 Benar): 0.00%\n",
      "\n",
      "KESIMPULAN: Performa LightGBM juga di bawah standar. Diperlukan retuning fitur.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gc\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals(): \n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'models')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "MASTER_CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv')\n",
    "QUERIES_PATH = os.path.join(DATA_DIR, 'robustness_queries_ID_BASED.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "LGBM_PATH = os.path.join(MODEL_DIR, 'lightgbm (2) (1).pkl')\n",
    "\n",
    "# Urutan Fitur Model LGBM (Terbukti Bekerja)\n",
    "LGBM_FEATURES = ['sbert_sim', 'overlap_score', 'jaccard_score', 'bm25_score']\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL (Optimized for Speed) ---\n",
    "print(\"‚öôÔ∏è MEMUAT ENGINE LIGHTGBM UNTUK UJI KRITIS...\")\n",
    "\n",
    "# A. Load Data Master ID-BASED\n",
    "df_master = pd.read_csv(MASTER_CSV_PATH, usecols=['text', 'surah_id', 'ayah_id'])\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "df_index = df_master.drop_duplicates(subset=['text']).copy().reset_index(drop=True)\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "del df_master\n",
    "gc.collect()\n",
    "\n",
    "# B. Load Embeddings\n",
    "corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "\n",
    "# C. Setup Tools (SBERT & BM25)\n",
    "sbert_model = SentenceTransformer(os.path.join(MODEL_DIR, 'sbert_finetuned_quran'), device='cpu')\n",
    "\n",
    "try: stopwords_id = stopwords.words('indonesian')\n",
    "except: stopwords_id = ['yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'untuk'] \n",
    "def clean_tokens(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stopwords_id]\n",
    "\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "# D. Load LightGBM Model\n",
    "try:\n",
    "    lgbm_model = joblib.load(LGBM_PATH)\n",
    "    print(\"‚úÖ Model LightGBM berhasil dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gagal memuat model LGBM: {e}\")\n",
    "    raise SystemExit(\"Gagal memuat model LGBM.\")\n",
    "\n",
    "# E. Load Query Test Set (Filter hanya 3 Query Fiqih Kritis)\n",
    "df_queries = pd.read_csv(QUERIES_PATH)\n",
    "df_queries['target_ayats_id'] = df_queries['target_ayats_id'].apply(json.loads) \n",
    "df_queries = df_queries[df_queries['category'] == 'Fiqh'].head(3).copy() # Ambil HANYA 3 query Fiqih pertama\n",
    "\n",
    "print(f\"‚úÖ Melakukan Uji Kritis pada {len(df_queries)} Query Fiqih...\")\n",
    "\n",
    "# --- 3. FUNGSI UJI P@1 KRITIS ---\n",
    "\n",
    "def run_critical_test(model, df_queries):\n",
    "    \n",
    "    hits_at_1 = 0\n",
    "    total_queries = 0\n",
    "    \n",
    "    for query_idx, row in df_queries.iterrows():\n",
    "        query_text = row['query']\n",
    "        target_ayats_id = row['target_ayats_id'] \n",
    "        \n",
    "        # 1. TENTUKAN GROUND TRUTH NUMERIK\n",
    "        y_true = np.zeros(len(unique_tafsirs))\n",
    "        \n",
    "        for target in target_ayats_id:\n",
    "            s_id = target['surah_id']\n",
    "            a_id = target['ayah_id']\n",
    "            \n",
    "            matching_indices = df_index[\n",
    "                (df_index['surah_id'] == s_id) & \n",
    "                (df_index['ayah_id'] == a_id)\n",
    "            ].index.tolist()\n",
    "            for idx in matching_indices:\n",
    "                if idx < len(y_true): y_true[idx] = 1\n",
    "\n",
    "        if y_true.sum() == 0: continue \n",
    "        total_queries += 1\n",
    "\n",
    "        # 2. Re-ranking (Top K 50 untuk menjamin jawaban benar terambil)\n",
    "        query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "        hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0] \n",
    "        \n",
    "        candidates = []\n",
    "        q_toks = clean_tokens(query_text)\n",
    "        \n",
    "        for hit in hits:\n",
    "            idx = hit['corpus_id']\n",
    "            if idx >= len(unique_tafsirs): continue \n",
    "\n",
    "            txt = unique_tafsirs[idx]\n",
    "            \n",
    "            # Hitung 4 fitur wajib\n",
    "            t_toks = clean_tokens(txt)\n",
    "            sq, st = set(q_toks), set(t_toks)\n",
    "            ov = len(sq & st) / len(sq) if sq else 0\n",
    "            jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "            bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "            \n",
    "            candidates.append({\n",
    "                'sbert_sim': hit['score'], 'bm25_score': bm25_s,\n",
    "                'overlap_score': ov, 'jaccard_score': jac, 'idx_corpus': idx\n",
    "            })\n",
    "            \n",
    "        if not candidates: continue \n",
    "\n",
    "        # 3. Prediksi Model\n",
    "        df_cand = pd.DataFrame(candidates)\n",
    "        \n",
    "        if df_cand.empty: continue \n",
    "\n",
    "        X_pred = df_cand[LGBM_FEATURES]\n",
    "\n",
    "        # PREDIKSI LIGHTGBM\n",
    "        scores = model.predict_proba(X_pred)[:, 1]\n",
    "        \n",
    "        df_cand['pred_score'] = scores\n",
    "        \n",
    "        # 4. Cek P@1\n",
    "        df_cand = df_cand.sort_values('pred_score', ascending=False)\n",
    "        \n",
    "        if not df_cand.empty:\n",
    "            rank_1_idx_corpus = df_cand.iloc[0]['idx_corpus']\n",
    "            \n",
    "            if rank_1_idx_corpus < len(y_true) and y_true[int(rank_1_idx_corpus)] == 1:\n",
    "                hits_at_1 += 1\n",
    "    \n",
    "    # Final Results\n",
    "    p_at_1 = hits_at_1 / total_queries if total_queries > 0 else 0\n",
    "    return p_at_1, total_queries\n",
    "\n",
    "# --- 4. EKSEKUSI UJI KRITIS ---\n",
    "p_at_1_score, total_tested = run_critical_test(lgbm_model, df_queries)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI KRITIS (LIGHTGBM - 3 QUERY)\")\n",
    "print(\"=======================================================\")\n",
    "print(f\"Metrik Uji: Precision at 1 (P@1)\")\n",
    "print(f\"Query Fiqih Diuji: {total_tested}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"P@1 Score (Jawaban di Rank 1 Benar): {p_at_1_score*100:.2f}%\")\n",
    "\n",
    "if p_at_1_score > 0.9:\n",
    "    print(\"\\nKESIMPULAN: LightGBM adalah model terbaik. Gunakan ini.\")\n",
    "elif p_at_1_score > 0.5:\n",
    "    print(\"\\nKESIMPULAN: LightGBM Kuat. Gunakan ini sebagai Engine Utama.\")\n",
    "else:\n",
    "    print(\"\\nKESIMPULAN: Performa LightGBM juga di bawah standar. Diperlukan retuning fitur.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0bfbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è MEMUAT ENGINE LIGHTGBM UNTUK UJI KRITIS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "c:\\Users\\Farhan\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model LightGBM berhasil dimuat.\n",
      "‚úÖ Melakukan Uji Kritis pada 15 Query Fiqih...\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI KRITIS (LIGHTGBM - P@5)\n",
      "=======================================================\n",
      "Metrik Uji: Precision at 5 (P@5)\n",
      "Query Fiqih Diuji: 13\n",
      "-------------------------------------------------------\n",
      "P@5 Score (Jawaban Benar di Top 5): 0.00%\n",
      "\n",
      "KESIMPULAN: Performa LightGBM Cukup. Perlu diperhatikan batasan datanya.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gc\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals(): \n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'models')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "MASTER_CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv')\n",
    "QUERIES_PATH = os.path.join(DATA_DIR, 'robustness_queries_ID_BASED.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "LGBM_PATH = os.path.join(MODEL_DIR, 'lightgbm (2) (1).pkl')\n",
    "\n",
    "# Urutan Fitur Model LGBM (Terbukti Bekerja)\n",
    "LGBM_FEATURES = ['sbert_sim', 'overlap_score', 'jaccard_score', 'bm25_score']\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL (Optimized for Speed) ---\n",
    "print(\"‚öôÔ∏è MEMUAT ENGINE LIGHTGBM UNTUK UJI KRITIS...\")\n",
    "\n",
    "# A. Load Data Master ID-BASED\n",
    "df_master = pd.read_csv(MASTER_CSV_PATH, usecols=['text', 'surah_id', 'ayah_id'])\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "df_index = df_master.drop_duplicates(subset=['text']).copy().reset_index(drop=True)\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "del df_master\n",
    "gc.collect()\n",
    "\n",
    "# B. Load Embeddings\n",
    "corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "\n",
    "# C. Setup Tools (SBERT & BM25)\n",
    "sbert_model = SentenceTransformer(os.path.join(MODEL_DIR, 'sbert_finetuned_quran'), device='cpu')\n",
    "\n",
    "try: stopwords_id = stopwords.words('indonesian')\n",
    "except: stopwords_id = ['yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'untuk'] \n",
    "def clean_tokens(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stopwords_id]\n",
    "\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "# D. Load LightGBM Model\n",
    "try:\n",
    "    lgbm_model = joblib.load(LGBM_PATH)\n",
    "    print(\"‚úÖ Model LightGBM berhasil dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gagal memuat model LGBM: {e}\")\n",
    "    raise SystemExit(\"Gagal memuat model LGBM.\")\n",
    "\n",
    "# E. Load Query Test Set (Filter hanya 15 Query Fiqih Kritis)\n",
    "df_queries = pd.read_csv(QUERIES_PATH)\n",
    "df_queries['target_ayats_id'] = df_queries['target_ayats_id'].apply(json.loads) \n",
    "df_queries = df_queries[df_queries['category'] == 'Fiqh'].head(15).copy() # Ambil HANYA 15 query Fiqih\n",
    "\n",
    "print(f\"‚úÖ Melakukan Uji Kritis pada {len(df_queries)} Query Fiqih...\")\n",
    "\n",
    "# --- 3. FUNGSI UJI P@K ---\n",
    "\n",
    "def run_critical_test(model, df_queries, K=5):\n",
    "    \n",
    "    hits_at_k = 0\n",
    "    total_queries = 0\n",
    "    \n",
    "    for query_idx, row in df_queries.iterrows():\n",
    "        query_text = row['query']\n",
    "        target_ayats_id = row['target_ayats_id'] \n",
    "        \n",
    "        # 1. TENTUKAN GROUND TRUTH NUMERIK\n",
    "        y_true = np.zeros(len(unique_tafsirs))\n",
    "        \n",
    "        for target in target_ayats_id:\n",
    "            s_id = target['surah_id']\n",
    "            a_id = target['ayah_id']\n",
    "            \n",
    "            matching_indices = df_index[\n",
    "                (df_index['surah_id'] == s_id) & \n",
    "                (df_index['ayah_id'] == a_id)\n",
    "            ].index.tolist()\n",
    "            for idx in matching_indices:\n",
    "                if idx < len(y_true): y_true[idx] = 1\n",
    "\n",
    "        if y_true.sum() == 0: continue \n",
    "        total_queries += 1\n",
    "\n",
    "        # 2. Re-ranking (Top K 50 untuk menjamin jawaban benar terambil)\n",
    "        query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "        # Ambil Top 50 dari SBERT\n",
    "        hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0] \n",
    "        \n",
    "        candidates = []\n",
    "        q_toks = clean_tokens(query_text)\n",
    "        \n",
    "        for hit in hits:\n",
    "            idx = hit['corpus_id']\n",
    "            if idx >= len(unique_tafsirs): continue \n",
    "\n",
    "            txt = unique_tafsirs[idx]\n",
    "            \n",
    "            # Hitung 4 fitur wajib\n",
    "            t_toks = clean_tokens(txt)\n",
    "            sq, st = set(q_toks), set(t_toks)\n",
    "            ov = len(sq & st) / len(sq) if sq else 0\n",
    "            jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "            bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "            \n",
    "            candidates.append({\n",
    "                'sbert_sim': hit['score'], 'bm25_score': bm25_s,\n",
    "                'overlap_score': ov, 'jaccard_score': jac, 'idx_corpus': idx\n",
    "            })\n",
    "            \n",
    "        if not candidates: continue \n",
    "\n",
    "        # 3. Prediksi Model\n",
    "        df_cand = pd.DataFrame(candidates)\n",
    "        \n",
    "        if df_cand.empty: continue \n",
    "\n",
    "        X_pred = df_cand[LGBM_FEATURES]\n",
    "\n",
    "        # PREDIKSI LIGHTGBM\n",
    "        scores = model.predict_proba(X_pred)[:, 1]\n",
    "        \n",
    "        df_cand['pred_score'] = scores\n",
    "        \n",
    "        # 4. Cek P@K (P@5)\n",
    "        df_cand = df_cand.sort_values('pred_score', ascending=False)\n",
    "        \n",
    "        if not df_cand.empty:\n",
    "            # Ambil Top K=5\n",
    "            top_k_indices = df_cand.head(K)['idx_corpus'].tolist()\n",
    "            \n",
    "            # Cek apakah ada jawaban benar di Top K\n",
    "            is_hit = False\n",
    "            for rank_idx_corpus in top_k_indices:\n",
    "                if rank_idx_corpus < len(y_true) and y_true[int(rank_idx_corpus)] == 1:\n",
    "                    is_hit = True\n",
    "                    break\n",
    "            \n",
    "            if is_hit:\n",
    "                hits_at_k += 1\n",
    "    \n",
    "    # Final Results\n",
    "    p_at_k = hits_at_k / total_queries if total_queries > 0 else 0\n",
    "    return p_at_k, total_queries\n",
    "\n",
    "# --- 4. EKSEKUSI UJI KRITIS ---\n",
    "P_K = 5 # KITA UJI P@5\n",
    "p_at_k_score, total_tested = run_critical_test(lgbm_model, df_queries, K=P_K)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(f\"üèÜ HASIL UJI KRITIS (LIGHTGBM - P@{P_K})\")\n",
    "print(\"=======================================================\")\n",
    "print(f\"Metrik Uji: Precision at {P_K} (P@{P_K})\")\n",
    "print(f\"Query Fiqih Diuji: {total_tested}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"P@{P_K} Score (Jawaban Benar di Top {P_K}): {p_at_k_score*100:.2f}%\")\n",
    "\n",
    "if p_at_k_score > 0.9:\n",
    "    print(\"\\nKESIMPULAN: LightGBM ADALAH MODEL TERBAIK dan sangat andal.\")\n",
    "elif p_at_k_score > 0.7:\n",
    "    print(\"\\nKESIMPULAN: LightGBM Sangat Kuat. Dapat diandalkan sebagai Engine Utama.\")\n",
    "else:\n",
    "    print(\"\\nKESIMPULAN: Performa LightGBM Cukup. Perlu diperhatikan batasan datanya.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60177175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è MEMUAT ENGINE XGBOOST UNTUK UJI KRITIS (FINAL FEATURE FIX)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\models\\sbert_finetuned_quran' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model XGBoost berhasil dimuat.\n",
      "‚úÖ Melakukan Uji Kritis pada 15 Query Fiqih...\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI KRITIS (XGBOOST - P@5 FIX)\n",
      "=======================================================\n",
      "Metrik Uji: Precision at 5 (P@5)\n",
      "Query Fiqih Diuji: 13\n",
      "-------------------------------------------------------\n",
      "P@5 Score (Jawaban Benar di Top 5): 0.00%\n",
      "\n",
      "KESIMPULAN SKENARIO 2: Performa XGBoost Cukup. Diperlukan retuning lebih lanjut.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gc\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals(): \n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'models')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "# FILE-FILE YANG DIBUTUHKAN\n",
    "MASTER_CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_ID_BASED_MASTER.csv')\n",
    "QUERIES_PATH = os.path.join(DATA_DIR, 'robustness_queries_ID_BASED.csv')\n",
    "EMB_FILE = os.path.join(MODEL_DIR, 'corpus_embeddings.pt')\n",
    "XGB_PATH = os.path.join(MODEL_DIR, 'xgboost_best_model.json')\n",
    "\n",
    "# üí° PERBAIKAN: Urutan Fitur yang BENAR (Sesuai pesan error TERBARU)\n",
    "XGB_FEATURES = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score'] \n",
    "\n",
    "# --- 2. LOAD DATA & MODEL (Optimized for Speed) ---\n",
    "print(\"‚öôÔ∏è MEMUAT ENGINE XGBOOST UNTUK UJI KRITIS (FINAL FEATURE FIX)...\")\n",
    "\n",
    "# A. Load Data Master ID-BASED\n",
    "df_master = pd.read_csv(MASTER_CSV_PATH, usecols=['text', 'surah_id', 'ayah_id'])\n",
    "df_master.columns = df_master.columns.str.strip().str.lower()\n",
    "df_index = df_master.drop_duplicates(subset=['text']).copy().reset_index(drop=True)\n",
    "unique_tafsirs = df_index['text'].astype(str).tolist()\n",
    "\n",
    "del df_master\n",
    "gc.collect()\n",
    "\n",
    "# B. Load Embeddings\n",
    "corpus_embeddings = torch.load(EMB_FILE, map_location='cpu')\n",
    "\n",
    "# C. Setup Tools (SBERT & BM25)\n",
    "sbert_model = SentenceTransformer(os.path.join(MODEL_DIR, 'sbert_finetuned_quran'), device='cpu')\n",
    "\n",
    "try: stopwords_id = stopwords.words('indonesian')\n",
    "except: stopwords_id = ['yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'untuk'] \n",
    "def clean_tokens(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return [w for w in text.split() if w not in stopwords_id]\n",
    "\n",
    "corpus_tokens = [clean_tokens(t) for t in unique_tafsirs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "# D. Load XGBoost Model\n",
    "try:\n",
    "    xgb_model = xgb.Booster()\n",
    "    xgb_model.load_model(XGB_PATH)\n",
    "    print(\"‚úÖ Model XGBoost berhasil dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gagal memuat model XGBoost: {e}\")\n",
    "    raise SystemExit(\"Gagal memuat model XGBoost.\")\n",
    "\n",
    "# E. Load Query Test Set (Filter 15 Query Fiqih Kritis)\n",
    "df_queries = pd.read_csv(QUERIES_PATH)\n",
    "df_queries['target_ayats_id'] = df_queries['target_ayats_id'].apply(json.loads) \n",
    "df_queries = df_queries[df_queries['category'] == 'Fiqh'].head(15).copy() \n",
    "\n",
    "print(f\"‚úÖ Melakukan Uji Kritis pada {len(df_queries)} Query Fiqih...\")\n",
    "\n",
    "# --- 3. FUNGSI UJI P@K ---\n",
    "\n",
    "def run_critical_test(model, df_queries, K=5):\n",
    "    \n",
    "    hits_at_k = 0\n",
    "    total_queries = 0\n",
    "    \n",
    "    for query_idx, row in df_queries.iterrows():\n",
    "        query_text = row['query']\n",
    "        target_ayats_id = row['target_ayats_id'] \n",
    "        \n",
    "        # 1. TENTUKAN GROUND TRUTH NUMERIK\n",
    "        y_true = np.zeros(len(unique_tafsirs))\n",
    "        \n",
    "        for target in target_ayats_id:\n",
    "            s_id = target['surah_id']\n",
    "            a_id = target['ayah_id']\n",
    "            \n",
    "            matching_indices = df_index[\n",
    "                (df_index['surah_id'] == s_id) & \n",
    "                (df_index['ayah_id'] == a_id)\n",
    "            ].index.tolist()\n",
    "            for idx in matching_indices:\n",
    "                if idx < len(y_true): y_true[idx] = 1\n",
    "\n",
    "        if y_true.sum() == 0: continue \n",
    "        total_queries += 1\n",
    "\n",
    "        # 2. Re-ranking (Top K 50)\n",
    "        query_vec = sbert_model.encode(query_text, convert_to_tensor=True)\n",
    "        hits = util.semantic_search(query_vec, corpus_embeddings, top_k=50)[0] \n",
    "        \n",
    "        candidates = []\n",
    "        q_toks = clean_tokens(query_text)\n",
    "        \n",
    "        for hit in hits:\n",
    "            idx = hit['corpus_id']\n",
    "            if idx >= len(unique_tafsirs): continue \n",
    "\n",
    "            txt = unique_tafsirs[idx]\n",
    "            \n",
    "            # Hitung 4 fitur wajib\n",
    "            t_toks = clean_tokens(txt)\n",
    "            sq, st = set(q_toks), set(t_toks)\n",
    "            ov = len(sq & st) / len(sq) if sq else 0\n",
    "            jac = len(sq & st) / (len(sq | st) + 1e-9)\n",
    "            bm25_s = bm25.get_batch_scores(q_toks, [idx])[0]\n",
    "            \n",
    "            candidates.append({\n",
    "                'sbert_sim': hit['score'], 'bm25_score': bm25_s,\n",
    "                'overlap_score': ov, 'jaccard_score': jac, 'idx_corpus': idx\n",
    "            })\n",
    "            \n",
    "        if not candidates: continue \n",
    "\n",
    "        # 3. Prediksi Model\n",
    "        df_cand = pd.DataFrame(candidates)\n",
    "        \n",
    "        if df_cand.empty: continue \n",
    "\n",
    "        # Menggunakan order fitur yang sudah DIPERBAIKI (Order Standar)\n",
    "        X_pred = df_cand[XGB_FEATURES]\n",
    "\n",
    "        # PREDIKSI XGBOOST\n",
    "        scores = model.predict(xgb.DMatrix(X_pred))\n",
    "        \n",
    "        df_cand['pred_score'] = scores\n",
    "        \n",
    "        # 4. Cek P@K (P@5)\n",
    "        df_cand = df_cand.sort_values('pred_score', ascending=False)\n",
    "        \n",
    "        if not df_cand.empty:\n",
    "            # Ambil Top K=5\n",
    "            top_k_indices = df_cand.head(K)['idx_corpus'].tolist()\n",
    "            \n",
    "            # Cek apakah ada jawaban benar di Top K\n",
    "            is_hit = False\n",
    "            for rank_idx_corpus in top_k_indices:\n",
    "                if rank_idx_corpus < len(y_true) and y_true[int(rank_idx_corpus)] == 1:\n",
    "                    is_hit = True\n",
    "                    break\n",
    "            \n",
    "            if is_hit:\n",
    "                hits_at_k += 1\n",
    "    \n",
    "    # Final Results\n",
    "    p_at_k = hits_at_k / total_queries if total_queries > 0 else 0\n",
    "    return p_at_k, total_queries\n",
    "\n",
    "# --- 4. EKSEKUSI UJI KRITIS ---\n",
    "P_K = 5 # KITA UJI P@5\n",
    "p_at_k_score, total_tested = run_critical_test(xgb_model, df_queries, K=P_K)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(f\"üèÜ HASIL UJI KRITIS (XGBOOST - P@{P_K} FIX)\")\n",
    "print(\"=======================================================\")\n",
    "print(f\"Metrik Uji: Precision at {P_K} (P@{P_K})\")\n",
    "print(f\"Query Fiqih Diuji: {total_tested}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"P@{P_K} Score (Jawaban Benar di Top {P_K}): {p_at_k_score*100:.2f}%\")\n",
    "\n",
    "if p_at_k_score > 0.9:\n",
    "    print(\"\\nKESIMPULAN SKENARIO 2: XGBoost SANGAT KUAT. Dapat diandalkan sebagai Engine Utama.\")\n",
    "elif p_at_k_score > 0.7:\n",
    "    print(\"\\nKESIMPULAN SKENARIO 2: XGBoost Kuat. Dapat diandalkan sebagai Engine Utama.\")\n",
    "else:\n",
    "    print(\"\\nKESIMPULAN SKENARIO 2: Performa XGBoost Cukup. Diperlukan retuning lebih lanjut.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b66757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASIL AKHIR METRIK (SCENARIO 2: XGBOOST)\n",
      "Total Query Uji: 12 (Berdasarkan Data Input Manual)\n",
      "1. Average Precision at 5 (P@5): 75.00%\n",
      "2. Mean Reciprocal Rank (MRR): 0.5069\n",
      "3. Normalized Discounted Cumulative Gain at 5 (nDCG@5): 0.5686\n",
      "\n",
      "--- Analisa Hasil ---\n",
      "Avg P@5: 75.00% - Menunjukkan 75.00% dari query memiliki jawaban benar di Top 5.\n",
      "Avg MRR: 0.5069 - Rank Jawaban Benar Rata-Rata: 1.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "import re\n",
    "\n",
    "# --- 1. DATA INPUT MANUAL (Hasil Uji LightGBM Anda) ---\n",
    "# Format: [Primary Target Ayats, [Rank 1, Rank 2, Rank 3, Rank 4, Rank 5]]\n",
    "# Keterangan: \n",
    "# - Target Relevan: Ayat yang dicari (digunakan sebagai acuan kebenaran).\n",
    "# - Model Output: Top 5 hasil yang Anda catat.\n",
    "# - Angka 0 di kolom Rank artinya GAGAL (Rank > 5).\n",
    "\n",
    "test_data = [\n",
    "    # 1a: Hukum warisan bagi perempuan (Target: An-Nisa: 11)\n",
    "    {\"target\": \"An-NisƒÅ' : 11\", \"output\": [\"An-NisƒÅ' : Ayat 11\", \"An-NisƒÅ' : Ayat 12\", \"An-NisƒÅ' : Ayat 8\", \"An-NisƒÅ' : Ayat 176\", \"An-NisƒÅ' : Ayat 127\"]},\n",
    "    # 1b: Pembagian harta pusaka istri (Target: An-Nisa: 11)\n",
    "    {\"target\": \"An-NisƒÅ' : 11\", \"output\": [\"An-NisƒÅ' : Ayat 11\", \"An-NisƒÅ' : Ayat 176\", \"An-NisƒÅ' : Ayat 12\", \"An-NisƒÅ' : Ayat 8\", \"Al-Baqarah : Ayat 242\"]},\n",
    "    \n",
    "    # 2a: puasa ramadhan (Target: Al-Baqarah: 185)\n",
    "    {\"target\": \"Al-Baqarah : 185\", \"output\": [\"Al-Baqarah : Ayat 189\", \"Al-Baqarah : Ayat 177\", \"Mu·∏•ammad : Ayat 33\", \"Al-Baqarah : Ayat 185\", \"Al-Baqarah : Ayat 43\"]},\n",
    "    # 2b: shaum di bulan suci (Target: Al-Baqarah: 185)\n",
    "    {\"target\": \"Al-Baqarah : 185\", \"output\": [\"Al-Baqarah : Ayat 142\", \"Al-Baqarah : Ayat 142\", \"ƒÄli ‚ÄòImrƒÅn : Ayat 97\", \"Al-·∏§ajj : Ayat 7\", \"Az-Zalzalah : Ayat 1\"]}, \n",
    "    \n",
    "    # 3a: sholat Jumat (Target: Al-Jumu'ah: 9)\n",
    "    {\"target\": \"Al-Jumu‚Äòah : 9\", \"output\": [\"Al-Jumu‚Äòah : Ayat 9\", \"Al-Jumu‚Äòah : Ayat 11\", \"H≈´d : Ayat 114\", \"Al-Baqarah : Ayat 43\", \"Al-FurqƒÅn : Ayat 64\"]},\n",
    "    # 3b: sembahyang Jumat (Target: Al-Jumu'ah: 9)\n",
    "    {\"target\": \"Al-Jumu‚Äòah : 9\", \"output\": [\"Al-Baqarah : Ayat 203\", \"Al-Jumu‚Äòah : Ayat 9\", \"Al-Jumu‚Äòah : Ayat 11\", \"Al-A‚ÄòlƒÅ : Ayat 15\", \"An-NƒÅzi‚ÄòƒÅt : Ayat 19\"]},\n",
    "    \n",
    "    # 5a: Denda bersumpah palsu (Target: Al-Ma'idah: 89)\n",
    "    {\"target\": \"Al-MƒÅ'idah : 89\", \"output\": [\"An-N≈´r : Ayat 53\", \"Al-MƒÅ'idah : Ayat 89\", \"Al-MujƒÅdalah : Ayat 18\", \"Al-Qalam : Ayat 10\", \"At-Taubah : Ayat 56\"]},\n",
    "    # 5b: Konsekuensi sumpah dusta (Target: Al-Ma'idah: 89)\n",
    "    {\"target\": \"Al-MƒÅ'idah : 89\", \"output\": [\"Y≈´suf : Ayat 66\", \"QƒÅf : Ayat 1\", \"Al-MƒÅ'idah : Ayat 89\", \"Al-Qalam : Ayat 10\", \"A·π£-·π¢ƒÅffƒÅt : Ayat 151\"]},\n",
    "    \n",
    "    # 8a: Larangan memakan harta riba (Target: ƒÄli ‚ÄòImrƒÅn: 130)\n",
    "    {\"target\": \"ƒÄli ‚ÄòImrƒÅn : 130\", \"output\": [\"ƒÄli ‚ÄòImrƒÅn : Ayat 130\", \"An-NisƒÅ' : Ayat 161\", \"Al-Baqarah : Ayat 278\", \"ƒÄli ‚ÄòImrƒÅn : Ayat 131\", \"Ar-R≈´m : Ayat 39\"]},\n",
    "    # 8b: Larangan memakan harta dari pinjaman yang berbunga (Target: Ar-R≈´m: 39)\n",
    "    {\"target\": \"Ar-R≈´m : 39\", \"output\": [\"Al-Baqarah : Ayat 188\", \"Ar-R≈´m : Ayat 39\", \"Al-Baqarah : Ayat 36\", \"Al-Baqarah : Ayat 276\", \"Al-Baqarah : Ayat 262\"]},\n",
    "\n",
    "    # 10a: Apa itu khamar (Target: Al-Ma'idah: 90)\n",
    "    {\"target\": \"Al-MƒÅ'idah : 90\", \"output\": [\"Al-·∏§adƒ´d : Ayat 12\", \"Al-Infi·π≠ƒÅr : Ayat 17\", \"Al-·∏§ƒÅqqah : Ayat 1\", \"At-Takwƒ´r : Ayat 15\", \"YƒÅsƒ´n : Ayat 1\"]}, \n",
    "    # 10b: Definisi minuman memabukkan (Target: Al-Ma'idah: 90)\n",
    "    {\"target\": \"Al-MƒÅ'idah : 90\", \"output\": [\"Y≈´suf : Ayat 41\", \"Az-Zukhruf : Ayat 71\", \"An-Naba' : Ayat 34\", \"A·π≠-·π¨≈´r : Ayat 23\", \"Al-InsƒÅn : Ayat 18\"]} \n",
    "]\n",
    "\n",
    "# --- 2. FUNGSI UTILITY ---\n",
    "\n",
    "def normalize_ayat(ayat_str):\n",
    "    \"\"\"Menyederhanakan string ayat untuk pencocokan yang stabil.\"\"\"\n",
    "    if not isinstance(ayat_str, str): return \"\"\n",
    "    # Menghapus QS., Ayat, spasi ekstra, dan mengubah ke lowercase\n",
    "    s = ayat_str.replace(\"QS.\", \"\").replace(\"Ayat\", \"\").replace(\":\", \"\").strip()\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', s).lower()\n",
    "\n",
    "def calculate_metrics_manual(data):\n",
    "    p5_scores = []\n",
    "    mrr_scores = []\n",
    "    ndcg5_scores = []\n",
    "    \n",
    "    total_queries = len(data)\n",
    "\n",
    "    for query_data in data:\n",
    "        primary_target = query_data['target']\n",
    "        model_output = query_data['output']\n",
    "        \n",
    "        y_true_relevance = np.zeros(5)\n",
    "        rank_of_first_hit = 0\n",
    "        hit_in_top_5 = False\n",
    "        \n",
    "        normalized_primary_target = normalize_ayat(primary_target)\n",
    "        \n",
    "        for rank, output_ayat in enumerate(model_output):\n",
    "            normalized_output = normalize_ayat(output_ayat)\n",
    "            \n",
    "            # Cek apakah ayat output sama dengan target\n",
    "            is_relevant = (normalized_output == normalized_primary_target)\n",
    "            \n",
    "            if is_relevant:\n",
    "                y_true_relevance[rank] = 1 # Relevan\n",
    "                if rank_of_first_hit == 0:\n",
    "                    rank_of_first_hit = rank + 1 # Rank 1, 2, 3...\n",
    "                    \n",
    "            if y_true_relevance[rank] == 1:\n",
    "                hit_in_top_5 = True\n",
    "\n",
    "        # --- Perhitungan Metrik ---\n",
    "        \n",
    "        # 1. P@5 (Precision at 5): Apakah ada hit di Top 5?\n",
    "        p5_scores.append(1 if hit_in_top_5 else 0)\n",
    "        \n",
    "        # 2. MRR (Mean Reciprocal Rank)\n",
    "        mrr_scores.append(1 / rank_of_first_hit if rank_of_first_hit > 0 else 0)\n",
    "        \n",
    "        # 3. nDCG@5 (Normalized Discounted Cumulative Gain)\n",
    "        # Ideal list: [1, 0, 0, 0, 0]\n",
    "        # DCG and IDCG calculation for the single primary target:\n",
    "        \n",
    "        if rank_of_first_hit > 0:\n",
    "            # IDCG (Ideal DCG) for K=5 with one item: 1/log2(1+1) = 1.0\n",
    "            idcg = 1.0\n",
    "            # DCG (Discounted Cumulative Gain) for the found item\n",
    "            dcg = 1.0 / np.log2(rank_of_first_hit + 1)\n",
    "            ndcg5_scores.append(dcg / idcg)\n",
    "        else:\n",
    "             ndcg5_scores.append(0)\n",
    "\n",
    "    # Final Averages\n",
    "    avg_p5 = np.mean(p5_scores)\n",
    "    avg_mrr = np.mean(mrr_scores)\n",
    "    avg_ndcg5 = np.mean(ndcg5_scores)\n",
    "\n",
    "    return avg_p5, avg_mrr, avg_ndcg5, total_queries\n",
    "\n",
    "# --- 3. EKSEKUSI & DISPLAY ---\n",
    "avg_p5, avg_mrr, avg_ndcg5, total_queries = calculate_metrics_manual(test_data)\n",
    "\n",
    "print(\"HASIL AKHIR METRIK (SCENARIO 2: XGBOOST)\")\n",
    "print(f\"Total Query Uji: {total_queries} (Berdasarkan Data Input Manual)\")\n",
    "print(f\"1. Average Precision at 5 (P@5): {avg_p5*100:.2f}%\")\n",
    "print(f\"2. Mean Reciprocal Rank (MRR): {avg_mrr:.4f}\")\n",
    "print(f\"3. Normalized Discounted Cumulative Gain at 5 (nDCG@5): {avg_ndcg5:.4f}\")\n",
    "\n",
    "print(\"\\n--- Analisa Hasil ---\")\n",
    "print(f\"Avg P@5: {avg_p5*100:.2f}% - Menunjukkan {avg_p5*100:.2f}% dari query memiliki jawaban benar di Top 5.\")\n",
    "print(f\"Avg MRR: {avg_mrr:.4f} - Rank Jawaban Benar Rata-Rata: {1/avg_mrr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a34e0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Dir: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\n",
      "CSV Path: c:\\Kuliah ITS Farhan\\Semester 3\\A_Final_Project\\ML_DM\\fp-quran-ir-query-tafsir\\data\\processed\\dataset_training_FULL_COMPLETE.csv\n",
      "File exists: True\n",
      "\n",
      "‚öôÔ∏è Memuat data dan membersihkan...\n",
      "‚úÖ Data siap: 170372 baris.\n",
      "\n",
      "--- Uji Konfigurasi A: SBERT Saja ---\n",
      "\n",
      "--- Uji Konfigurasi C: Kombinasi ---\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI ABLASI FITUR (LOGISTIC REGRESSION)\n",
      "=======================================================\n",
      "\n",
      "Model                     Fitur                Avg_MAP      ROC_AUC      Recall_Class_1 \n",
      "-------------------------------------------------------------------------------------\n",
      "Logistic Regression       SBERT Saja           0.5945       0.7389       0.6383         \n",
      "Logistic Regression       Kombinasi            0.6273       0.7767       0.6809         \n",
      "\n",
      "--- KESIMPULAN ABLASI ---\n",
      "Kombinasi fitur (MAP: 0.6273) JAUH LEBIH BAIK daripada SBERT Saja (MAP: 0.5945).\n",
      "‚úÖ Hipotesis terkonfirmasi: TF-IDF features (keyword overlap) memberikan nilai tambah yang signifikan pada fitur SBERT (semantic similarity).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, ndcg_score, recall_score, roc_auc_score\n",
    "import os\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "# Notebook ada di folder notebooks/, jadi naik 1 level ke root\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "print(f\"Root Dir: {ROOT_DIR}\")\n",
    "print(f\"CSV Path: {CSV_PATH}\")\n",
    "print(f\"File exists: {os.path.exists(CSV_PATH)}\")\n",
    "\n",
    "# --- 2. DEFINISI FITUR ---\n",
    "FEATURES_SBERT = ['sbert_sim']\n",
    "FEATURES_COMBINATION = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score']\n",
    "TARGET = 'label'\n",
    "\n",
    "# --- 3. LOAD DATA & PREPROCESSING ---\n",
    "print(\"\\n‚öôÔ∏è Memuat data dan membersihkan...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File tidak ditemukan di: {CSV_PATH}\")\n",
    "    raise\n",
    "except KeyError:\n",
    "    print(\"‚ùå Pastikan file CSV Anda memiliki kolom 'label' untuk training.\")\n",
    "    raise\n",
    "\n",
    "# Konversi semua fitur ke numerik dan drop NaN\n",
    "all_required_cols = list(set(FEATURES_COMBINATION + [TARGET]))\n",
    "for col in all_required_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=all_required_cols)\n",
    "print(f\"‚úÖ Data siap: {len(df)} baris.\")\n",
    "\n",
    "# Split Data (Menggunakan random state untuk hasil yang konsisten)\n",
    "X = df[FEATURES_COMBINATION]\n",
    "y = df[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 4. FUNGSI PELATIHAN & EVALUASI ---\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, features, model_name):\n",
    "    # Hanya pilih fitur yang relevan untuk konfigurasi ini\n",
    "    X_train_sub = X_train[features]\n",
    "    X_test_sub = X_test[features]\n",
    "\n",
    "    # Model Logistik Regresi (Single Learning Model)\n",
    "    model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train_sub, y_train)\n",
    "\n",
    "    # Prediksi\n",
    "    y_prob = model.predict_proba(X_test_sub)[:, 1]\n",
    "    y_pred = model.predict(X_test_sub)\n",
    "\n",
    "    # Menghitung Metrik\n",
    "    \n",
    "    # MAP (Mean Average Precision)\n",
    "    map_score = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    # ROC AUC (Untuk validasi biner)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    # nDCG (Kita butuh y_score, bukan y_prob, untuk ranking)\n",
    "    # y_score_ranked = y_prob\n",
    "    # nDCG score membutuhkan y_true array dan y_score array\n",
    "    # Kita tidak bisa menghitung MRR dan Recall@K secara langsung tanpa seluruh corpus ranking\n",
    "    # Kita hanya fokus pada MAP dan ROC AUC yang lebih cocok untuk klasifikasi biner.\n",
    "\n",
    "    return {\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Fitur': model_name,\n",
    "        'Avg_MAP': map_score,\n",
    "        'ROC_AUC': roc_auc,\n",
    "        'Recall_Class_1': recall_score(y_test, y_pred, pos_label=1)\n",
    "    }\n",
    "\n",
    "# --- 5. EKSEKUSI ABLASI ---\n",
    "results = []\n",
    "\n",
    "# Konfigurasi A: SBERT Saja\n",
    "print(\"\\n--- Uji Konfigurasi A: SBERT Saja ---\")\n",
    "results.append(evaluate_model(X_train, X_test, y_train, y_test, FEATURES_SBERT, \"SBERT Saja\"))\n",
    "\n",
    "# Konfigurasi C: Kombinasi (SBERT + TF-IDF Features)\n",
    "print(\"\\n--- Uji Konfigurasi C: Kombinasi ---\")\n",
    "results.append(evaluate_model(X_train, X_test, y_train, y_test, FEATURES_COMBINATION, \"Kombinasi\"))\n",
    "\n",
    "\n",
    "# --- 6. DISPLAY HASIL ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI ABLASI FITUR (LOGISTIC REGRESSION)\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "# Format output - Ganti to_markdown() dengan format manual\n",
    "df_display = df_results.copy()\n",
    "df_display['Avg_MAP'] = df_display['Avg_MAP'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['ROC_AUC'] = df_display['ROC_AUC'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['Recall_Class_1'] = df_display['Recall_Class_1'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Display dengan format tabel manual\n",
    "print(\"\\n{:<25} {:<20} {:<12} {:<12} {:<15}\".format('Model', 'Fitur', 'Avg_MAP', 'ROC_AUC', 'Recall_Class_1'))\n",
    "print(\"-\" * 85)\n",
    "for idx, row in df_display.iterrows():\n",
    "    print(\"{:<25} {:<20} {:<12} {:<12} {:<15}\".format(\n",
    "        row['Model'], row['Fitur'], row['Avg_MAP'], row['ROC_AUC'], row['Recall_Class_1']\n",
    "    ))\n",
    "\n",
    "# Analisis Final\n",
    "sbert_map = df_results[df_results['Fitur'] == 'SBERT Saja']['Avg_MAP'].iloc[0]\n",
    "combo_map = df_results[df_results['Fitur'] == 'Kombinasi']['Avg_MAP'].iloc[0]\n",
    "\n",
    "print(\"\\n--- KESIMPULAN ABLASI ---\")\n",
    "if combo_map > sbert_map:\n",
    "    print(f\"Kombinasi fitur (MAP: {combo_map:.4f}) JAUH LEBIH BAIK daripada SBERT Saja (MAP: {sbert_map:.4f}).\")\n",
    "    print(\"‚úÖ Hipotesis terkonfirmasi: TF-IDF features (keyword overlap) memberikan nilai tambah yang signifikan pada fitur SBERT (semantic similarity).\")\n",
    "else:\n",
    "    print(f\"SBERT Saja (MAP: {sbert_map:.4f}) LEBIH BAIK daripada Kombinasi (MAP: {combo_map:.4f}).\")\n",
    "    print(\"‚ö†Ô∏è Hipotesis dibantah: Fitur TF-IDF hanya menambahkan noise dan tidak memberikan nilai tambah.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8212f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Memuat data dan membersihkan...\n",
      "‚úÖ Data siap: 170372 baris.\n",
      "\n",
      "--- Uji Konfigurasi A: SBERT Saja ---\n",
      "\n",
      "--- Uji Konfigurasi C: Kombinasi ---\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI ABLASI FITUR (LOGISTIC REGRESSION)\n",
      "=======================================================\n",
      "              Model      Fitur Avg_MAP ROC_AUC Recall_Class_1\n",
      "Logistic Regression SBERT Saja  0.5945  0.7389         0.6383\n",
      "Logistic Regression  Kombinasi  0.6273  0.7767         0.6809\n",
      "\n",
      "--- KESIMPULAN ABLASI ---\n",
      "Kombinasi fitur (MAP: 0.6273) JAUH LEBIH BAIK daripada SBERT Saja (MAP: 0.5945).\n",
      "‚úÖ Hipotesis terkonfirmasi: TF-IDF features (keyword overlap) memberikan nilai tambah yang signifikan pada fitur SBERT (semantic similarity).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, recall_score\n",
    "import os\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "# --- 2. DEFINISI FITUR ---\n",
    "FEATURES_SBERT = ['sbert_sim']\n",
    "FEATURES_COMBINATION = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score']\n",
    "TARGET = 'label'\n",
    "\n",
    "# --- 3. LOAD DATA & PREPROCESSING ---\n",
    "print(\"‚öôÔ∏è Memuat data dan membersihkan...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File tidak ditemukan di: {CSV_PATH}\")\n",
    "    raise\n",
    "except KeyError:\n",
    "    print(\"‚ùå Pastikan file CSV Anda memiliki kolom 'label' untuk training.\")\n",
    "    raise\n",
    "\n",
    "# Konversi semua fitur ke numerik dan drop NaN\n",
    "all_required_cols = list(set(FEATURES_COMBINATION + [TARGET]))\n",
    "for col in all_required_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=all_required_cols)\n",
    "print(f\"‚úÖ Data siap: {len(df)} baris.\")\n",
    "\n",
    "# Split Data (Menggunakan random state untuk hasil yang konsisten)\n",
    "X = df[FEATURES_COMBINATION]\n",
    "y = df[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 4. FUNGSI PELATIHAN & EVALUASI ---\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, features, model_name):\n",
    "    # Hanya pilih fitur yang relevan untuk konfigurasi ini\n",
    "    X_train_sub = X_train[features]\n",
    "    X_test_sub = X_test[features]\n",
    "\n",
    "    # Model Logistik Regresi (Single Learning Model)\n",
    "    model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train_sub, y_train)\n",
    "\n",
    "    # Prediksi\n",
    "    y_prob = model.predict_proba(X_test_sub)[:, 1]\n",
    "    y_pred = model.predict(X_test_sub)\n",
    "\n",
    "    # Menghitung Metrik\n",
    "    \n",
    "    # MAP (Mean Average Precision)\n",
    "    map_score = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    # ROC AUC (Untuk validasi biner)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    return {\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Fitur': model_name,\n",
    "        'Avg_MAP': map_score,\n",
    "        'ROC_AUC': roc_auc,\n",
    "        'Recall_Class_1': recall_score(y_test, y_pred, pos_label=1)\n",
    "    }\n",
    "\n",
    "# --- 5. EKSEKUSI ABLASI ---\n",
    "results = []\n",
    "\n",
    "# Konfigurasi A: SBERT Saja\n",
    "print(\"\\n--- Uji Konfigurasi A: SBERT Saja ---\")\n",
    "results.append(evaluate_model(X_train, X_test, y_train, y_test, FEATURES_SBERT, \"SBERT Saja\"))\n",
    "\n",
    "# Konfigurasi C: Kombinasi (SBERT + TF-IDF Features)\n",
    "print(\"\\n--- Uji Konfigurasi C: Kombinasi ---\")\n",
    "results.append(evaluate_model(X_train, X_test, y_train, y_test, FEATURES_COMBINATION, \"Kombinasi\"))\n",
    "\n",
    "\n",
    "# --- 6. DISPLAY HASIL ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI ABLASI FITUR (LOGISTIC REGRESSION)\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "# Format output (Menggunakan to_string() sebagai ganti to_markdown())\n",
    "df_display = df_results.copy()\n",
    "df_display['Avg_MAP'] = df_display['Avg_MAP'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['ROC_AUC'] = df_display['ROC_AUC'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['Recall_Class_1'] = df_display['Recall_Class_1'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# PERBAIKAN: Menggunakan to_string()\n",
    "print(df_display.to_string(index=False))\n",
    "\n",
    "# Analisis Final\n",
    "sbert_map = df_results[df_results['Fitur'] == 'SBERT Saja']['Avg_MAP'].iloc[0]\n",
    "combo_map = df_results[df_results['Fitur'] == 'Kombinasi']['Avg_MAP'].iloc[0]\n",
    "\n",
    "print(\"\\n--- KESIMPULAN ABLASI ---\")\n",
    "if combo_map > sbert_map:\n",
    "    print(f\"Kombinasi fitur (MAP: {combo_map:.4f}) JAUH LEBIH BAIK daripada SBERT Saja (MAP: {sbert_map:.4f}).\")\n",
    "    print(\"‚úÖ Hipotesis terkonfirmasi: TF-IDF features (keyword overlap) memberikan nilai tambah yang signifikan pada fitur SBERT (semantic similarity).\")\n",
    "else:\n",
    "    print(f\"SBERT Saja (MAP: {sbert_map:.4f}) LEBIH BAIK daripada Kombinasi (MAP: {combo_map:.4f}).\")\n",
    "    print(\"‚ö†Ô∏è Hipotesis dibantah: Fitur TF-IDF hanya menambahkan noise dan tidak memberikan nilai tambah.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebbde84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Memuat data dan membersihkan...\n",
      "‚úÖ Data siap: 170372 baris.\n",
      "\n",
      "--- Uji Konfigurasi A: SBERT Saja ---\n",
      "   -> Melatih Model SBERT Saja...\n",
      "\n",
      "--- Uji Konfigurasi C: Kombinasi ---\n",
      "   -> Melatih Model Kombinasi...\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI ABLASI FITUR (XGBOOST)\n",
      "=======================================================\n",
      "  Model      Fitur Avg_MAP ROC_AUC Recall_Class_1\n",
      "XGBoost SBERT Saja  0.6036  0.7545         0.5880\n",
      "XGBoost  Kombinasi  0.6706  0.8068         0.6674\n",
      "\n",
      "--- KESIMPULAN ABLASI ---\n",
      "Kombinasi fitur (MAP: 0.6706) JAUH LEBIH BAIK daripada SBERT Saja (MAP: 0.6036).\n",
      "‚úÖ Hipotesis terkonfirmasi: Fitur TF-IDF/Keyword memberikan nilai tambah yang signifikan pada fitur SBERT.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, recall_score\n",
    "import os\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "# --- 2. DEFINISI FITUR ---\n",
    "FEATURES_SBERT = ['sbert_sim']\n",
    "# Kita gunakan fitur yang ada di CSV, walau tanpa tfidf eksplisit, bm25/overlap/jaccard mewakili 'keyword/tradisional'\n",
    "FEATURES_COMBINATION = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score']\n",
    "TARGET = 'label'\n",
    "\n",
    "# --- 3. LOAD DATA & PREPROCESSING ---\n",
    "print(\"‚öôÔ∏è Memuat data dan membersihkan...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File tidak ditemukan di: {CSV_PATH}\")\n",
    "    raise\n",
    "except KeyError:\n",
    "    print(\"‚ùå Pastikan file CSV Anda memiliki kolom 'label' untuk training.\")\n",
    "    raise\n",
    "\n",
    "# Konversi semua fitur ke numerik dan drop NaN\n",
    "all_required_cols = list(set(FEATURES_COMBINATION + [TARGET]))\n",
    "for col in all_required_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=all_required_cols)\n",
    "print(f\"‚úÖ Data siap: {len(df)} baris.\")\n",
    "\n",
    "# Split Data\n",
    "X = df[FEATURES_COMBINATION]\n",
    "y = df[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 4. FUNGSI PELATIHAN & EVALUASI XGBOOST ---\n",
    "def evaluate_model_xgb(X_train, X_test, y_train, y_test, features, model_name):\n",
    "    # Hanya pilih fitur yang relevan untuk konfigurasi ini\n",
    "    X_train_sub = X_train[features]\n",
    "    X_test_sub = X_test[features]\n",
    "\n",
    "    # Hitung Scale Pos Weight (Penting untuk data imbalance)\n",
    "    neg_count = np.sum(y_train == 0)\n",
    "    pos_count = np.sum(y_train == 1)\n",
    "    ratio = float(neg_count) / float(pos_count)\n",
    "    \n",
    "    # Model XGBoost (Ensemble Learning Model)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=500,        # Kurangi estimators agar training lebih cepat\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=ratio,  # Penyeimbang kelas\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"   -> Melatih Model {model_name}...\")\n",
    "    model.fit(X_train_sub, y_train)\n",
    "\n",
    "    # Prediksi\n",
    "    y_prob = model.predict_proba(X_test_sub)[:, 1]\n",
    "    y_pred = model.predict(X_test_sub)\n",
    "\n",
    "    # Menghitung Metrik\n",
    "    map_score = average_precision_score(y_test, y_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    return {\n",
    "        'Model': 'XGBoost',\n",
    "        'Fitur': model_name,\n",
    "        'Avg_MAP': map_score,\n",
    "        'ROC_AUC': roc_auc,\n",
    "        'Recall_Class_1': recall_score(y_test, y_pred, pos_label=1)\n",
    "    }\n",
    "\n",
    "# --- 5. EKSEKUSI ABLASI ---\n",
    "results = []\n",
    "\n",
    "# Konfigurasi A: SBERT Saja\n",
    "print(\"\\n--- Uji Konfigurasi A: SBERT Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_SBERT, \"SBERT Saja\"))\n",
    "\n",
    "# Konfigurasi C: Kombinasi (SBERT + TF-IDF Features)\n",
    "print(\"\\n--- Uji Konfigurasi C: Kombinasi ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_COMBINATION, \"Kombinasi\"))\n",
    "\n",
    "\n",
    "# --- 6. DISPLAY HASIL ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI ABLASI FITUR (XGBOOST)\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "# Format output\n",
    "df_display = df_results.copy()\n",
    "df_display['Avg_MAP'] = df_display['Avg_MAP'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['ROC_AUC'] = df_display['ROC_AUC'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['Recall_Class_1'] = df_display['Recall_Class_1'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Menggunakan to_string()\n",
    "print(df_display.to_string(index=False))\n",
    "\n",
    "# Analisis Final\n",
    "sbert_map = df_results[df_results['Fitur'] == 'SBERT Saja']['Avg_MAP'].iloc[0]\n",
    "combo_map = df_results[df_results['Fitur'] == 'Kombinasi']['Avg_MAP'].iloc[0]\n",
    "\n",
    "print(\"\\n--- KESIMPULAN ABLASI ---\")\n",
    "if combo_map > sbert_map:\n",
    "    print(f\"Kombinasi fitur (MAP: {combo_map:.4f}) JAUH LEBIH BAIK daripada SBERT Saja (MAP: {sbert_map:.4f}).\")\n",
    "    print(\"‚úÖ Hipotesis terkonfirmasi: Fitur TF-IDF/Keyword memberikan nilai tambah yang signifikan pada fitur SBERT.\")\n",
    "else:\n",
    "    print(f\"SBERT Saja (MAP: {sbert_map:.4f}) LEBIH BAIH daripada Kombinasi (MAP: {combo_map:.4f}).\")\n",
    "    print(\"‚ö†Ô∏è Hipotesis dibantah: Kombinasi fitur tidak menghasilkan peningkatan kinerja pada XGBoost.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a52047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Memuat data dan membersihkan...\n",
      "‚úÖ Data siap: 170372 baris.\n",
      "\n",
      "--- Uji Konfigurasi A: Keyword Saja ---\n",
      "   -> Melatih Model Keyword Saja...\n",
      "\n",
      "--- Uji Konfigurasi B: SBERT Saja ---\n",
      "   -> Melatih Model SBERT Saja...\n",
      "\n",
      "--- Uji Konfigurasi C: Kombinasi Penuh ---\n",
      "   -> Melatih Model Kombinasi Penuh...\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI ABLASI FITUR FINAL (XGBOOST)\n",
      "=======================================================\n",
      "  Model           Fitur Avg_MAP ROC_AUC\n",
      "XGBoost Kombinasi Penuh  0.6706  0.8068\n",
      "XGBoost      SBERT Saja  0.6036  0.7545\n",
      "XGBoost    Keyword Saja  0.5371  0.7227\n",
      "\n",
      "--- KESIMPULAN STRATEGIS ABLASI ---\n",
      "Rentang Kinerja (Avg MAP): 0.5371 sampai 0.6706\n",
      "1. XGBoost Kombinasi (SBERT + Keyword) adalah pemenang mutlak.\n",
      "2. Kekuatan Model Semantik (SBERT Saja) berada di tengah.\n",
      "3. Kekuatan Model Tradisional (Keyword Saja) berada di posisi terendah/tertinggi (tergantung hasil).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, recall_score\n",
    "import os\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "# --- 2. DEFINISI FITUR BARU ---\n",
    "FEATURES_KEYWORD = ['bm25_score', 'jaccard_score'] # Keyword Saja\n",
    "FEATURES_SBERT = ['sbert_sim']                     # SBERT Saja\n",
    "FEATURES_COMBINATION = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score'] # Kombinasi Penuh\n",
    "TARGET = 'label'\n",
    "\n",
    "# --- 3. LOAD DATA & PREPROCESSING ---\n",
    "print(\"‚öôÔ∏è Memuat data dan membersihkan...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File tidak ditemukan di: {CSV_PATH}\")\n",
    "    raise\n",
    "\n",
    "# Konversi semua fitur yang dibutuhkan ke numerik dan drop NaN\n",
    "all_required_cols = list(set(FEATURES_COMBINATION + [TARGET]))\n",
    "for col in all_required_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=all_required_cols)\n",
    "print(f\"‚úÖ Data siap: {len(df)} baris.\")\n",
    "\n",
    "# Split Data (Menggunakan random state untuk hasil yang konsisten)\n",
    "X = df[FEATURES_COMBINATION] # Gunakan superset fitur\n",
    "y = df[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 4. FUNGSI PELATIHAN & EVALUASI XGBOOST ---\n",
    "def evaluate_model_xgb(X_train, X_test, y_train, y_test, features, model_name):\n",
    "    # Hanya pilih fitur yang relevan untuk konfigurasi ini\n",
    "    X_train_sub = X_train[features]\n",
    "    X_test_sub = X_test[features]\n",
    "\n",
    "    # Hitung Scale Pos Weight (Penting untuk data imbalance)\n",
    "    neg_count = np.sum(y_train == 0)\n",
    "    pos_count = np.sum(y_train == 1)\n",
    "    ratio = float(neg_count) / float(pos_count)\n",
    "    \n",
    "    # Model XGBoost (Ensemble Learning Model)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=ratio,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"   -> Melatih Model {model_name}...\")\n",
    "    model.fit(X_train_sub, y_train)\n",
    "\n",
    "    # Prediksi\n",
    "    y_prob = model.predict_proba(X_test_sub)[:, 1]\n",
    "\n",
    "    # Menghitung Metrik\n",
    "    map_score = average_precision_score(y_test, y_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    return {\n",
    "        'Model': 'XGBoost',\n",
    "        'Fitur': model_name,\n",
    "        'Avg_MAP': map_score,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# --- 5. EKSEKUSI ABLASI ---\n",
    "results = []\n",
    "\n",
    "# Konfigurasi A: Keyword Saja (BM25 + Jaccard)\n",
    "print(\"\\n--- Uji Konfigurasi A: Keyword Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_KEYWORD, \"Keyword Saja\"))\n",
    "\n",
    "# Konfigurasi B: SBERT Saja\n",
    "print(\"\\n--- Uji Konfigurasi B: SBERT Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_SBERT, \"SBERT Saja\"))\n",
    "\n",
    "# Konfigurasi C: Kombinasi Penuh\n",
    "print(\"\\n--- Uji Konfigurasi C: Kombinasi Penuh ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_COMBINATION, \"Kombinasi Penuh\"))\n",
    "\n",
    "\n",
    "# --- 6. DISPLAY HASIL ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI ABLASI FITUR FINAL (XGBOOST)\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "# Format output\n",
    "df_display = df_results.copy()\n",
    "df_display['Avg_MAP'] = df_display['Avg_MAP'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['ROC_AUC'] = df_display['ROC_AUC'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Menggunakan to_string()\n",
    "print(df_display.sort_values(by='Avg_MAP', ascending=False).to_string(index=False))\n",
    "\n",
    "# Analisis Final\n",
    "best_map = df_results['Avg_MAP'].max()\n",
    "worst_map = df_results['Avg_MAP'].min()\n",
    "\n",
    "print(\"\\n--- KESIMPULAN STRATEGIS ABLASI ---\")\n",
    "print(f\"Rentang Kinerja (Avg MAP): {worst_map:.4f} sampai {best_map:.4f}\")\n",
    "print(\"1. XGBoost Kombinasi (SBERT + Keyword) adalah pemenang mutlak.\")\n",
    "print(\"2. Kekuatan Model Semantik (SBERT Saja) berada di tengah.\")\n",
    "print(\"3. Kekuatan Model Tradisional (Keyword Saja) berada di posisi terendah/tertinggi (tergantung hasil).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba18147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Memuat data dan membersihkan...\n",
      "‚úÖ Data siap: 170372 baris.\n",
      "\n",
      "--- Uji Konfigurasi A: Keyword Saja ---\n",
      "   -> Melatih Model Keyword Saja...\n",
      "\n",
      "--- Uji Konfigurasi B: SBERT Saja ---\n",
      "   -> Melatih Model SBERT Saja...\n",
      "\n",
      "--- Uji Konfigurasi C: Kombinasi Penuh ---\n",
      "   -> Melatih Model Kombinasi Penuh...\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI ABLASI FITUR FINAL (XGBOOST)\n",
      "=======================================================\n",
      "  Model           Fitur Avg_MAP ROC_AUC\n",
      "XGBoost Kombinasi Penuh  0.6706  0.8068\n",
      "XGBoost      SBERT Saja  0.6036  0.7545\n",
      "XGBoost    Keyword Saja  0.5371  0.7227\n",
      "\n",
      "--- KESIMPULAN STRATEGIS ABLASI ---\n",
      "1. Kombinasi Penuh: Pemenang Mutlak.\n",
      "2. Kontribusi SBERT Murni (MAP SBERT - MAP Keyword): 0.0665\n",
      "3. Kontribusi Tambahan Kombinasi (MAP Kombinasi - MAP SBERT): 0.0670\n",
      "\n",
      "‚úÖ Hipotesis terkonfirmasi: Kombinasi SBERT dan Keyword/BM25 memberikan sinergi terbaik untuk kinerja ranking.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, recall_score\n",
    "import os\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    # Mengasumsikan struktur direktori yang sama\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "# --- 2. DEFINISI FITUR ---\n",
    "# Konfigurasi A: Keyword Saja (BM25 + Jaccard)\n",
    "FEATURES_KEYWORD = ['bm25_score', 'jaccard_score'] \n",
    "# Konfigurasi B: SBERT Saja\n",
    "FEATURES_SBERT = ['sbert_sim']                     \n",
    "# Konfigurasi C: Kombinasi Penuh\n",
    "FEATURES_COMBINATION = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score'] \n",
    "TARGET = 'label'\n",
    "\n",
    "# --- 3. LOAD DATA & PREPROCESSING ---\n",
    "print(\"‚öôÔ∏è Memuat data dan membersihkan...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File tidak ditemukan di: {CSV_PATH}\")\n",
    "    raise\n",
    "\n",
    "# Konversi semua fitur yang dibutuhkan ke numerik dan drop NaN\n",
    "all_required_cols = list(set(FEATURES_COMBINATION + [TARGET]))\n",
    "for col in all_required_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=all_required_cols)\n",
    "print(f\"‚úÖ Data siap: {len(df)} baris.\")\n",
    "\n",
    "# Split Data (Menggunakan random state untuk hasil yang konsisten)\n",
    "X = df[FEATURES_COMBINATION] # Gunakan superset fitur untuk training/testing\n",
    "y = df[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 4. FUNGSI PELATIHAN & EVALUASI XGBOOST ---\n",
    "def evaluate_model_xgb(X_train, X_test, y_train, y_test, features, model_name):\n",
    "    \n",
    "    # Hanya pilih fitur yang relevan untuk konfigurasi ini\n",
    "    X_train_sub = X_train[features]\n",
    "    X_test_sub = X_test[features]\n",
    "\n",
    "    # Hitung Scale Pos Weight (Penting untuk data imbalance)\n",
    "    neg_count = np.sum(y_train == 0)\n",
    "    pos_count = np.sum(y_train == 1)\n",
    "    ratio = float(neg_count) / float(pos_count)\n",
    "    \n",
    "    # Model XGBoost (Setting optimal)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=ratio,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"   -> Melatih Model {model_name}...\")\n",
    "    model.fit(X_train_sub, y_train)\n",
    "\n",
    "    # Prediksi\n",
    "    y_prob = model.predict_proba(X_test_sub)[:, 1]\n",
    "\n",
    "    # Menghitung Metrik\n",
    "    map_score = average_precision_score(y_test, y_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    return {\n",
    "        'Model': 'XGBoost',\n",
    "        'Fitur': model_name,\n",
    "        'Avg_MAP': map_score,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# --- 5. EKSEKUSI ABLASI ---\n",
    "results = []\n",
    "\n",
    "# Konfigurasi A: Keyword Saja (BM25 + Jaccard)\n",
    "print(\"\\n--- Uji Konfigurasi A: Keyword Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_KEYWORD, \"Keyword Saja\"))\n",
    "\n",
    "# Konfigurasi B: SBERT Saja\n",
    "print(\"\\n--- Uji Konfigurasi B: SBERT Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_SBERT, \"SBERT Saja\"))\n",
    "\n",
    "# Konfigurasi C: Kombinasi Penuh\n",
    "print(\"\\n--- Uji Konfigurasi C: Kombinasi Penuh ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_COMBINATION, \"Kombinasi Penuh\"))\n",
    "\n",
    "\n",
    "# --- 6. DISPLAY HASIL ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI ABLASI FITUR FINAL (XGBOOST)\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "# Format output\n",
    "df_display = df_results.copy()\n",
    "df_display['Avg_MAP'] = df_display['Avg_MAP'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['ROC_AUC'] = df_display['ROC_AUC'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Sortir berdasarkan MAP (Paling penting untuk ranking)\n",
    "df_display = df_display.sort_values(by='Avg_MAP', ascending=False)\n",
    "\n",
    "# Menggunakan to_string()\n",
    "print(df_display.to_string(index=False))\n",
    "\n",
    "# Analisis Final\n",
    "best_map = df_results['Avg_MAP'].max()\n",
    "worst_map = df_results['Avg_MAP'].min()\n",
    "\n",
    "print(\"\\n--- KESIMPULAN STRATEGIS ABLASI ---\")\n",
    "print(\"1. Kombinasi Penuh: Pemenang Mutlak.\")\n",
    "print(f\"2. Kontribusi SBERT Murni (MAP SBERT - MAP Keyword): {df_results[df_results['Fitur'] == 'SBERT Saja']['Avg_MAP'].iloc[0] - df_results[df_results['Fitur'] == 'Keyword Saja']['Avg_MAP'].iloc[0]:.4f}\")\n",
    "print(f\"3. Kontribusi Tambahan Kombinasi (MAP Kombinasi - MAP SBERT): {df_results[df_results['Fitur'] == 'Kombinasi Penuh']['Avg_MAP'].iloc[0] - df_results[df_results['Fitur'] == 'SBERT Saja']['Avg_MAP'].iloc[0]:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Hipotesis terkonfirmasi: Kombinasi SBERT dan Keyword/BM25 memberikan sinergi terbaik untuk kinerja ranking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f41e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Memuat data dan membersihkan...\n",
      "‚úÖ Data siap: 170372 baris.\n",
      "\n",
      "--- Uji Konfigurasi A: Keyword Saja ---\n",
      "   -> Melatih Model Keyword Saja...\n",
      "\n",
      "--- Uji Konfigurasi B: SBERT Saja ---\n",
      "   -> Melatih Model SBERT Saja...\n",
      "\n",
      "--- Uji Konfigurasi C: Kombinasi Penuh ---\n",
      "   -> Melatih Model Kombinasi Penuh...\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI ABLASI FITUR (XGBOOST)\n",
      "=======================================================\n",
      "Metrik: Avg MAP dan Avg nDCG@5\n",
      "  Model           Fitur Avg_MAP Avg_nDCG_K\n",
      "XGBoost Kombinasi Penuh  0.6703     1.0000\n",
      "XGBoost      SBERT Saja  0.6036     0.9922\n",
      "XGBoost    Keyword Saja  0.5504     0.8304\n",
      "\n",
      "--- KESIMPULAN STRATEGIS ABLASI ---\n",
      "Kontribusi Tambahan Kombinasi (MAP Kombinasi - MAP SBERT): 0.0667\n",
      "‚úÖ Kombinasi fitur terkonfirmasi memberikan sinergi terbaik untuk kinerja ranking.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "# Import nDCG dari sklearn, yang paling stabil untuk environment ini\n",
    "from sklearn.metrics import ndcg_score\n",
    "import os\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "# Menggunakan file dataset yang sudah Anda miliki untuk training\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "# --- 2. DEFINISI FITUR ---\n",
    "# Konfigurasi A: Keyword Saja (BM25 + Jaccard)\n",
    "FEATURES_KEYWORD = ['bm25_score', 'jaccard_score', 'overlap_score'] # Kita pakai overlap juga\n",
    "# Konfigurasi B: SBERT Saja\n",
    "FEATURES_SBERT = ['sbert_sim']                     \n",
    "# Konfigurasi C: Kombinasi Penuh\n",
    "FEATURES_COMBINATION = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score'] \n",
    "TARGET = 'label'\n",
    "N_DCG_K = 5 # KITA TETAPKAN K=5\n",
    "\n",
    "# --- 3. LOAD DATA & PREPROCESSING ---\n",
    "print(\"‚öôÔ∏è Memuat data dan membersihkan...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File tidak ditemukan di: {CSV_PATH}\")\n",
    "    raise\n",
    "\n",
    "# Konversi semua fitur yang dibutuhkan ke numerik dan drop NaN\n",
    "all_required_cols = list(set(FEATURES_COMBINATION + [TARGET]))\n",
    "for col in all_required_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=all_required_cols)\n",
    "print(f\"‚úÖ Data siap: {len(df)} baris.\")\n",
    "\n",
    "# Split Data (Menggunakan random state untuk hasil yang konsisten)\n",
    "X = df[FEATURES_COMBINATION] # Gunakan superset fitur\n",
    "y = df[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 4. FUNGSI PELATIHAN & EVALUASI XGBOOST ---\n",
    "def evaluate_model_xgb(X_train, X_test, y_train, y_test, features, model_name):\n",
    "    \n",
    "    # Hanya pilih fitur yang relevan untuk konfigurasi ini\n",
    "    X_train_sub = X_train[features]\n",
    "    X_test_sub = X_test[features]\n",
    "\n",
    "    # Hitung Scale Pos Weight\n",
    "    neg_count = np.sum(y_train == 0)\n",
    "    pos_count = np.sum(y_train == 1)\n",
    "    ratio = float(neg_count) / float(pos_count)\n",
    "    \n",
    "    # Model XGBoost (Setting optimal)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=ratio,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"   -> Melatih Model {model_name}...\")\n",
    "    model.fit(X_train_sub, y_train)\n",
    "\n",
    "    # Prediksi\n",
    "    y_prob = model.predict_proba(X_test_sub)[:, 1]\n",
    "\n",
    "    # Menghitung Metrik\n",
    "    \n",
    "    # 1. MAP (Mean Average Precision)\n",
    "    map_score = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    # 2. nDCG@K (Membutuhkan y_true dan y_score/y_prob)\n",
    "    # nDCG harus dihitung pada array 2D\n",
    "    try:\n",
    "        ndcg_score_val = ndcg_score(np.asarray([y_test]), np.asarray([y_prob]), k=N_DCG_K)\n",
    "    except ValueError:\n",
    "        # Jika hanya ada satu kelas (0 atau 1) di y_test, nDCG tidak bisa dihitung.\n",
    "        ndcg_score_val = 0.0\n",
    "\n",
    "    return {\n",
    "        'Model': 'XGBoost',\n",
    "        'Fitur': model_name,\n",
    "        'Avg_MAP': map_score,\n",
    "        'Avg_nDCG_K': ndcg_score_val\n",
    "    }\n",
    "\n",
    "# --- 5. EKSEKUSI ABLASI ---\n",
    "results = []\n",
    "\n",
    "# Konfigurasi A: Keyword Saja (BM25 + Jaccard + Overlap)\n",
    "print(\"\\n--- Uji Konfigurasi A: Keyword Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_KEYWORD, \"Keyword Saja\"))\n",
    "\n",
    "# Konfigurasi B: SBERT Saja\n",
    "print(\"\\n--- Uji Konfigurasi B: SBERT Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_SBERT, \"SBERT Saja\"))\n",
    "\n",
    "# Konfigurasi C: Kombinasi Penuh\n",
    "print(\"\\n--- Uji Konfigurasi C: Kombinasi Penuh ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_COMBINATION, \"Kombinasi Penuh\"))\n",
    "\n",
    "\n",
    "# --- 6. DISPLAY HASIL ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI ABLASI FITUR (XGBOOST)\")\n",
    "print(\"=======================================================\")\n",
    "print(\"Metrik: Avg MAP dan Avg nDCG@5\")\n",
    "\n",
    "# Format output\n",
    "df_display = df_results.copy()\n",
    "df_display['Avg_MAP'] = df_display['Avg_MAP'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['Avg_nDCG_K'] = df_display['Avg_nDCG_K'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Sortir berdasarkan MAP (Paling penting untuk ranking)\n",
    "df_display = df_display.sort_values(by='Avg_MAP', ascending=False)\n",
    "\n",
    "# Menggunakan to_string()\n",
    "print(df_display.to_string(index=False))\n",
    "\n",
    "# Analisis Final\n",
    "sbert_map = df_results[df_results['Fitur'] == 'SBERT Saja']['Avg_MAP'].iloc[0]\n",
    "keyword_map = df_results[df_results['Fitur'] == 'Keyword Saja']['Avg_MAP'].iloc[0]\n",
    "combo_map = df_results[df_results['Fitur'] == 'Kombinasi Penuh']['Avg_MAP'].iloc[0]\n",
    "\n",
    "print(\"\\n--- KESIMPULAN STRATEGIS ABLASI ---\")\n",
    "print(f\"Kontribusi Tambahan Kombinasi (MAP Kombinasi - MAP SBERT): {combo_map - sbert_map:.4f}\")\n",
    "print(\"‚úÖ Kombinasi fitur terkonfirmasi memberikan sinergi terbaik untuk kinerja ranking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a4377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Memuat data dan membersihkan...\n",
      "‚úÖ Data siap: 170372 baris.\n",
      "\n",
      "--- Uji Konfigurasi A: Keyword Saja ---\n",
      "   -> Melatih Model Keyword Saja...\n",
      "\n",
      "--- Uji Konfigurasi B: SBERT Saja ---\n",
      "   -> Melatih Model SBERT Saja...\n",
      "\n",
      "--- Uji Konfigurasi C: Kombinasi Penuh ---\n",
      "   -> Melatih Model Kombinasi Penuh...\n",
      "\n",
      "\n",
      "=======================================================\n",
      "üèÜ HASIL UJI ABLASI FITUR (XGBOOST)\n",
      "=======================================================\n",
      "Metrik: Avg MAP dan ROC AUC\n",
      "  Model           Fitur Avg_MAP ROC_AUC\n",
      "XGBoost Kombinasi Penuh  0.6706  0.8068\n",
      "XGBoost      SBERT Saja  0.6036  0.7545\n",
      "XGBoost    Keyword Saja  0.5509  0.7287\n",
      "\n",
      "--- KESIMPULAN STRATEGIS ABLASI ---\n",
      "1. Peningkatan Kinerja Ranking (MAP): 0.0670 poin (dari SBERT ke Kombinasi)\n",
      "2. Hipotesis terkonfirmasi: Kombinasi SBERT dan Keyword/BM25 memberikan sinergi terbaik.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, recall_score\n",
    "import os\n",
    "\n",
    "# --- 1. KONFIGURASI PATHS ---\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    if not os.path.exists(os.path.join(ROOT_DIR, 'data')):\n",
    "        ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "# Menggunakan file dataset yang sudah Anda miliki untuk training\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'dataset_training_FULL_COMPLETE.csv')\n",
    "\n",
    "# --- 2. DEFINISI FITUR ---\n",
    "# Konfigurasi A: Keyword Saja (BM25 + Jaccard + Overlap)\n",
    "FEATURES_KEYWORD = ['bm25_score', 'jaccard_score', 'overlap_score']\n",
    "# Konfigurasi B: SBERT Saja\n",
    "FEATURES_SBERT = ['sbert_sim']                     \n",
    "# Konfigurasi C: Kombinasi Penuh\n",
    "FEATURES_COMBINATION = ['sbert_sim', 'bm25_score', 'overlap_score', 'jaccard_score'] \n",
    "TARGET = 'label'\n",
    "\n",
    "# --- 3. LOAD DATA & PREPROCESSING ---\n",
    "print(\"‚öôÔ∏è Memuat data dan membersihkan...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File tidak ditemukan di: {CSV_PATH}\")\n",
    "    raise\n",
    "\n",
    "# Konversi semua fitur yang dibutuhkan ke numerik dan drop NaN\n",
    "all_required_cols = list(set(FEATURES_COMBINATION + [TARGET]))\n",
    "for col in all_required_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=all_required_cols)\n",
    "print(f\"‚úÖ Data siap: {len(df)} baris.\")\n",
    "\n",
    "# Split Data (Menggunakan random state untuk hasil yang konsisten)\n",
    "X = df[FEATURES_COMBINATION] # Gunakan superset fitur\n",
    "y = df[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 4. FUNGSI PELATIHAN & EVALUASI XGBOOST ---\n",
    "def evaluate_model_xgb(X_train, X_test, y_train, y_test, features, model_name):\n",
    "    \n",
    "    # Hanya pilih fitur yang relevan untuk konfigurasi ini\n",
    "    X_train_sub = X_train[features]\n",
    "    X_test_sub = X_test[features]\n",
    "\n",
    "    # Hitung Scale Pos Weight\n",
    "    neg_count = np.sum(y_train == 0)\n",
    "    pos_count = np.sum(y_train == 1)\n",
    "    ratio = float(neg_count) / float(pos_count)\n",
    "    \n",
    "    # Model XGBoost (Setting optimal)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=ratio,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"   -> Melatih Model {model_name}...\")\n",
    "    model.fit(X_train_sub, y_train)\n",
    "\n",
    "    # Prediksi\n",
    "    y_prob = model.predict_proba(X_test_sub)[:, 1]\n",
    "\n",
    "    # Menghitung Metrik\n",
    "    \n",
    "    # 1. Avg MAP (Mean Average Precision)\n",
    "    map_score = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    # 2. ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    return {\n",
    "        'Model': 'XGBoost',\n",
    "        'Fitur': model_name,\n",
    "        'Avg_MAP': map_score,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# --- 5. EKSEKUSI ABLASI ---\n",
    "results = []\n",
    "\n",
    "# Konfigurasi A: Keyword Saja (BM25 + Jaccard + Overlap)\n",
    "print(\"\\n--- Uji Konfigurasi A: Keyword Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_KEYWORD, \"Keyword Saja\"))\n",
    "\n",
    "# Konfigurasi B: SBERT Saja\n",
    "print(\"\\n--- Uji Konfigurasi B: SBERT Saja ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_SBERT, \"SBERT Saja\"))\n",
    "\n",
    "# Konfigurasi C: Kombinasi Penuh\n",
    "print(\"\\n--- Uji Konfigurasi C: Kombinasi Penuh ---\")\n",
    "results.append(evaluate_model_xgb(X_train, X_test, y_train, y_test, FEATURES_COMBINATION, \"Kombinasi Penuh\"))\n",
    "\n",
    "\n",
    "# --- 6. DISPLAY HASIL ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"üèÜ HASIL UJI ABLASI FITUR (XGBOOST)\")\n",
    "print(\"=======================================================\")\n",
    "print(\"Metrik: Avg MAP dan ROC AUC\")\n",
    "\n",
    "# Format output\n",
    "df_display = df_results.copy()\n",
    "df_display['Avg_MAP'] = df_display['Avg_MAP'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_display['ROC_AUC'] = df_display['ROC_AUC'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Sortir berdasarkan MAP (Paling penting untuk ranking)\n",
    "df_display = df_display.sort_values(by='Avg_MAP', ascending=False)\n",
    "\n",
    "# Menggunakan to_string()\n",
    "print(df_display.to_string(index=False))\n",
    "\n",
    "# Analisis Final\n",
    "sbert_map = df_results[df_results['Fitur'] == 'SBERT Saja']['Avg_MAP'].iloc[0]\n",
    "keyword_map = df_results[df_results['Fitur'] == 'Keyword Saja']['Avg_MAP'].iloc[0]\n",
    "combo_map = df_results[df_results['Fitur'] == 'Kombinasi Penuh']['Avg_MAP'].iloc[0]\n",
    "\n",
    "print(\"\\n--- KESIMPULAN STRATEGIS ABLASI ---\")\n",
    "print(f\"1. Peningkatan Kinerja Ranking (MAP): {combo_map - sbert_map:.4f} poin (dari SBERT ke Kombinasi)\")\n",
    "print(\"2. Hipotesis terkonfirmasi: Kombinasi SBERT dan Keyword/BM25 memberikan sinergi terbaik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd33d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
